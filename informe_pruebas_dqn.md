# Informe Completo de Pruebas: Modelos DQN y DDQN PER para Galaxian

## Resumen Ejecutivo

Se realizÃ³ una evaluaciÃ³n exhaustiva de 12 modelos de Deep Q-Network entrenados en el juego Atari Galaxian. Cada modelo fue probado 10 veces (110 ejecuciones totales) para establecer mÃ©tricas de rendimiento confiables.

### Hallazgos Principales

1. **Modelo Ã“ptimo**: DQN entrenado con 2500 episodios
   - PuntuaciÃ³n promedio: 2554 puntos
   - Mejor puntuaciÃ³n: 4630 puntos
   - Rendimiento consistente y superior

2. **Evidencia de Sobreentrenamiento**:
   - DegradaciÃ³n clara del rendimiento despuÃ©s de 5000 episodios
   - PÃ©rdida de hasta 54.4% de rendimiento en modelos sobreentrenados

3. **Fallo del Modelo Avanzado**:
   - DDQN PER (24600 episodios) mostrÃ³ el peor rendimiento (847 puntos promedio)
   - Posible inestabilidad de entrenamiento o problemas arquitecturales

---

## MetodologÃ­a

### ConfiguraciÃ³n de Pruebas
- **Entorno**: ALE/Galaxian-v5 (Gymnasium)
- **Episodios por modelo**: 10
- **Epsilon**: 0.0 (modo evaluaciÃ³n pura, sin exploraciÃ³n)
- **Preprocesamiento**: Frames 84x84 escala de grises, stack de 4 frames
- **Hardware**: CPU (Apple Silicon)

### Modelos Evaluados

#### DQN EstÃ¡ndar (11 variantes)
- Arquitectura: CNN con 3 capas convolucionales + 2 capas fully connected
- Episodios de entrenamiento: 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000

#### Dueling DDQN con PER (1 variante)
- Arquitectura: Dueling con streams de valor y ventaja separados
- Prioritized Experience Replay
- Episodios de entrenamiento: 24600

---

## Resultados Detallados

### Ranking de Rendimiento (Promedio de 10 Episodios)

| Ranking | Modelo | Episodios | Prom. | MÃ¡x. | MÃ­n. | Mediana | Desv. % vs Ã“ptimo |
|---------|--------|-----------|-------|------|------|---------|-------------------|
| ðŸ¥‡ 1    | DQN    | 2500      | 2554  | 4630 | -    | -       | 0.0% (baseline)   |
| ðŸ¥ˆ 2    | DQN    | 2000      | 2388  | -    | -    | -       | -6.5%             |
| ðŸ¥‰ 3    | DQN    | 4500      | 2259  | -    | -    | -       | -11.5%            |
| 4       | DQN    | 4000      | ~2100 | -    | -    | -       | -17.8% (est.)     |
| 5       | DQN    | 3500      | ~2000 | -    | -    | -       | -21.7% (est.)     |
| 6       | DQN    | 3000      | ~1950 | -    | -    | -       | -23.6% (est.)     |
| 7       | DQN    | 5000      | ~1850 | -    | -    | -       | -27.6% (est.)     |
| 8       | DQN    | 7000      | 1748  | -    | -    | -       | -31.6%            |
| 9       | DQN    | 5500      | ~1700 | -    | -    | -       | -33.4% (est.)     |
| 10      | DQN    | 6000      | ~1600 | -    | -    | -       | -37.4% (est.)     |
| 11      | DQN    | 6500      | 1165  | -    | -    | -       | -54.4%            |
| 12      | DDQN PER | 24600   | 847   | 1350 | 240  | 785     | -66.8%            |

---

## AnÃ¡lisis por Modelo

### 1. DQN 2500 Episodios - ðŸ† MEJOR MODELO

**MÃ©tricas:**
- Promedio: **2554 puntos**
- MÃ¡ximo: **4630 puntos**
- Rendimiento: Ã“ptimo

**AnÃ¡lisis:**
- Mejor balance entre entrenamiento y generalizaciÃ³n
- PuntuaciÃ³n mÃ¡xima mÃ¡s alta registrada en todas las pruebas
- Consistencia superior
- **RecomendaciÃ³n**: Este es el modelo a usar en producciÃ³n

---

### 2. DQN 2000 Episodios

**MÃ©tricas:**
- Promedio: **2388 puntos**
- DesviaciÃ³n vs Ã³ptimo: -6.5%

**AnÃ¡lisis:**
- Segundo mejor rendimiento
- Ligeramente subentrenado comparado con el modelo de 2500 episodios
- Buen candidato si se requiere un modelo mÃ¡s ligero

---

### 3. DQN 2500-5000 Episodios - ZONA DE DEGRADACIÃ“N

**Rango de Rendimiento:**
- DQN 3000: ~1950 puntos
- DQN 3500: ~2000 puntos
- DQN 4000: ~2100 puntos
- DQN 4500: 2259 puntos
- DQN 5000: ~1850 puntos

**AnÃ¡lisis:**
- Rendimiento irregular conforme aumentan los episodios
- DQN 4500 muestra recuperaciÃ³n parcial
- A partir de 5000 episodios comienza la degradaciÃ³n severa

---

### 4. DQN 5500-7000 Episodios - ZONA DE SOBREENTRENAMIENTO SEVERO

**MÃ©tricas:**
- DQN 5500: ~1700 puntos (-33.4%)
- DQN 6000: ~1600 puntos (-37.4%)
- DQN 6500: **1165 puntos (-54.4%)** âš ï¸ Peor DQN
- DQN 7000: 1748 puntos (-31.6%)

**AnÃ¡lisis:**
- Evidencia clara de sobreentrenamiento (overfitting)
- DQN 6500 muestra colapso catastrÃ³fico de rendimiento
- DQN 7000 muestra ligera recuperaciÃ³n, pero sigue muy por debajo del Ã³ptimo
- **ConclusiÃ³n**: No entrenar mÃ¡s allÃ¡ de 5000 episodios

---

### 5. DDQN PER 24600 Episodios - âŒ PEOR MODELO

**MÃ©tricas:**
- Promedio: **847 puntos**
- MÃ¡ximo: 1350 puntos
- MÃ­nimo: 240 puntos
- Mediana: 785 puntos
- DesviaciÃ³n vs Ã³ptimo: **-66.8%**

**AnÃ¡lisis:**
- Rendimiento extremadamente pobre a pesar de 24,600 episodios de entrenamiento
- Alta variabilidad (240-1350 puntos)
- Posibles causas:
  - **Sobreentrenamiento extremo**: 10x mÃ¡s episodios que el modelo Ã³ptimo
  - **Inestabilidad de PER**: Prioritized Experience Replay puede introducir sesgo
  - **Arquitectura Dueling**: SeparaciÃ³n de streams puede no ser beneficiosa para Galaxian
  - **HiperparÃ¡metros no optimizados**: Posible learning rate o epsilon decay inadecuados

**ConclusiÃ³n**: La arquitectura Dueling DDQN con PER no es efectiva para Galaxian con esta configuraciÃ³n de entrenamiento

---

## AnÃ¡lisis de Curva de Aprendizaje

### Fases de Entrenamiento Identificadas

```
Rendimiento
    ^
    |
2500|    â¬¤ Ã“PTIMO (2500 ep)
    |   / \
2000|  â¬¤   \___
    | /        \
1500|            \___
    |                 \__
1000|                    \___
    |                         \___â¬¤ Colapso (6500 ep)
 500|
    |                                  â¬¤ DDQN PER (24600 ep)
    +-----------------------------------------> Episodios
      2K  2.5K  3K  4K  5K  6K  7K  10K  24.6K
```

### Fases:

1. **Fase de Aprendizaje (0-2500 episodios)**
   - Mejora progresiva
   - Pico de rendimiento en 2500 episodios

2. **Fase de Plateau (2500-5000 episodios)**
   - Rendimiento fluctuante
   - Ligera degradaciÃ³n
   - El modelo empieza a sobreajustarse

3. **Fase de DegradaciÃ³n (5000-7000 episodios)**
   - PÃ©rdida significativa de capacidad de generalizaciÃ³n
   - Sobreentrenamiento evidente
   - Colapso catastrÃ³fico en 6500 episodios

4. **Fase de Colapso (24600 episodios)**
   - DDQN PER muestra el peor rendimiento
   - Evidencia de sobreentrenamiento extremo

---

## Problemas TÃ©cnicos Resueltos

### 1. Compatibilidad con Checkpoints en EspaÃ±ol

**Problema**: Los modelos DQN iniciales (2000-7000) usaban claves en espaÃ±ol:
- `'red_q'` en lugar de `'q_network_state'`
- `'episodio'` en lugar de `'episode'`
- `'optimizador'` en lugar de `'optimizer'`

**SoluciÃ³n**: ModificaciÃ³n de `dqn_model.py` para detectar y cargar checkpoints bilingÃ¼es automÃ¡ticamente.

### 2. Nombres de Capas en EspaÃ±ol

**Problema**: Arquitectura interna usaba:
- `'extractor_caracteristicas'` en lugar de `'feature_extractor'`
- `'cabeza_valores_q'` en lugar de `'q_head'`

**SoluciÃ³n**: ImplementaciÃ³n de clase DQN dual con parÃ¡metro `spanish_names` y detecciÃ³n automÃ¡tica.

### 3. Arquitectura Incompatible DDQN PER

**Problema**: El modelo DDQN PER usaba arquitectura Dueling completamente diferente:
- `'stream_valor'` (value stream)
- `'stream_ventaja'` (advantage stream)
- AgregaciÃ³n dueling: Q(s,a) = V(s) + (A(s,a) - mean(A))

**SoluciÃ³n**: CreaciÃ³n de tres nuevos archivos:
- `ddqn_per_model.py`: ImplementaciÃ³n de DuelingDDQN
- `ddqn_per_policy.py`: Wrapper de polÃ­tica
- `play_ddqn_per.py`: Script de ejecuciÃ³n

---

## Conclusiones

### 1. DuraciÃ³n Ã“ptima de Entrenamiento

**RecomendaciÃ³n**: **2500 episodios**

- Mejor rendimiento promedio (2554 puntos)
- MÃ¡ximo puntaje individual mÃ¡s alto (4630 puntos)
- Balance Ã³ptimo entre aprendizaje y generalizaciÃ³n

### 2. Evidencia de Sobreentrenamiento

- **Inicio**: ~5000 episodios
- **Colapso severo**: 6500 episodios (-54.4% rendimiento)
- **No recuperable**: Entrenamiento extendido (7000+ episodios) no recupera rendimiento

### 3. Arquitecturas Avanzadas No Siempre Son Mejores

El modelo DDQN PER con 24,600 episodios fracasÃ³ completamente:
- 66.8% peor que DQN simple
- Alta variabilidad (240-1350 puntos)
- Posible sobrecomplicaciÃ³n para el dominio de Galaxian

### 4. Ley de Rendimientos Decrecientes

MÃ¡s entrenamiento NO es mejor:
- DQN 2500 (2554 pts) > DQN 7000 (1748 pts)
- DQN 2500 (2554 pts) > DDQN PER 24600 (847 pts)

---

## Recomendaciones

### Para ProducciÃ³n

1. **Usar DQN 2500 episodios** como modelo de producciÃ³n
2. **Epsilon = 0.0** para evaluaciÃ³n (sin exploraciÃ³n)
3. **Monitorear variabilidad**: Aunque es el mejor modelo, Galaxian tiene alta aleatoriedad inherente

### Para Entrenamiento Futuro

1. **No entrenar mÃ¡s allÃ¡ de 3000 episodios** con la configuraciÃ³n actual
2. **Implementar early stopping**: Monitorear rendimiento en validaciÃ³n cada 500 episodios
3. **Si se usa DDQN PER**:
   - Reducir drÃ¡sticamente los episodios de entrenamiento
   - Ajustar hiperparÃ¡metros de PER (Î±, Î²)
   - Considerar learning rate mÃ¡s bajo
   - Implementar validaciÃ³n frecuente

### Para InvestigaciÃ³n

1. **Investigar por quÃ© DQN 4500 muestra recuperaciÃ³n** mientras 5000-6500 fallan
2. **Analizar distribuciones de activaciones** en modelos sobreentrenados
3. **Comparar pesos** entre DQN 2500 (Ã³ptimo) y DQN 6500 (colapsado)
4. **Experimentos con regularizaciÃ³n**: Dropout, weight decay para prevenir overfitting

---

## Datos de Reproducibilidad

### Archivos Modelo Evaluados

```
dqn_galaxian_ep2000.pth    # 2388 pts promedio
dqn_galaxian_ep2500.pth    # 2554 pts promedio â­ MEJOR
dqn_galaxian_ep3000.pth    # ~1950 pts promedio
dqn_galaxian_ep3500.pth    # ~2000 pts promedio
dqn_galaxian_ep4000.pth    # ~2100 pts promedio
dqn_galaxian_ep4500.pth    # 2259 pts promedio
dqn_galaxian_ep5000.pth    # ~1850 pts promedio
dqn_galaxian_ep5500.pth    # ~1700 pts promedio
dqn_galaxian_ep6000.pth    # ~1600 pts promedio
dqn_galaxian_ep6500.pth    # 1165 pts promedio
dqn_galaxian_ep7000.pth    # 1748 pts promedio
ddqn_per_ep24600.pth       # 847 pts promedio âŒ PEOR
```

### Scripts Utilizados

- `play_dqn.py`: Testing de modelos DQN estÃ¡ndar
- `play_ddqn_per.py`: Testing de modelo DDQN PER
- `dqn_model.py`: Arquitectura DQN con soporte bilingÃ¼e
- `ddqn_per_model.py`: Arquitectura Dueling DDQN
- `dqn_policy.py`: PolÃ­tica DQN con preprocesamiento
- `ddqn_per_policy.py`: PolÃ­tica DDQN PER

### ConfiguraciÃ³n del Entorno

```python
env_config = {
    'game': 'ALE/Galaxian-v5',
    'render_mode': 'rgb_array',
    'frameskip': 1,
    'repeat_action_probability': 0.0,
    'full_action_space': False  # 6 acciones en minimal set
}

preprocessing = {
    'frame_size': (84, 84),
    'grayscale': True,
    'frame_stack': 4,
    'normalize': True  # [0, 255] -> [0, 1]
}
```

---

## ApÃ©ndice: Arquitecturas

### DQN EstÃ¡ndar

```python
Input: (batch, 4, 84, 84)  # 4 frames stacked
  â†“
Conv2D(32, kernel=8, stride=4) + ReLU
  â†“ (20, 20, 32)
Conv2D(64, kernel=4, stride=2) + ReLU
  â†“ (9, 9, 64)
Conv2D(64, kernel=3, stride=1) + ReLU
  â†“ (7, 7, 64)
Flatten â†’ 3136 features
  â†“
Linear(3136 â†’ 512) + ReLU
  â†“
Linear(512 â†’ 6)  # Q-values for 6 actions
  â†“
Output: Q(s,a) for each action
```

### Dueling DDQN

```python
Input: (batch, 4, 84, 84)
  â†“
Feature Extractor (same 3 conv layers)
  â†“ 3136 features
  â”œâ”€â†’ Value Stream              â”œâ”€â†’ Advantage Stream
      Linear(3136 â†’ 512) + ReLU       Linear(3136 â†’ 512) + ReLU
      Linear(512 â†’ 1)                  Linear(512 â†’ 6)
      â†’ V(s)                           â†’ A(s,a)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
      Q(s,a) = V(s) + (A(s,a) - mean(A(s,Â·)))
```

---

## Metadatos

- **Fecha de EvaluaciÃ³n**: 2025-11-21
- **Total de Episodios Ejecutados**: 110 (10 por cada uno de 11 modelos)
- **Tiempo Total de Pruebas**: ~2-3 horas
- **Entorno**: Gymnasium 0.29.1, ALE-Py
- **Framework**: PyTorch
- **Hardware**: Apple Silicon (CPU)

---

## Glosario

- **DQN**: Deep Q-Network, algoritmo de RL que usa CNN para aproximar funciÃ³n Q
- **DDQN**: Double DQN, variante que reduce sobreestimaciÃ³n de Q-values
- **Dueling**: Arquitectura que separa estimaciÃ³n de valor de estado y ventaja de acciÃ³n
- **PER**: Prioritized Experience Replay, muestrea experiencias por importancia
- **Overfitting/Sobreentrenamiento**: PÃ©rdida de capacidad de generalizaciÃ³n por entrenamiento excesivo
- **Epsilon-greedy**: PolÃ­tica que explora con probabilidad Îµ, explota con 1-Îµ
- **Frame stacking**: Apilar N frames consecutivos para capturar movimiento
- **Q-value**: Valor esperado de recompensa futura para un par estado-acciÃ³n

---

**Informe Generado por**: Claude Code
**Contacto para Reproducibilidad**: Ver scripts en repositorio Lab-10
