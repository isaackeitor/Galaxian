{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9764e5",
   "metadata": {},
   "source": [
    "# DQN - Continuar Entrenamiento (Hiperpar√°metros ORIGINALES)\n",
    "\n",
    "Este notebook permite:\n",
    "1. ‚úÖ Cargar un modelo DQN pre-entrenado desde checkpoint\n",
    "2. üîß Continuar entrenamiento con **hiperpar√°metros ORIGINALES** (probados)\n",
    "3. üìà Evitar colapso de rendimiento\n",
    "\n",
    "## üéØ Estrategia:\n",
    "Despu√©s de m√∫ltiples experimentos, la conclusi√≥n es clara: **los hiperpar√°metros originales del c√≥digo base YA estaban optimizados**. Todos los cambios causaron ca√≠das brutales.\n",
    "\n",
    "## ‚úÖ Configuraci√≥n Final:\n",
    "- **Hiperpar√°metros:** EXACTAMENTE como el c√≥digo original\n",
    "- **√önico cambio:** Epsilon m√°s bajo (0.15 ‚Üí 0.05) porque el modelo ya aprendi√≥\n",
    "\n",
    "## üìä Hiperpar√°metros Originales Probados:\n",
    "```python\n",
    "capacidad_replay = 100_000    # ‚úì Funciona\n",
    "tam_lote = 32                 # ‚úì Funciona\n",
    "factor_descuento = 0.99       # ‚úì Funciona\n",
    "tasa_aprendizaje = 1e-4       # ‚úì Funciona\n",
    "target_update = 1000          # ‚úì Funciona\n",
    "```\n",
    "\n",
    "Estos valores son est√°ndar en la literatura de DQN y han sido probados extensivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de directorios\n",
    "import os\n",
    "\n",
    "DIRECTORIO_BASE = os.path.join(os.getcwd(), \"resultados_entrenamiento\")\n",
    "os.makedirs(DIRECTORIO_BASE, exist_ok=True)\n",
    "\n",
    "print(\"Directorio base:\", DIRECTORIO_BASE)\n",
    "print(\"Resultados se guardar√°n en:\", os.path.abspath(DIRECTORIO_BASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df075bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Wrappers de preprocesamiento cargados\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento del entorno Atari\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class EnvolturaPreprocesamiento(gym.Wrapper):\n",
    "    def __init__(self, entorno_base, salto_cuadros=4, tam_pantalla=84, escala_grises=True):\n",
    "        super().__init__(entorno_base)\n",
    "        self.salto_cuadros = salto_cuadros\n",
    "        self.tam_pantalla = tam_pantalla\n",
    "        self.escala_grises = escala_grises\n",
    "        forma_obs = (tam_pantalla, tam_pantalla)\n",
    "        if not escala_grises:\n",
    "            forma_obs += (3,)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=forma_obs, dtype=np.uint8)\n",
    "\n",
    "    def procesar_cuadro(self, cuadro):\n",
    "        if self.escala_grises:\n",
    "            cuadro = cv2.cvtColor(cuadro, cv2.COLOR_RGB2GRAY)\n",
    "            cuadro = cv2.resize(cuadro, (self.tam_pantalla, self.tam_pantalla), interpolation=cv2.INTER_AREA)\n",
    "            return cuadro\n",
    "        else:\n",
    "            cuadro = cv2.resize(cuadro, (self.tam_pantalla, self.tam_pantalla), interpolation=cv2.INTER_AREA)\n",
    "            return cuadro\n",
    "\n",
    "    def step(self, accion):\n",
    "        recompensa_acumulada = 0.0\n",
    "        terminado = truncado = False\n",
    "        for _ in range(self.salto_cuadros):\n",
    "            observacion_raw, recomp, term, trunc, informacion = self.env.step(accion)\n",
    "            recompensa_acumulada += recomp\n",
    "            terminado |= term\n",
    "            truncado |= trunc\n",
    "            if terminado or truncado:\n",
    "                break\n",
    "        cuadro_procesado = self.procesar_cuadro(observacion_raw)\n",
    "        return cuadro_procesado, recompensa_acumulada, terminado, truncado, informacion\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observacion_raw, informacion = self.env.reset(**kwargs)\n",
    "        cuadro_procesado = self.procesar_cuadro(observacion_raw)\n",
    "        return cuadro_procesado, informacion\n",
    "\n",
    "\n",
    "class EnvolturaApilamiento(gym.Wrapper):\n",
    "    def __init__(self, entorno_base, num_apilar=4):\n",
    "        super().__init__(entorno_base)\n",
    "        self.num_apilar = num_apilar\n",
    "        self.cuadros_memoria = deque([], maxlen=num_apilar)\n",
    "        bajo = np.repeat(entorno_base.observation_space.low[np.newaxis, ...], num_apilar, axis=0)\n",
    "        alto = np.repeat(entorno_base.observation_space.high[np.newaxis, ...], num_apilar, axis=0)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=bajo.min(), high=alto.max(), dtype=entorno_base.observation_space.dtype,\n",
    "            shape=(num_apilar, *entorno_base.observation_space.shape)\n",
    "        )\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observacion, informacion = self.env.reset(**kwargs)\n",
    "        for _ in range(self.num_apilar):\n",
    "            self.cuadros_memoria.append(observacion)\n",
    "        return self._obtener_obs(), informacion\n",
    "\n",
    "    def step(self, accion):\n",
    "        observacion, recompensa, terminado, truncado, informacion = self.env.step(accion)\n",
    "        self.cuadros_memoria.append(observacion)\n",
    "        return self._obtener_obs(), recompensa, terminado, truncado, informacion\n",
    "\n",
    "    def _obtener_obs(self):\n",
    "        return np.stack(self.cuadros_memoria, axis=0)\n",
    "\n",
    "\n",
    "def crear_entorno_galaxian(semilla=None, modo_render=None):\n",
    "    entorno = gym.make(\"ALE/Galaxian-v5\", render_mode=modo_render)\n",
    "    if semilla is not None:\n",
    "        entorno.reset(seed=semilla)\n",
    "    entorno = EnvolturaPreprocesamiento(entorno, salto_cuadros=4, tam_pantalla=84, escala_grises=True)\n",
    "    entorno = EnvolturaApilamiento(entorno, num_apilar=4)\n",
    "    return entorno\n",
    "\n",
    "print(\"‚úÖ Wrappers de preprocesamiento cargados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e95652d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Usando GPU de Apple Silicon (MPS)\n",
      "‚úÖ Clases DQN cargadas\n"
     ]
    }
   ],
   "source": [
    "# Implementaci√≥n DQN\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import Tuple, Deque, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Detectar dispositivo\n",
    "if torch.backends.mps.is_available():\n",
    "    dispositivo = torch.device(\"mps\")\n",
    "    print(\"‚úÖ Usando GPU de Apple Silicon (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    dispositivo = torch.device(\"cuda\")\n",
    "    print(\"‚úÖ Usando GPU CUDA\")\n",
    "else:\n",
    "    dispositivo = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è Usando CPU\")\n",
    "\n",
    "\n",
    "class RedDQN(nn.Module):\n",
    "    def __init__(self, forma_entrada: Tuple[int, int, int], num_acciones: int):\n",
    "        super().__init__()\n",
    "        canales, alto, ancho = forma_entrada\n",
    "        self.canales_esperados = canales\n",
    "\n",
    "        self.extractor_caracteristicas = nn.Sequential(\n",
    "            nn.Conv2d(canales, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tensor_prueba = torch.zeros(1, canales, alto, ancho)\n",
    "            tam_aplanado = self.extractor_caracteristicas(tensor_prueba).shape[1]\n",
    "\n",
    "        self.cabeza_valores_q = nn.Sequential(\n",
    "            nn.Linear(tam_aplanado, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_acciones)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor_entrada: torch.Tensor) -> torch.Tensor:\n",
    "        if tensor_entrada.ndim != 4:\n",
    "            raise ValueError(f\"Esperado tensor 4D, recibido {tensor_entrada.ndim}D\")\n",
    "        if tensor_entrada.shape[1] != self.canales_esperados and tensor_entrada.shape[-1] == self.canales_esperados:\n",
    "            tensor_entrada = tensor_entrada.permute(0, 3, 1, 2)\n",
    "        tensor_entrada = tensor_entrada.float() / 255.0\n",
    "        caracteristicas = self.extractor_caracteristicas(tensor_entrada)\n",
    "        return self.cabeza_valores_q(caracteristicas)\n",
    "\n",
    "\n",
    "class BufferReplay:\n",
    "    def __init__(self, capacidad_maxima: int):\n",
    "        self.almacen: Deque = deque(maxlen=capacidad_maxima)\n",
    "\n",
    "    def agregar(self, estado, accion, recompensa, estado_sig, terminado):\n",
    "        self.almacen.append((estado, accion, recompensa, estado_sig, terminado))\n",
    "\n",
    "    def muestrear(self, tam_lote: int):\n",
    "        lote = random.sample(self.almacen, tam_lote)\n",
    "        estados, acciones, recompensas, estados_sig, terminados = map(np.array, zip(*lote))\n",
    "        return estados, acciones, recompensas, estados_sig, terminados\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.almacen)\n",
    "\n",
    "\n",
    "def _calcular_media_movil(valores: List[float], ventana: int = 100):\n",
    "    if len(valores) == 0:\n",
    "        return []\n",
    "    resultado = []\n",
    "    suma_acum = 0.0\n",
    "    cola = []\n",
    "    for v in valores:\n",
    "        cola.append(v)\n",
    "        suma_acum += v\n",
    "        if len(cola) > ventana:\n",
    "            suma_acum -= cola.pop(0)\n",
    "        resultado.append(suma_acum / len(cola))\n",
    "    return resultado\n",
    "\n",
    "\n",
    "def _guardar_grafica_y_csv(dir_checkpoints: str, recompensas: List[float], episodio: int):\n",
    "    os.makedirs(dir_checkpoints, exist_ok=True)\n",
    "    ruta_csv = os.path.join(dir_checkpoints, \"registro_recompensas.csv\")\n",
    "    archivo_nuevo = not os.path.exists(ruta_csv)\n",
    "    with open(ruta_csv, \"a\", newline=\"\") as archivo:\n",
    "        escritor = csv.writer(archivo)\n",
    "        if archivo_nuevo:\n",
    "            escritor.writerow([\"episodio\", \"recompensa\"])\n",
    "        escritor.writerow([episodio, recompensas[-1]])\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(recompensas, label=\"Recompensa por episodio\", color=\"#FF6B35\", linewidth=1.2, alpha=0.7)\n",
    "    media_movil = _calcular_media_movil(recompensas, ventana=100)\n",
    "    if len(media_movil) > 0:\n",
    "        plt.plot(media_movil, label=\"Media M√≥vil (100 eps)\", color=\"#00A896\", linewidth=2.5)\n",
    "    plt.xlabel(\"Episodio\", fontsize=12)\n",
    "    plt.ylabel(\"Recompensa Total\", fontsize=12)\n",
    "    plt.title(\"Progreso de Entrenamiento - DQN\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    ruta_png = os.path.join(dir_checkpoints, f\"recompensas_ep{episodio}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ruta_png, dpi=120)\n",
    "    plt.close()\n",
    "    print(f\"[REGISTRO] Gr√°fica guardada: {ruta_png}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Clases DQN cargadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c16c5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de entrenamiento continuo cargada\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n de entrenamiento MODIFICADA para continuar desde checkpoint\n",
    "\n",
    "def continuar_entrenamiento_dqn(\n",
    "    ruta_checkpoint: str,\n",
    "    directorio_checkpoints: str,\n",
    "    episodios_adicionales: int = 2000,\n",
    "    capacidad_replay: int = 200_000,\n",
    "    tam_lote: int = 128,\n",
    "    factor_descuento: float = 0.995,\n",
    "    tasa_aprendizaje: float = 1e-4,\n",
    "    epsilon_inicial: float = 0.3,\n",
    "    epsilon_final: float = 0.1,\n",
    "    episodios_decaimiento_eps: int = 1500,\n",
    "    intervalo_actualizacion_target: int = 2500,\n",
    "    pasos_inicio_entrenamiento: int = 5_000,\n",
    "    intervalo_guardado: int = 250,\n",
    "    intervalo_graficas: int = 250,\n",
    "    semilla_aleatoria: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Contin√∫a el entrenamiento DQN desde un checkpoint previo.\n",
    "    \n",
    "    Args:\n",
    "        ruta_checkpoint: Ruta al archivo .pth del checkpoint\n",
    "        directorio_checkpoints: Donde guardar nuevos checkpoints\n",
    "        episodios_adicionales: Cu√°ntos episodios m√°s entrenar\n",
    "        epsilon_inicial: Exploraci√≥n inicial (menor que antes, ya aprendi√≥)\n",
    "    \"\"\"\n",
    "    os.makedirs(directorio_checkpoints, exist_ok=True)\n",
    "    \n",
    "    # 1. CARGAR CHECKPOINT\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìÇ CARGANDO CHECKPOINT\")\n",
    "    print(\"=\"*70)\n",
    "    checkpoint = torch.load(ruta_checkpoint, map_location=dispositivo, weights_only=False)\n",
    "    \n",
    "    episodio_inicio = checkpoint['episodio']\n",
    "    pasos_globales_inicio = checkpoint['pasos_globales']\n",
    "    recompensas_previas = checkpoint['recompensas']\n",
    "    forma_entrada = tuple(checkpoint['forma_entrada'])\n",
    "    num_acciones = checkpoint['num_acciones']\n",
    "    \n",
    "    # Estad√≠sticas del modelo cargado\n",
    "    recompensa_promedio = np.mean(recompensas_previas[-100:]) if len(recompensas_previas) >= 100 else np.mean(recompensas_previas)\n",
    "    recompensa_maxima = max(recompensas_previas)\n",
    "    \n",
    "    print(f\"‚úÖ Checkpoint cargado: episodio {episodio_inicio}\")\n",
    "    print(f\"üìä Recompensa promedio √∫ltimos 100 eps: {recompensa_promedio:.1f}\")\n",
    "    print(f\"üèÜ Recompensa m√°xima alcanzada: {recompensa_maxima:.1f}\")\n",
    "    print(f\"üéØ Pasos totales: {pasos_globales_inicio:,}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # 2. CREAR ENTORNO\n",
    "    entorno_juego = crear_entorno_galaxian(semilla=semilla_aleatoria, modo_render=None)\n",
    "    \n",
    "    # 3. RECREAR REDES\n",
    "    red_q_principal = RedDQN(forma_entrada, num_acciones).to(dispositivo)\n",
    "    red_q_objetivo = RedDQN(forma_entrada, num_acciones).to(dispositivo)\n",
    "    \n",
    "    # Cargar pesos\n",
    "    red_q_principal.load_state_dict(checkpoint['red_q'])\n",
    "    red_q_objetivo.load_state_dict(checkpoint['red_objetivo'])\n",
    "    red_q_objetivo.eval()\n",
    "    \n",
    "    # 4. RECREAR OPTIMIZADOR\n",
    "    optimizador = optim.Adam(red_q_principal.parameters(), lr=tasa_aprendizaje)\n",
    "    optimizador.load_state_dict(checkpoint['optimizador'])\n",
    "    \n",
    "    # 5. CREAR NUEVO BUFFER (vac√≠o, se llenar√° durante entrenamiento)\n",
    "    memoria_experiencias = BufferReplay(capacidad_replay)\n",
    "    \n",
    "    # 6. CONFIGURACI√ìN DE ENTRENAMIENTO CONTINUO\n",
    "    pasos_globales = pasos_globales_inicio\n",
    "    registro_recompensas = recompensas_previas.copy()\n",
    "    \n",
    "    episodio_final = episodio_inicio + episodios_adicionales\n",
    "    \n",
    "    def calcular_epsilon(ep_actual: int, ep_inicio_checkpoint: int) -> float:\n",
    "        ep_relativo = ep_actual - ep_inicio_checkpoint\n",
    "        if ep_relativo >= episodios_decaimiento_eps:\n",
    "            return epsilon_final\n",
    "        fraccion = ep_relativo / float(episodios_decaimiento_eps)\n",
    "        return epsilon_inicial + fraccion * (epsilon_final - epsilon_inicial)\n",
    "    \n",
    "    # 7. MOSTRAR CONFIGURACI√ìN\n",
    "    print(\"=\"*70)\n",
    "    print(\"üöÄ INICIANDO ENTRENAMIENTO CONTINUO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üìç Episodio inicial: {episodio_inicio + 1}\")\n",
    "    print(f\"üéØ Episodio final: {episodio_final}\")\n",
    "    print(f\"‚ûï Episodios adicionales: {episodios_adicionales}\")\n",
    "    print(f\"\\nüîß HIPERPAR√ÅMETROS ANTI-OVERFITTING:\")\n",
    "    print(f\"   Buffer: {capacidad_replay:,}\")\n",
    "    print(f\"   Batch: {tam_lote}\")\n",
    "    print(f\"   Learning rate: {tasa_aprendizaje}\")\n",
    "    print(f\"   Gamma: {factor_descuento}\")\n",
    "    print(f\"   Epsilon: {epsilon_inicial} ‚Üí {epsilon_final} (en {episodios_decaimiento_eps} eps)\")\n",
    "    print(f\"   Target update: cada {intervalo_actualizacion_target} pasos\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # 8. BUCLE DE ENTRENAMIENTO\n",
    "    for episodio_actual in range(episodio_inicio + 1, episodio_final + 1):\n",
    "        observacion, _ = entorno_juego.reset()\n",
    "        finalizado = False\n",
    "        recompensa_total = 0.0\n",
    "        epsilon_actual = calcular_epsilon(episodio_actual, episodio_inicio)\n",
    "        pasos_en_episodio = 0\n",
    "        \n",
    "        while not finalizado:\n",
    "            pasos_globales += 1\n",
    "            pasos_en_episodio += 1\n",
    "            \n",
    "            # Pol√≠tica epsilon-greedy\n",
    "            if random.random() < epsilon_actual:\n",
    "                accion_elegida = entorno_juego.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    obs_lote = np.expand_dims(observacion, axis=0)\n",
    "                    obs_tensor = torch.from_numpy(obs_lote).to(dispositivo)\n",
    "                    valores_q = red_q_principal(obs_tensor)\n",
    "                    accion_elegida = int(torch.argmax(valores_q, dim=1).item())\n",
    "            \n",
    "            siguiente_obs, recompensa_step, terminado, truncado, _ = entorno_juego.step(accion_elegida)\n",
    "            finalizado = terminado or truncado\n",
    "            recompensa_total += float(recompensa_step)\n",
    "            \n",
    "            memoria_experiencias.agregar(observacion, accion_elegida, recompensa_step, siguiente_obs, finalizado)\n",
    "            observacion = siguiente_obs\n",
    "            \n",
    "            # Entrenamiento\n",
    "            if len(memoria_experiencias) >= pasos_inicio_entrenamiento:\n",
    "                estados, acciones, recompensas, estados_sig, terminados = memoria_experiencias.muestrear(tam_lote)\n",
    "                \n",
    "                estados_t = torch.from_numpy(estados).to(dispositivo)\n",
    "                estados_sig_t = torch.from_numpy(estados_sig).to(dispositivo)\n",
    "                acciones_t = torch.from_numpy(acciones).long().to(dispositivo)\n",
    "                recompensas_t = torch.from_numpy(recompensas).float().to(dispositivo)\n",
    "                terminados_t = torch.from_numpy(terminados.astype(np.float32)).to(dispositivo)\n",
    "                \n",
    "                valores_q = red_q_principal(estados_t)\n",
    "                q_valores_accion = valores_q.gather(1, acciones_t.unsqueeze(1)).squeeze(1)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    q_siguientes = red_q_objetivo(estados_sig_t).max(1)[0]\n",
    "                    q_objetivo = recompensas_t + factor_descuento * q_siguientes * (1.0 - terminados_t)\n",
    "                \n",
    "                perdida = nn.functional.mse_loss(q_valores_accion, q_objetivo)\n",
    "                \n",
    "                optimizador.zero_grad()\n",
    "                perdida.backward()\n",
    "                nn.utils.clip_grad_norm_(red_q_principal.parameters(), 10.0)\n",
    "                optimizador.step()\n",
    "            \n",
    "            # Actualizar red objetivo\n",
    "            if pasos_globales % intervalo_actualizacion_target == 0:\n",
    "                red_q_objetivo.load_state_dict(red_q_principal.state_dict())\n",
    "        \n",
    "        registro_recompensas.append(recompensa_total)\n",
    "        \n",
    "        # Calcular estad√≠sticas\n",
    "        promedio_100 = np.mean(registro_recompensas[-100:]) if len(registro_recompensas) >= 100 else np.mean(registro_recompensas)\n",
    "        \n",
    "        print(f\"[DQN] Ep {episodio_actual}/{episodio_final} | R: {recompensa_total:.1f} | \"\n",
    "              f\"Prom100: {promedio_100:.1f} | Œµ={epsilon_actual:.3f} | Mem={len(memoria_experiencias)}\")\n",
    "        \n",
    "        # Guardar checkpoint\n",
    "        if episodio_actual % intervalo_guardado == 0:\n",
    "            ruta_ckpt = os.path.join(directorio_checkpoints, f\"dqn_galaxian_ep{episodio_actual}.pth\")\n",
    "            torch.save({\n",
    "                \"red_q\": red_q_principal.state_dict(),\n",
    "                \"red_objetivo\": red_q_objetivo.state_dict(),\n",
    "                \"optimizador\": optimizador.state_dict(),\n",
    "                \"episodio\": episodio_actual,\n",
    "                \"pasos_globales\": pasos_globales,\n",
    "                \"recompensas\": registro_recompensas,\n",
    "                \"forma_entrada\": forma_entrada,\n",
    "                \"num_acciones\": num_acciones,\n",
    "            }, ruta_ckpt)\n",
    "            print(f\"[CHECKPOINT] Guardado: {ruta_ckpt}\")\n",
    "        \n",
    "        # Guardar gr√°ficas\n",
    "        if episodio_actual % intervalo_graficas == 0:\n",
    "            _guardar_grafica_y_csv(directorio_checkpoints, registro_recompensas, episodio_actual)\n",
    "    \n",
    "    entorno_juego.close()\n",
    "    \n",
    "    # Guardar modelo final\n",
    "    ruta_final = os.path.join(directorio_checkpoints, f\"dqn_galaxian_final_ep{episodio_final}.pth\")\n",
    "    torch.save(red_q_principal.state_dict(), ruta_final)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ ENTRENAMIENTO COMPLETADO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"‚úÖ Modelo final guardado: {ruta_final}\")\n",
    "    print(f\"üìä Episodios totales: {episodio_final}\")\n",
    "    print(f\"üèÜ Mejor recompensa: {max(registro_recompensas):.1f}\")\n",
    "    print(f\"üìà Promedio √∫ltimos 100 eps: {np.mean(registro_recompensas[-100:]):.1f}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return red_q_principal\n",
    "\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de entrenamiento continuo cargada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1b721",
   "metadata": {},
   "source": [
    "## üéØ Estrategia Final: Hiperpar√°metros ORIGINALES\n",
    "\n",
    "### üìâ An√°lisis del Problema:\n",
    "Todas las modificaciones causaron **ca√≠das brutales**:\n",
    "- Primera ca√≠da: 3000 ‚Üí 2200 puntos (config conservadora)\n",
    "- Segunda ca√≠da: 3100 ‚Üí 1800 puntos (config balanceada)\n",
    "\n",
    "**Conclusi√≥n:** Los hiperpar√°metros originales del c√≥digo base YA estaban optimizados. Los cambios rompieron el equilibrio.\n",
    "\n",
    "### ‚úÖ Soluci√≥n Final:\n",
    "**Volver a los hiperpar√°metros ORIGINALES** exactamente como estaban en el c√≥digo base:\n",
    "\n",
    "| Par√°metro | Valor ORIGINAL | Por qu√© funcionaba |\n",
    "|-----------|----------------|-------------------|\n",
    "| **capacidad_replay** | 100,000 | Equilibrio memoria/diversidad |\n",
    "| **tam_lote** | 32 | Updates frecuentes y estables |\n",
    "| **factor_descuento** | 0.99 | Balance presente/futuro est√°ndar |\n",
    "| **tasa_aprendizaje** | 1e-4 | Estable, probado en papers |\n",
    "| **target_update** | 1000 | Sincronizaci√≥n est√°ndar DQN |\n",
    "\n",
    "### üéØ √öNICO Ajuste:\n",
    "**Epsilon m√°s bajo** (el modelo ya aprendi√≥ durante 2500+ episodios):\n",
    "- `epsilon_inicial`: 1.0 ‚Üí **0.15** (menos exploraci√≥n aleatoria)\n",
    "- `epsilon_final`: 0.1 ‚Üí **0.05** (m√°s greedy)\n",
    "- `episodios_decaimiento_eps`: 300 ‚Üí **1000** (transici√≥n suave)\n",
    "\n",
    "### üìç Checkpoint Recomendado:\n",
    "```python\n",
    "RUTA_CHECKPOINT = \"dqn/dqn_galaxian_ep2500.pth\"  # Pico de ~3100 puntos\n",
    "```\n",
    "\n",
    "### üìà Resultado Esperado:\n",
    "Con los hiperpar√°metros originales + epsilon bajo:\n",
    "```\n",
    "Episodio 2500 (inicio): 3100 puntos\n",
    "Episodio 3000:          3100-3200 puntos (ESTABLE)\n",
    "Episodio 3500:          3200-3300 puntos (MEJORA GRADUAL)\n",
    "Episodio 4500:          3300-3400 puntos (SIN CA√çDAS)\n",
    "```\n",
    "\n",
    "El modelo se mantendr√° **estable** porque usamos los hiperpar√°metros probados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1726a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CONFIGURACI√ìN: HIPERPAR√ÅMETROS ORIGINALES (Probados y funcionales)\n",
    "# ========================================\n",
    "\n",
    "# 1. RUTA DEL CHECKPOINT A CARGAR  \n",
    "# Carga el checkpoint del PICO (~episodio 2500 con ~3100 puntos)\n",
    "if \"DIRECTORIO_BASE\" not in globals():\n",
    "    DIRECTORIO_BASE = os.path.join(os.getcwd(), \"resultados_entrenamiento\")\n",
    "    os.makedirs(DIRECTORIO_BASE, exist_ok=True)\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANTE: Carga el checkpoint del MEJOR rendimiento (episodio 2500)\n",
    "RUTA_CHECKPOINT = os.path.join(DIRECTORIO_BASE, \"dqn/dqn_galaxian_ep2500.pth\")\n",
    "\n",
    "# Verificar que existe\n",
    "if not os.path.exists(RUTA_CHECKPOINT):\n",
    "    print(f\"‚ö†Ô∏è ERROR: No se encontr√≥ el checkpoint: {RUTA_CHECKPOINT}\")\n",
    "    print(f\"\\nüìÅ Archivos disponibles:\")\n",
    "    if os.path.exists(os.path.dirname(RUTA_CHECKPOINT)):\n",
    "        archivos_pth = [f for f in sorted(os.listdir(os.path.dirname(RUTA_CHECKPOINT))) if f.endswith('.pth')]\n",
    "        for archivo in archivos_pth:\n",
    "            print(f\"   - {archivo}\")\n",
    "        print(f\"\\nüí° Usa el checkpoint del episodio ~2500 (pico de ~3100 puntos)\")\n",
    "else:\n",
    "    print(f\"‚úÖ Checkpoint encontrado: {RUTA_CHECKPOINT}\")\n",
    "    \n",
    "    # 2. DIRECTORIO PARA NUEVOS CHECKPOINTS\n",
    "    DIRECTORIO_NUEVO = os.path.join(DIRECTORIO_BASE, \"dqn_v3_original\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéØ CONFIGURACI√ìN: HIPERPAR√ÅMETROS ORIGINALES\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"üìâ Problema: Todos los cambios causaron ca√≠das brutales\")\n",
    "    print(\"‚úÖ Soluci√≥n: Volver a los hiperpar√°metros ORIGINALES que S√ç funcionaban\")\n",
    "    print(\"   Solo ajuste: Epsilon m√°s bajo (ya aprendi√≥ mucho)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # 3. ENTRENAR con hiperpar√°metros ORIGINALES\n",
    "    red_mejorada = continuar_entrenamiento_dqn(\n",
    "        ruta_checkpoint=RUTA_CHECKPOINT,\n",
    "        directorio_checkpoints=DIRECTORIO_NUEVO,\n",
    "        \n",
    "        # Episodios\n",
    "        episodios_adicionales=2000,          # 2000 episodios m√°s\n",
    "        \n",
    "        # ‚ö° HIPERPAR√ÅMETROS ORIGINALES (los que S√ç funcionaban) ‚ö°\n",
    "        capacidad_replay=100_000,            # ‚úì Original del c√≥digo base\n",
    "        tam_lote=32,                         # ‚úì Original del c√≥digo base\n",
    "        factor_descuento=0.99,               # ‚úì Original del c√≥digo base\n",
    "        tasa_aprendizaje=1e-4,               # ‚úì Original del c√≥digo base\n",
    "        \n",
    "        # √öNICO CAMBIO: Exploraci√≥n m√°s baja (ya aprendi√≥)\n",
    "        epsilon_inicial=0.15,                # ‚¨áÔ∏è Bajo (antes 1.0, ya explor√≥)\n",
    "        epsilon_final=0.05,                  # ‚¨áÔ∏è M√°s greedy al final\n",
    "        episodios_decaimiento_eps=1000,      # ‚¨áÔ∏è R√°pido para ser greedy\n",
    "        \n",
    "        # Configuraci√≥n original\n",
    "        intervalo_actualizacion_target=1_000, # ‚úì Original\n",
    "        pasos_inicio_entrenamiento=5_000,     # ‚¨áÔ∏è Un poco menos (era 10K)\n",
    "        \n",
    "        # Guardado\n",
    "        intervalo_guardado=200,\n",
    "        intervalo_graficas=200,\n",
    "        \n",
    "        semilla_aleatoria=42,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ CONFIGURACI√ìN APLICADA:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"üìä Buffer:          100K    (‚úì Original probado)\")\n",
    "    print(\"üì¶ Batch:           32      (‚úì Original probado)\")\n",
    "    print(\"üé≤ Gamma:           0.99    (‚úì Original probado)\")\n",
    "    print(\"üìö Learning rate:   1e-4    (‚úì Original probado)\")\n",
    "    print(\"üîÑ Target update:   1000    (‚úì Original probado)\")\n",
    "    print(\"\")\n",
    "    print(\"üéØ √öNICO CAMBIO:\")\n",
    "    print(\"   Epsilon: 0.15 ‚Üí 0.05 (modelo ya aprendi√≥, explora menos)\")\n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3298b4ae",
   "metadata": {},
   "source": [
    "## üìä Monitorear el progreso\n",
    "\n",
    "Durante el entrenamiento:\n",
    "- Las gr√°ficas se guardan en `resultados_entrenamiento/dqn_continuado/`\n",
    "- Los checkpoints se guardan cada 250 episodios\n",
    "- Revisa `registro_recompensas.csv` para an√°lisis detallado\n",
    "\n",
    "Si ves que la recompensa se estabiliza o mejora: ‚úÖ Funcionando bien\n",
    "\n",
    "Si ves que vuelve a caer: ‚ö†Ô∏è Det√©n el entrenamiento y carga un checkpoint anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c649b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPCIONAL: Analizar el rendimiento del modelo mejorado\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar registro CSV\n",
    "ruta_csv = os.path.join(DIRECTORIO_NUEVO, \"registro_recompensas.csv\")\n",
    "if os.path.exists(ruta_csv):\n",
    "    df = pd.read_csv(ruta_csv)\n",
    "    \n",
    "    print(\"\\nüìä ESTAD√çSTICAS DEL ENTRENAMIENTO CONTINUO:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total episodios: {len(df)}\")\n",
    "    print(f\"Recompensa promedio: {df['recompensa'].mean():.1f}\")\n",
    "    print(f\"Recompensa m√°xima: {df['recompensa'].max():.1f}\")\n",
    "    print(f\"Recompensa m√≠nima: {df['recompensa'].min():.1f}\")\n",
    "    print(f\"Desviaci√≥n est√°ndar: {df['recompensa'].std():.1f}\")\n",
    "    print(\"\\n√öltimos 100 episodios:\")\n",
    "    print(f\"  Promedio: {df['recompensa'].tail(100).mean():.1f}\")\n",
    "    print(f\"  M√°ximo: {df['recompensa'].tail(100).max():.1f}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se encontr√≥ registro CSV en: {ruta_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
