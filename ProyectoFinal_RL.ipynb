{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "config-local-paths",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio base configurado: /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento\n",
      "Los resultados se guardarán en: /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DIRECTORIO_BASE = os.path.join(os.getcwd(), \"resultados_entrenamiento\")\n",
    "os.makedirs(DIRECTORIO_BASE, exist_ok=True)\n",
    "print(\"Directorio base configurado:\", DIRECTORIO_BASE)\n",
    "print(\"Los resultados se guardarán en:\", os.path.abspath(DIRECTORIO_BASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "environment-wrappers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidades para preprocesamiento del entorno Atari\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class EnvolturaPreprocesamiento(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Preprocesamiento personalizado de Atari:\n",
    "    - Frame skipping (salto de cuadros)\n",
    "    - Conversión a escala de grises\n",
    "    - Reescalado a 84x84 píxeles\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, entorno_base, salto_cuadros=4, tam_pantalla=84, escala_grises=True):\n",
    "        super().__init__(entorno_base)\n",
    "        self.salto_cuadros = salto_cuadros\n",
    "        self.tam_pantalla = tam_pantalla\n",
    "        self.escala_grises = escala_grises\n",
    "\n",
    "        forma_obs = (tam_pantalla, tam_pantalla)\n",
    "        if not escala_grises:\n",
    "            forma_obs += (3,)\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=forma_obs, dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def procesar_cuadro(self, cuadro):\n",
    "        \"\"\"Convierte y reescala el cuadro\"\"\"\n",
    "        if self.escala_grises:\n",
    "            cuadro = cv2.cvtColor(cuadro, cv2.COLOR_RGB2GRAY)\n",
    "            cuadro = cv2.resize(cuadro, (self.tam_pantalla, self.tam_pantalla), interpolation=cv2.INTER_AREA)\n",
    "            return cuadro\n",
    "        else:\n",
    "            cuadro = cv2.resize(cuadro, (self.tam_pantalla, self.tam_pantalla), interpolation=cv2.INTER_AREA)\n",
    "            return cuadro\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"Ejecuta múltiples pasos y acumula recompensas\"\"\"\n",
    "        recompensa_acumulada = 0.0\n",
    "        terminado = truncado = False\n",
    "        for _ in range(self.salto_cuadros):\n",
    "            observacion_raw, recomp, term, trunc, informacion = self.env.step(accion)\n",
    "            recompensa_acumulada += recomp\n",
    "            terminado |= term\n",
    "            truncado |= trunc\n",
    "            if terminado or truncado:\n",
    "                break\n",
    "        cuadro_procesado = self.procesar_cuadro(observacion_raw)\n",
    "        return cuadro_procesado, recompensa_acumulada, terminado, truncado, informacion\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observacion_raw, informacion = self.env.reset(**kwargs)\n",
    "        cuadro_procesado = self.procesar_cuadro(observacion_raw)\n",
    "        return cuadro_procesado, informacion\n",
    "\n",
    "\n",
    "class EnvolturaApilamiento(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Apila los últimos N cuadros para capturar dinámica temporal.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, entorno_base, num_apilar=4):\n",
    "        super().__init__(entorno_base)\n",
    "        self.num_apilar = num_apilar\n",
    "        self.cuadros_memoria = deque([], maxlen=num_apilar)\n",
    "        bajo = np.repeat(entorno_base.observation_space.low[np.newaxis, ...], num_apilar, axis=0)\n",
    "        alto = np.repeat(entorno_base.observation_space.high[np.newaxis, ...], num_apilar, axis=0)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=bajo.min(), high=alto.max(), dtype=entorno_base.observation_space.dtype, \n",
    "            shape=(num_apilar, *entorno_base.observation_space.shape)\n",
    "        )\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observacion, informacion = self.env.reset(**kwargs)\n",
    "        for _ in range(self.num_apilar):\n",
    "            self.cuadros_memoria.append(observacion)\n",
    "        return self._obtener_obs(), informacion\n",
    "\n",
    "    def step(self, accion):\n",
    "        observacion, recompensa, terminado, truncado, informacion = self.env.step(accion)\n",
    "        self.cuadros_memoria.append(observacion)\n",
    "        return self._obtener_obs(), recompensa, terminado, truncado, informacion\n",
    "\n",
    "    def _obtener_obs(self):\n",
    "        return np.stack(self.cuadros_memoria, axis=0)\n",
    "\n",
    "\n",
    "def crear_entorno_galaxian(semilla: int | None = None, modo_render: str | None = None):\n",
    "    \"\"\"\n",
    "    Crea el entorno ALE/Galaxian-v5 con preprocesamiento completo.\n",
    "    - Reescalado a 84x84\n",
    "    - Escala de grises\n",
    "    - Frame skip = 4\n",
    "    - Frame stack = 4\n",
    "    \"\"\"\n",
    "    entorno = gym.make(\"ALE/Galaxian-v5\", render_mode=modo_render)\n",
    "    if semilla is not None:\n",
    "        entorno.reset(seed=semilla)\n",
    "\n",
    "    entorno = EnvolturaPreprocesamiento(entorno, salto_cuadros=4, tam_pantalla=84, escala_grises=True)\n",
    "    entorno = EnvolturaApilamiento(entorno, num_apilar=4)\n",
    "\n",
    "    return entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dqn-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de DQN (Deep Q-Network) desde cero\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import Tuple, Deque, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # backend sin interfaz gráfica para guardar PNG\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Arquitectura de Red Neuronal DQN\n",
    "# ============================================================\n",
    "class RedDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Red convolucional profunda para aproximar Q(s,a).\n",
    "    Arquitectura estilo Nature DQN.\n",
    "    Formato esperado: (Batch, Canales, Alto, Ancho)\n",
    "    \"\"\"\n",
    "    def __init__(self, forma_entrada: Tuple[int, int, int], num_acciones: int):\n",
    "        super().__init__()\n",
    "        canales, alto, ancho = forma_entrada\n",
    "        self.canales_esperados = canales\n",
    "\n",
    "        self.extractor_caracteristicas = nn.Sequential(\n",
    "            nn.Conv2d(canales, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Inferir tamaño del vector aplanado\n",
    "        with torch.no_grad():\n",
    "            tensor_prueba = torch.zeros(1, canales, alto, ancho)\n",
    "            tam_aplanado = self.extractor_caracteristicas(tensor_prueba).shape[1]\n",
    "\n",
    "        self.cabeza_valores_q = nn.Sequential(\n",
    "            nn.Linear(tam_aplanado, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_acciones)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor_entrada: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        tensor_entrada: (B,C,H,W) o (B,H,W,C)\n",
    "        Retorna: Q(s,·) de forma (B, num_acciones)\n",
    "        \"\"\"\n",
    "        if tensor_entrada.ndim != 4:\n",
    "            raise ValueError(f\"Esperado tensor 4D, recibido tensor_entrada.ndim={tensor_entrada.ndim}\")\n",
    "\n",
    "        # Auto-detectar formato y permutar si es necesario\n",
    "        if tensor_entrada.shape[1] != self.canales_esperados and tensor_entrada.shape[-1] == self.canales_esperados:\n",
    "            tensor_entrada = tensor_entrada.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Normalizar a rango [0,1]\n",
    "        tensor_entrada = tensor_entrada.float() / 255.0\n",
    "        caracteristicas = self.extractor_caracteristicas(tensor_entrada)\n",
    "        return self.cabeza_valores_q(caracteristicas)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Buffer de Replay (Memoria de Experiencias)\n",
    "# ============================================================\n",
    "class BufferReplay:\n",
    "    def __init__(self, capacidad_maxima: int):\n",
    "        self.almacen: Deque = deque(maxlen=capacidad_maxima)\n",
    "\n",
    "    def agregar(self, estado, accion, recompensa, estado_sig, terminado):\n",
    "        self.almacen.append((estado, accion, recompensa, estado_sig, terminado))\n",
    "\n",
    "    def muestrear(self, tam_lote: int):\n",
    "        lote = random.sample(self.almacen, tam_lote)\n",
    "        estados, acciones, recompensas, estados_sig, terminados = map(np.array, zip(*lote))\n",
    "        return estados, acciones, recompensas, estados_sig, terminados\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.almacen)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Política de Evaluación DQN\n",
    "# ============================================================\n",
    "class PoliticaDQN:\n",
    "    def __init__(self, red_q: 'RedDQN'):\n",
    "        self.red_q = red_q.to(dispositivo)\n",
    "        self.red_q.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, observacion: np.ndarray, info: dict) -> int:\n",
    "        \"\"\"Selecciona acción greedy (argmax Q)\"\"\"\n",
    "        if observacion.ndim != 3:\n",
    "            raise ValueError(f\"Esperado obs 3D, recibido observacion.ndim={observacion.ndim}\")\n",
    "\n",
    "        obs_lote = np.expand_dims(observacion, axis=0)\n",
    "        obs_tensor = torch.from_numpy(obs_lote).to(dispositivo)\n",
    "        valores_q = self.red_q(obs_tensor)\n",
    "        return int(torch.argmax(valores_q, dim=1).item())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Funciones de Logging y Visualización\n",
    "# ============================================================\n",
    "def _calcular_media_movil(valores: List[float], ventana: int = 100):\n",
    "    \"\"\"Calcula media móvil con ventana deslizante\"\"\"\n",
    "    if len(valores) == 0:\n",
    "        return []\n",
    "    resultado = []\n",
    "    suma_acum = 0.0\n",
    "    cola = []\n",
    "    for v in valores:\n",
    "        cola.append(v)\n",
    "        suma_acum += v\n",
    "        if len(cola) > ventana:\n",
    "            suma_acum -= cola.pop(0)\n",
    "        resultado.append(suma_acum / len(cola))\n",
    "    return resultado\n",
    "\n",
    "def _guardar_grafica_y_csv(dir_checkpoints: str, recompensas: List[float], episodio: int):\n",
    "    \"\"\"Guarda gráfica PNG y registro CSV de recompensas\"\"\"\n",
    "    os.makedirs(dir_checkpoints, exist_ok=True)\n",
    "\n",
    "    # Guardar registro CSV\n",
    "    ruta_csv = os.path.join(dir_checkpoints, \"registro_recompensas.csv\")\n",
    "    archivo_nuevo = not os.path.exists(ruta_csv)\n",
    "    with open(ruta_csv, \"a\", newline=\"\") as archivo:\n",
    "        escritor = csv.writer(archivo)\n",
    "        if archivo_nuevo:\n",
    "            escritor.writerow([\"episodio\", \"recompensa\"])\n",
    "        escritor.writerow([episodio, recompensas[-1]])\n",
    "\n",
    "    # Generar gráfica con nuevos colores\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(recompensas, label=\"Recompensa por episodio\", color=\"#FF6B35\", linewidth=1.2, alpha=0.7)\n",
    "    media_movil = _calcular_media_movil(recompensas, ventana=100)\n",
    "    if len(media_movil) > 0:\n",
    "        plt.plot(media_movil, label=\"Media Móvil (100 eps)\", color=\"#00A896\", linewidth=2.5)\n",
    "    plt.xlabel(\"Episodio\", fontsize=12)\n",
    "    plt.ylabel(\"Recompensa Total\", fontsize=12)\n",
    "    plt.title(\"Progreso de Entrenamiento - DQN\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    ruta_png = os.path.join(dir_checkpoints, f\"recompensas_ep{episodio}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ruta_png, dpi=120)\n",
    "    plt.close()\n",
    "    print(f\"[REGISTRO] Gráfica y CSV guardados: {ruta_png} / {ruta_csv}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Función de Entrenamiento DQN\n",
    "# ============================================================\n",
    "def entrenar_dqn(\n",
    "    directorio_checkpoints: str,\n",
    "    episodios_totales: int = 500,\n",
    "    capacidad_replay: int = 100_000,\n",
    "    tam_lote: int = 32,\n",
    "    factor_descuento: float = 0.99,\n",
    "    tasa_aprendizaje: float = 1e-4,\n",
    "    epsilon_inicial: float = 1.0,\n",
    "    epsilon_final: float = 0.1,\n",
    "    episodios_decaimiento_eps: int = 300,\n",
    "    intervalo_actualizacion_target: int = 1_000,\n",
    "    pasos_inicio_entrenamiento: int = 10_000,\n",
    "    intervalo_guardado: int = 50,\n",
    "    intervalo_graficas: int = 50,\n",
    "    pasos_maximos_por_episodio: int | None = None,\n",
    "    semilla_aleatoria: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un agente DQN para Galaxian.\n",
    "    Guarda checkpoints y gráficas en directorio_checkpoints.\n",
    "    \"\"\"\n",
    "    os.makedirs(directorio_checkpoints, exist_ok=True)\n",
    "\n",
    "    # Crear entorno con preprocesamiento\n",
    "    entorno_juego = crear_entorno_galaxian(semilla=semilla_aleatoria, modo_render=None)\n",
    "\n",
    "    # Detectar forma de observación\n",
    "    observacion, _ = entorno_juego.reset()\n",
    "    if observacion.ndim != 3:\n",
    "        entorno_juego.close()\n",
    "        raise ValueError(f\"Observación inesperada: ndim={observacion.ndim}\")\n",
    "\n",
    "    if observacion.shape[0] in (1, 3, 4):\n",
    "        forma_entrada = (observacion.shape[0], observacion.shape[1], observacion.shape[2])\n",
    "    else:\n",
    "        forma_entrada = (observacion.shape[2], observacion.shape[0], observacion.shape[1])\n",
    "\n",
    "    num_acciones = entorno_juego.action_space.n\n",
    "\n",
    "    # Inicializar redes\n",
    "    red_q_principal = RedDQN(forma_entrada, num_acciones).to(dispositivo)\n",
    "    red_q_objetivo = RedDQN(forma_entrada, num_acciones).to(dispositivo)\n",
    "    red_q_objetivo.load_state_dict(red_q_principal.state_dict())\n",
    "    red_q_objetivo.eval()\n",
    "\n",
    "    optimizador = optim.Adam(red_q_principal.parameters(), lr=tasa_aprendizaje)\n",
    "    memoria_experiencias = BufferReplay(capacidad_replay)\n",
    "\n",
    "    pasos_globales = 0\n",
    "\n",
    "    def calcular_epsilon(ep: int) -> float:\n",
    "        if ep >= episodios_decaimiento_eps:\n",
    "            return epsilon_final\n",
    "        fraccion = ep / float(episodios_decaimiento_eps)\n",
    "        return epsilon_inicial + fraccion * (epsilon_final - epsilon_inicial)\n",
    "\n",
    "    registro_recompensas: List[float] = []\n",
    "\n",
    "    for episodio_actual in range(1, episodios_totales + 1):\n",
    "        observacion, _ = entorno_juego.reset()\n",
    "        finalizado = False\n",
    "        recompensa_total = 0.0\n",
    "        epsilon_actual = calcular_epsilon(episodio_actual)\n",
    "        pasos_en_episodio = 0\n",
    "\n",
    "        while not finalizado:\n",
    "            pasos_globales += 1\n",
    "            pasos_en_episodio += 1\n",
    "\n",
    "            # Política epsilon-greedy\n",
    "            if random.random() < epsilon_actual:\n",
    "                accion_elegida = entorno_juego.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    obs_lote = np.expand_dims(observacion, axis=0)\n",
    "                    obs_tensor = torch.from_numpy(obs_lote).to(dispositivo)\n",
    "                    valores_q = red_q_principal(obs_tensor)\n",
    "                    accion_elegida = int(torch.argmax(valores_q, dim=1).item())\n",
    "\n",
    "            siguiente_obs, recompensa_step, terminado, truncado, _ = entorno_juego.step(accion_elegida)\n",
    "            finalizado = terminado or truncado\n",
    "            recompensa_total += float(recompensa_step)\n",
    "\n",
    "            memoria_experiencias.agregar(observacion, accion_elegida, recompensa_step, siguiente_obs, finalizado)\n",
    "            observacion = siguiente_obs\n",
    "\n",
    "            # Entrenamiento cuando hay suficientes experiencias\n",
    "            if len(memoria_experiencias) >= pasos_inicio_entrenamiento:\n",
    "                estados, acciones, recompensas, estados_sig, terminados = memoria_experiencias.muestrear(tam_lote)\n",
    "\n",
    "                estados_t = torch.from_numpy(estados).to(dispositivo)\n",
    "                estados_sig_t = torch.from_numpy(estados_sig).to(dispositivo)\n",
    "                acciones_t = torch.from_numpy(acciones).long().to(dispositivo)\n",
    "                recompensas_t = torch.from_numpy(recompensas).float().to(dispositivo)\n",
    "                terminados_t = torch.from_numpy(terminados.astype(np.float32)).to(dispositivo)\n",
    "\n",
    "                # Calcular Q(s,a) actual\n",
    "                valores_q = red_q_principal(estados_t)\n",
    "                q_valores_accion = valores_q.gather(1, acciones_t.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Calcular target: r + gamma * max Q_target(s',a')\n",
    "                with torch.no_grad():\n",
    "                    q_siguientes = red_q_objetivo(estados_sig_t).max(1)[0]\n",
    "                    q_objetivo = recompensas_t + factor_descuento * q_siguientes * (1.0 - terminados_t)\n",
    "\n",
    "                perdida = nn.functional.mse_loss(q_valores_accion, q_objetivo)\n",
    "\n",
    "                optimizador.zero_grad()\n",
    "                perdida.backward()\n",
    "                nn.utils.clip_grad_norm_(red_q_principal.parameters(), 10.0)\n",
    "                optimizador.step()\n",
    "\n",
    "            # Actualizar red objetivo\n",
    "            if pasos_globales % intervalo_actualizacion_target == 0:\n",
    "                red_q_objetivo.load_state_dict(red_q_principal.state_dict())\n",
    "\n",
    "            if pasos_maximos_por_episodio is not None and pasos_en_episodio >= pasos_maximos_por_episodio:\n",
    "                break\n",
    "\n",
    "        registro_recompensas.append(recompensa_total)\n",
    "        print(f\"[DQN] Episodio {episodio_actual}/{episodios_totales} | Recompensa: {recompensa_total:.1f} | ε={epsilon_actual:.3f} | Memoria={len(memoria_experiencias)}\")\n",
    "\n",
    "        # Guardar checkpoint\n",
    "        if episodio_actual % intervalo_guardado == 0:\n",
    "            ruta_ckpt = os.path.join(directorio_checkpoints, f\"dqn_galaxian_ep{episodio_actual}.pth\")\n",
    "            torch.save({\n",
    "                \"red_q\": red_q_principal.state_dict(),\n",
    "                \"red_objetivo\": red_q_objetivo.state_dict(),\n",
    "                \"optimizador\": optimizador.state_dict(),\n",
    "                \"episodio\": episodio_actual,\n",
    "                \"pasos_globales\": pasos_globales,\n",
    "                \"recompensas\": registro_recompensas,\n",
    "                \"forma_entrada\": forma_entrada,\n",
    "                \"num_acciones\": num_acciones,\n",
    "            }, ruta_ckpt)\n",
    "            print(f\"[CHECKPOINT] Guardado en: {ruta_ckpt}\")\n",
    "\n",
    "        # Guardar gráficas\n",
    "        if episodio_actual % intervalo_graficas == 0:\n",
    "            _guardar_grafica_y_csv(directorio_checkpoints, registro_recompensas, episodio_actual)\n",
    "\n",
    "    entorno_juego.close()\n",
    "\n",
    "    # Guardar modelo final\n",
    "    ruta_final = os.path.join(directorio_checkpoints, \"dqn_galaxian_final.pth\")\n",
    "    torch.save(red_q_principal.state_dict(), ruta_final)\n",
    "    print(f\"[FINALIZADO] Modelo final guardado en: {ruta_final}\")\n",
    "\n",
    "    return red_q_principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "train-dqn",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DQN] Episodio 1/10000 | Recompensa: 850.0 | ε=0.997 | Memoria=300\n",
      "[DQN] Episodio 2/10000 | Recompensa: 1120.0 | ε=0.994 | Memoria=658\n",
      "[DQN] Episodio 3/10000 | Recompensa: 680.0 | ε=0.991 | Memoria=841\n",
      "[DQN] Episodio 4/10000 | Recompensa: 570.0 | ε=0.988 | Memoria=990\n",
      "[DQN] Episodio 5/10000 | Recompensa: 360.0 | ε=0.985 | Memoria=1060\n",
      "[DQN] Episodio 6/10000 | Recompensa: 950.0 | ε=0.982 | Memoria=1260\n",
      "[DQN] Episodio 7/10000 | Recompensa: 980.0 | ε=0.979 | Memoria=1486\n",
      "[DQN] Episodio 8/10000 | Recompensa: 800.0 | ε=0.976 | Memoria=1642\n",
      "[DQN] Episodio 9/10000 | Recompensa: 970.0 | ε=0.973 | Memoria=1856\n",
      "[DQN] Episodio 10/10000 | Recompensa: 500.0 | ε=0.970 | Memoria=1959\n",
      "[DQN] Episodio 11/10000 | Recompensa: 330.0 | ε=0.967 | Memoria=2059\n",
      "[DQN] Episodio 12/10000 | Recompensa: 240.0 | ε=0.964 | Memoria=2132\n",
      "[DQN] Episodio 13/10000 | Recompensa: 690.0 | ε=0.961 | Memoria=2312\n",
      "[DQN] Episodio 14/10000 | Recompensa: 720.0 | ε=0.958 | Memoria=2554\n",
      "[DQN] Episodio 15/10000 | Recompensa: 520.0 | ε=0.955 | Memoria=2692\n",
      "[DQN] Episodio 16/10000 | Recompensa: 770.0 | ε=0.952 | Memoria=2830\n",
      "[DQN] Episodio 17/10000 | Recompensa: 410.0 | ε=0.949 | Memoria=2945\n",
      "[DQN] Episodio 18/10000 | Recompensa: 210.0 | ε=0.946 | Memoria=3016\n",
      "[DQN] Episodio 19/10000 | Recompensa: 850.0 | ε=0.943 | Memoria=3186\n",
      "[DQN] Episodio 20/10000 | Recompensa: 1290.0 | ε=0.940 | Memoria=3424\n",
      "[DQN] Episodio 21/10000 | Recompensa: 540.0 | ε=0.937 | Memoria=3545\n",
      "[DQN] Episodio 22/10000 | Recompensa: 520.0 | ε=0.934 | Memoria=3686\n",
      "[DQN] Episodio 23/10000 | Recompensa: 570.0 | ε=0.931 | Memoria=3806\n",
      "[DQN] Episodio 24/10000 | Recompensa: 520.0 | ε=0.928 | Memoria=3986\n",
      "[DQN] Episodio 25/10000 | Recompensa: 300.0 | ε=0.925 | Memoria=4133\n",
      "[DQN] Episodio 26/10000 | Recompensa: 330.0 | ε=0.922 | Memoria=4243\n",
      "[DQN] Episodio 27/10000 | Recompensa: 480.0 | ε=0.919 | Memoria=4372\n",
      "[DQN] Episodio 28/10000 | Recompensa: 810.0 | ε=0.916 | Memoria=4603\n",
      "[DQN] Episodio 29/10000 | Recompensa: 640.0 | ε=0.913 | Memoria=4829\n",
      "[DQN] Episodio 30/10000 | Recompensa: 530.0 | ε=0.910 | Memoria=4999\n",
      "[DQN] Episodio 31/10000 | Recompensa: 320.0 | ε=0.907 | Memoria=5126\n",
      "[DQN] Episodio 32/10000 | Recompensa: 550.0 | ε=0.904 | Memoria=5259\n",
      "[DQN] Episodio 33/10000 | Recompensa: 490.0 | ε=0.901 | Memoria=5473\n",
      "[DQN] Episodio 34/10000 | Recompensa: 240.0 | ε=0.898 | Memoria=5559\n",
      "[DQN] Episodio 35/10000 | Recompensa: 420.0 | ε=0.895 | Memoria=5685\n",
      "[DQN] Episodio 36/10000 | Recompensa: 920.0 | ε=0.892 | Memoria=5987\n",
      "[DQN] Episodio 37/10000 | Recompensa: 670.0 | ε=0.889 | Memoria=6154\n",
      "[DQN] Episodio 38/10000 | Recompensa: 330.0 | ε=0.886 | Memoria=6218\n",
      "[DQN] Episodio 39/10000 | Recompensa: 860.0 | ε=0.883 | Memoria=6426\n",
      "[DQN] Episodio 40/10000 | Recompensa: 590.0 | ε=0.880 | Memoria=6614\n",
      "[DQN] Episodio 41/10000 | Recompensa: 710.0 | ε=0.877 | Memoria=6789\n",
      "[DQN] Episodio 42/10000 | Recompensa: 390.0 | ε=0.874 | Memoria=6883\n",
      "[DQN] Episodio 43/10000 | Recompensa: 450.0 | ε=0.871 | Memoria=6984\n",
      "[DQN] Episodio 44/10000 | Recompensa: 480.0 | ε=0.868 | Memoria=7164\n",
      "[DQN] Episodio 45/10000 | Recompensa: 840.0 | ε=0.865 | Memoria=7472\n",
      "[DQN] Episodio 46/10000 | Recompensa: 390.0 | ε=0.862 | Memoria=7601\n",
      "[DQN] Episodio 47/10000 | Recompensa: 840.0 | ε=0.859 | Memoria=7793\n",
      "[DQN] Episodio 48/10000 | Recompensa: 690.0 | ε=0.856 | Memoria=7990\n",
      "[DQN] Episodio 49/10000 | Recompensa: 510.0 | ε=0.853 | Memoria=8148\n",
      "[DQN] Episodio 50/10000 | Recompensa: 400.0 | ε=0.850 | Memoria=8250\n",
      "[DQN] Episodio 51/10000 | Recompensa: 240.0 | ε=0.847 | Memoria=8429\n",
      "[DQN] Episodio 52/10000 | Recompensa: 610.0 | ε=0.844 | Memoria=8588\n",
      "[DQN] Episodio 53/10000 | Recompensa: 750.0 | ε=0.841 | Memoria=8820\n",
      "[DQN] Episodio 54/10000 | Recompensa: 550.0 | ε=0.838 | Memoria=9016\n",
      "[DQN] Episodio 55/10000 | Recompensa: 1280.0 | ε=0.835 | Memoria=9299\n",
      "[DQN] Episodio 56/10000 | Recompensa: 450.0 | ε=0.832 | Memoria=9425\n",
      "[DQN] Episodio 57/10000 | Recompensa: 830.0 | ε=0.829 | Memoria=9636\n",
      "[DQN] Episodio 58/10000 | Recompensa: 670.0 | ε=0.826 | Memoria=9804\n",
      "[DQN] Episodio 59/10000 | Recompensa: 360.0 | ε=0.823 | Memoria=9930\n",
      "[DQN] Episodio 60/10000 | Recompensa: 390.0 | ε=0.820 | Memoria=10042\n",
      "[DQN] Episodio 61/10000 | Recompensa: 1050.0 | ε=0.817 | Memoria=10358\n",
      "[DQN] Episodio 62/10000 | Recompensa: 460.0 | ε=0.814 | Memoria=10518\n",
      "[DQN] Episodio 63/10000 | Recompensa: 330.0 | ε=0.811 | Memoria=10615\n",
      "[DQN] Episodio 64/10000 | Recompensa: 940.0 | ε=0.808 | Memoria=10852\n",
      "[DQN] Episodio 65/10000 | Recompensa: 340.0 | ε=0.805 | Memoria=10953\n",
      "[DQN] Episodio 66/10000 | Recompensa: 1110.0 | ε=0.802 | Memoria=11216\n",
      "[DQN] Episodio 67/10000 | Recompensa: 520.0 | ε=0.799 | Memoria=11346\n",
      "[DQN] Episodio 68/10000 | Recompensa: 900.0 | ε=0.796 | Memoria=11545\n",
      "[DQN] Episodio 69/10000 | Recompensa: 630.0 | ε=0.793 | Memoria=11673\n",
      "[DQN] Episodio 70/10000 | Recompensa: 280.0 | ε=0.790 | Memoria=11781\n",
      "[DQN] Episodio 71/10000 | Recompensa: 420.0 | ε=0.787 | Memoria=11902\n",
      "[DQN] Episodio 72/10000 | Recompensa: 360.0 | ε=0.784 | Memoria=12008\n",
      "[DQN] Episodio 73/10000 | Recompensa: 480.0 | ε=0.781 | Memoria=12129\n",
      "[DQN] Episodio 74/10000 | Recompensa: 540.0 | ε=0.778 | Memoria=12236\n",
      "[DQN] Episodio 75/10000 | Recompensa: 390.0 | ε=0.775 | Memoria=12314\n",
      "[DQN] Episodio 76/10000 | Recompensa: 210.0 | ε=0.772 | Memoria=12386\n",
      "[DQN] Episodio 77/10000 | Recompensa: 1140.0 | ε=0.769 | Memoria=12679\n",
      "[DQN] Episodio 78/10000 | Recompensa: 520.0 | ε=0.766 | Memoria=12806\n",
      "[DQN] Episodio 79/10000 | Recompensa: 610.0 | ε=0.763 | Memoria=12944\n",
      "[DQN] Episodio 80/10000 | Recompensa: 860.0 | ε=0.760 | Memoria=13101\n",
      "[DQN] Episodio 81/10000 | Recompensa: 540.0 | ε=0.757 | Memoria=13241\n",
      "[DQN] Episodio 82/10000 | Recompensa: 720.0 | ε=0.754 | Memoria=13362\n",
      "[DQN] Episodio 83/10000 | Recompensa: 1150.0 | ε=0.751 | Memoria=13614\n",
      "[DQN] Episodio 84/10000 | Recompensa: 770.0 | ε=0.748 | Memoria=13786\n",
      "[DQN] Episodio 85/10000 | Recompensa: 900.0 | ε=0.745 | Memoria=13976\n",
      "[DQN] Episodio 86/10000 | Recompensa: 760.0 | ε=0.742 | Memoria=14125\n",
      "[DQN] Episodio 87/10000 | Recompensa: 800.0 | ε=0.739 | Memoria=14304\n",
      "[DQN] Episodio 88/10000 | Recompensa: 750.0 | ε=0.736 | Memoria=14459\n",
      "[DQN] Episodio 89/10000 | Recompensa: 330.0 | ε=0.733 | Memoria=14520\n",
      "[DQN] Episodio 90/10000 | Recompensa: 540.0 | ε=0.730 | Memoria=14642\n",
      "[DQN] Episodio 91/10000 | Recompensa: 300.0 | ε=0.727 | Memoria=14733\n",
      "[DQN] Episodio 92/10000 | Recompensa: 700.0 | ε=0.724 | Memoria=14927\n",
      "[DQN] Episodio 93/10000 | Recompensa: 410.0 | ε=0.721 | Memoria=15036\n",
      "[DQN] Episodio 94/10000 | Recompensa: 620.0 | ε=0.718 | Memoria=15161\n",
      "[DQN] Episodio 95/10000 | Recompensa: 360.0 | ε=0.715 | Memoria=15246\n",
      "[DQN] Episodio 96/10000 | Recompensa: 540.0 | ε=0.712 | Memoria=15365\n",
      "[DQN] Episodio 97/10000 | Recompensa: 300.0 | ε=0.709 | Memoria=15448\n",
      "[DQN] Episodio 98/10000 | Recompensa: 890.0 | ε=0.706 | Memoria=15651\n",
      "[DQN] Episodio 99/10000 | Recompensa: 480.0 | ε=0.703 | Memoria=15772\n",
      "[DQN] Episodio 100/10000 | Recompensa: 730.0 | ε=0.700 | Memoria=15918\n",
      "[DQN] Episodio 101/10000 | Recompensa: 1130.0 | ε=0.697 | Memoria=16114\n",
      "[DQN] Episodio 102/10000 | Recompensa: 440.0 | ε=0.694 | Memoria=16219\n",
      "[DQN] Episodio 103/10000 | Recompensa: 610.0 | ε=0.691 | Memoria=16322\n",
      "[DQN] Episodio 104/10000 | Recompensa: 660.0 | ε=0.688 | Memoria=16434\n",
      "[DQN] Episodio 105/10000 | Recompensa: 430.0 | ε=0.685 | Memoria=16524\n",
      "[DQN] Episodio 106/10000 | Recompensa: 550.0 | ε=0.682 | Memoria=16630\n",
      "[DQN] Episodio 107/10000 | Recompensa: 810.0 | ε=0.679 | Memoria=16809\n",
      "[DQN] Episodio 108/10000 | Recompensa: 610.0 | ε=0.676 | Memoria=16951\n",
      "[DQN] Episodio 109/10000 | Recompensa: 1190.0 | ε=0.673 | Memoria=17226\n",
      "[DQN] Episodio 110/10000 | Recompensa: 300.0 | ε=0.670 | Memoria=17318\n",
      "[DQN] Episodio 111/10000 | Recompensa: 400.0 | ε=0.667 | Memoria=17438\n",
      "[DQN] Episodio 112/10000 | Recompensa: 1900.0 | ε=0.664 | Memoria=17786\n",
      "[DQN] Episodio 113/10000 | Recompensa: 450.0 | ε=0.661 | Memoria=17895\n",
      "[DQN] Episodio 114/10000 | Recompensa: 1140.0 | ε=0.658 | Memoria=18074\n",
      "[DQN] Episodio 115/10000 | Recompensa: 1070.0 | ε=0.655 | Memoria=18259\n",
      "[DQN] Episodio 116/10000 | Recompensa: 330.0 | ε=0.652 | Memoria=18346\n",
      "[DQN] Episodio 117/10000 | Recompensa: 1110.0 | ε=0.649 | Memoria=18531\n",
      "[DQN] Episodio 118/10000 | Recompensa: 630.0 | ε=0.646 | Memoria=18680\n",
      "[DQN] Episodio 119/10000 | Recompensa: 330.0 | ε=0.643 | Memoria=18761\n",
      "[DQN] Episodio 120/10000 | Recompensa: 840.0 | ε=0.640 | Memoria=18891\n",
      "[DQN] Episodio 121/10000 | Recompensa: 240.0 | ε=0.637 | Memoria=18958\n",
      "[DQN] Episodio 122/10000 | Recompensa: 300.0 | ε=0.634 | Memoria=19026\n",
      "[DQN] Episodio 123/10000 | Recompensa: 180.0 | ε=0.631 | Memoria=19098\n",
      "[DQN] Episodio 124/10000 | Recompensa: 390.0 | ε=0.628 | Memoria=19172\n",
      "[DQN] Episodio 125/10000 | Recompensa: 420.0 | ε=0.625 | Memoria=19323\n",
      "[DQN] Episodio 126/10000 | Recompensa: 210.0 | ε=0.622 | Memoria=19395\n",
      "[DQN] Episodio 127/10000 | Recompensa: 510.0 | ε=0.619 | Memoria=19476\n",
      "[DQN] Episodio 128/10000 | Recompensa: 310.0 | ε=0.616 | Memoria=19578\n",
      "[DQN] Episodio 129/10000 | Recompensa: 970.0 | ε=0.613 | Memoria=19776\n",
      "[DQN] Episodio 130/10000 | Recompensa: 660.0 | ε=0.610 | Memoria=19896\n",
      "[DQN] Episodio 131/10000 | Recompensa: 710.0 | ε=0.607 | Memoria=20001\n",
      "[DQN] Episodio 132/10000 | Recompensa: 870.0 | ε=0.604 | Memoria=20151\n",
      "[DQN] Episodio 133/10000 | Recompensa: 360.0 | ε=0.601 | Memoria=20230\n",
      "[DQN] Episodio 134/10000 | Recompensa: 490.0 | ε=0.598 | Memoria=20335\n",
      "[DQN] Episodio 135/10000 | Recompensa: 490.0 | ε=0.595 | Memoria=20425\n",
      "[DQN] Episodio 136/10000 | Recompensa: 320.0 | ε=0.592 | Memoria=20492\n",
      "[DQN] Episodio 137/10000 | Recompensa: 920.0 | ε=0.589 | Memoria=20628\n",
      "[DQN] Episodio 138/10000 | Recompensa: 490.0 | ε=0.586 | Memoria=20710\n",
      "[DQN] Episodio 139/10000 | Recompensa: 1110.0 | ε=0.583 | Memoria=20881\n",
      "[DQN] Episodio 140/10000 | Recompensa: 810.0 | ε=0.580 | Memoria=21028\n",
      "[DQN] Episodio 141/10000 | Recompensa: 500.0 | ε=0.577 | Memoria=21132\n",
      "[DQN] Episodio 142/10000 | Recompensa: 770.0 | ε=0.574 | Memoria=21269\n",
      "[DQN] Episodio 143/10000 | Recompensa: 450.0 | ε=0.571 | Memoria=21336\n",
      "[DQN] Episodio 144/10000 | Recompensa: 600.0 | ε=0.568 | Memoria=21450\n",
      "[DQN] Episodio 145/10000 | Recompensa: 700.0 | ε=0.565 | Memoria=21582\n",
      "[DQN] Episodio 146/10000 | Recompensa: 340.0 | ε=0.562 | Memoria=21669\n",
      "[DQN] Episodio 147/10000 | Recompensa: 1020.0 | ε=0.559 | Memoria=21817\n",
      "[DQN] Episodio 148/10000 | Recompensa: 410.0 | ε=0.556 | Memoria=21907\n",
      "[DQN] Episodio 149/10000 | Recompensa: 1040.0 | ε=0.553 | Memoria=22092\n",
      "[DQN] Episodio 150/10000 | Recompensa: 990.0 | ε=0.550 | Memoria=22290\n",
      "[DQN] Episodio 151/10000 | Recompensa: 620.0 | ε=0.547 | Memoria=22441\n",
      "[DQN] Episodio 152/10000 | Recompensa: 1050.0 | ε=0.544 | Memoria=22627\n",
      "[DQN] Episodio 153/10000 | Recompensa: 390.0 | ε=0.541 | Memoria=22730\n",
      "[DQN] Episodio 154/10000 | Recompensa: 210.0 | ε=0.538 | Memoria=22802\n",
      "[DQN] Episodio 155/10000 | Recompensa: 300.0 | ε=0.535 | Memoria=22870\n",
      "[DQN] Episodio 156/10000 | Recompensa: 870.0 | ε=0.532 | Memoria=23007\n",
      "[DQN] Episodio 157/10000 | Recompensa: 450.0 | ε=0.529 | Memoria=23086\n",
      "[DQN] Episodio 158/10000 | Recompensa: 600.0 | ε=0.526 | Memoria=23211\n",
      "[DQN] Episodio 159/10000 | Recompensa: 760.0 | ε=0.523 | Memoria=23375\n",
      "[DQN] Episodio 160/10000 | Recompensa: 330.0 | ε=0.520 | Memoria=23465\n",
      "[DQN] Episodio 161/10000 | Recompensa: 1130.0 | ε=0.517 | Memoria=23741\n",
      "[DQN] Episodio 162/10000 | Recompensa: 1040.0 | ε=0.514 | Memoria=23981\n",
      "[DQN] Episodio 163/10000 | Recompensa: 1320.0 | ε=0.511 | Memoria=24234\n",
      "[DQN] Episodio 164/10000 | Recompensa: 1080.0 | ε=0.508 | Memoria=24421\n",
      "[DQN] Episodio 165/10000 | Recompensa: 440.0 | ε=0.505 | Memoria=24507\n",
      "[DQN] Episodio 166/10000 | Recompensa: 530.0 | ε=0.502 | Memoria=24613\n",
      "[DQN] Episodio 167/10000 | Recompensa: 690.0 | ε=0.499 | Memoria=24771\n",
      "[DQN] Episodio 168/10000 | Recompensa: 510.0 | ε=0.496 | Memoria=24857\n",
      "[DQN] Episodio 169/10000 | Recompensa: 1020.0 | ε=0.493 | Memoria=24995\n",
      "[DQN] Episodio 170/10000 | Recompensa: 670.0 | ε=0.490 | Memoria=25156\n",
      "[DQN] Episodio 171/10000 | Recompensa: 980.0 | ε=0.487 | Memoria=25312\n",
      "[DQN] Episodio 172/10000 | Recompensa: 520.0 | ε=0.484 | Memoria=25398\n",
      "[DQN] Episodio 173/10000 | Recompensa: 800.0 | ε=0.481 | Memoria=25582\n",
      "[DQN] Episodio 174/10000 | Recompensa: 990.0 | ε=0.478 | Memoria=25902\n",
      "[DQN] Episodio 175/10000 | Recompensa: 970.0 | ε=0.475 | Memoria=26072\n",
      "[DQN] Episodio 176/10000 | Recompensa: 660.0 | ε=0.472 | Memoria=26185\n",
      "[DQN] Episodio 177/10000 | Recompensa: 180.0 | ε=0.469 | Memoria=26256\n",
      "[DQN] Episodio 178/10000 | Recompensa: 480.0 | ε=0.466 | Memoria=26342\n",
      "[DQN] Episodio 179/10000 | Recompensa: 580.0 | ε=0.463 | Memoria=26438\n",
      "[DQN] Episodio 180/10000 | Recompensa: 390.0 | ε=0.460 | Memoria=26520\n",
      "[DQN] Episodio 181/10000 | Recompensa: 540.0 | ε=0.457 | Memoria=26645\n",
      "[DQN] Episodio 182/10000 | Recompensa: 910.0 | ε=0.454 | Memoria=26866\n",
      "[DQN] Episodio 183/10000 | Recompensa: 310.0 | ε=0.451 | Memoria=26958\n",
      "[DQN] Episodio 184/10000 | Recompensa: 430.0 | ε=0.448 | Memoria=27093\n",
      "[DQN] Episodio 185/10000 | Recompensa: 510.0 | ε=0.445 | Memoria=27218\n",
      "[DQN] Episodio 186/10000 | Recompensa: 1090.0 | ε=0.442 | Memoria=27449\n",
      "[DQN] Episodio 187/10000 | Recompensa: 990.0 | ε=0.439 | Memoria=27585\n",
      "[DQN] Episodio 188/10000 | Recompensa: 830.0 | ε=0.436 | Memoria=27758\n",
      "[DQN] Episodio 189/10000 | Recompensa: 1770.0 | ε=0.433 | Memoria=28093\n",
      "[DQN] Episodio 190/10000 | Recompensa: 870.0 | ε=0.430 | Memoria=28240\n",
      "[DQN] Episodio 191/10000 | Recompensa: 1380.0 | ε=0.427 | Memoria=28463\n",
      "[DQN] Episodio 192/10000 | Recompensa: 430.0 | ε=0.424 | Memoria=28564\n",
      "[DQN] Episodio 193/10000 | Recompensa: 1310.0 | ε=0.421 | Memoria=28835\n",
      "[DQN] Episodio 194/10000 | Recompensa: 730.0 | ε=0.418 | Memoria=28959\n",
      "[DQN] Episodio 195/10000 | Recompensa: 510.0 | ε=0.415 | Memoria=29068\n",
      "[DQN] Episodio 196/10000 | Recompensa: 1480.0 | ε=0.412 | Memoria=29336\n",
      "[DQN] Episodio 197/10000 | Recompensa: 240.0 | ε=0.409 | Memoria=29408\n",
      "[DQN] Episodio 198/10000 | Recompensa: 620.0 | ε=0.406 | Memoria=29525\n",
      "[DQN] Episodio 199/10000 | Recompensa: 1190.0 | ε=0.403 | Memoria=29881\n",
      "[DQN] Episodio 200/10000 | Recompensa: 180.0 | ε=0.400 | Memoria=29956\n",
      "[DQN] Episodio 201/10000 | Recompensa: 360.0 | ε=0.397 | Memoria=30053\n",
      "[DQN] Episodio 202/10000 | Recompensa: 860.0 | ε=0.394 | Memoria=30176\n",
      "[DQN] Episodio 203/10000 | Recompensa: 740.0 | ε=0.391 | Memoria=30301\n",
      "[DQN] Episodio 204/10000 | Recompensa: 210.0 | ε=0.388 | Memoria=30373\n",
      "[DQN] Episodio 205/10000 | Recompensa: 980.0 | ε=0.385 | Memoria=30563\n",
      "[DQN] Episodio 206/10000 | Recompensa: 1520.0 | ε=0.382 | Memoria=30805\n",
      "[DQN] Episodio 207/10000 | Recompensa: 850.0 | ε=0.379 | Memoria=30948\n",
      "[DQN] Episodio 208/10000 | Recompensa: 460.0 | ε=0.376 | Memoria=31025\n",
      "[DQN] Episodio 209/10000 | Recompensa: 780.0 | ε=0.373 | Memoria=31136\n",
      "[DQN] Episodio 210/10000 | Recompensa: 700.0 | ε=0.370 | Memoria=31282\n",
      "[DQN] Episodio 211/10000 | Recompensa: 1050.0 | ε=0.367 | Memoria=31433\n",
      "[DQN] Episodio 212/10000 | Recompensa: 890.0 | ε=0.364 | Memoria=31629\n",
      "[DQN] Episodio 213/10000 | Recompensa: 360.0 | ε=0.361 | Memoria=31700\n",
      "[DQN] Episodio 214/10000 | Recompensa: 1330.0 | ε=0.358 | Memoria=31884\n",
      "[DQN] Episodio 215/10000 | Recompensa: 490.0 | ε=0.355 | Memoria=31963\n",
      "[DQN] Episodio 216/10000 | Recompensa: 600.0 | ε=0.352 | Memoria=32079\n",
      "[DQN] Episodio 217/10000 | Recompensa: 1420.0 | ε=0.349 | Memoria=32488\n",
      "[DQN] Episodio 218/10000 | Recompensa: 510.0 | ε=0.346 | Memoria=32602\n",
      "[DQN] Episodio 219/10000 | Recompensa: 1240.0 | ε=0.343 | Memoria=32799\n",
      "[DQN] Episodio 220/10000 | Recompensa: 1240.0 | ε=0.340 | Memoria=33009\n",
      "[DQN] Episodio 221/10000 | Recompensa: 590.0 | ε=0.337 | Memoria=33148\n",
      "[DQN] Episodio 222/10000 | Recompensa: 930.0 | ε=0.334 | Memoria=33325\n",
      "[DQN] Episodio 223/10000 | Recompensa: 300.0 | ε=0.331 | Memoria=33397\n",
      "[DQN] Episodio 224/10000 | Recompensa: 360.0 | ε=0.328 | Memoria=33501\n",
      "[DQN] Episodio 225/10000 | Recompensa: 540.0 | ε=0.325 | Memoria=33600\n",
      "[DQN] Episodio 226/10000 | Recompensa: 240.0 | ε=0.322 | Memoria=33672\n",
      "[DQN] Episodio 227/10000 | Recompensa: 870.0 | ε=0.319 | Memoria=33850\n",
      "[DQN] Episodio 228/10000 | Recompensa: 960.0 | ε=0.316 | Memoria=33998\n",
      "[DQN] Episodio 229/10000 | Recompensa: 800.0 | ε=0.313 | Memoria=34122\n",
      "[DQN] Episodio 230/10000 | Recompensa: 610.0 | ε=0.310 | Memoria=34215\n",
      "[DQN] Episodio 231/10000 | Recompensa: 840.0 | ε=0.307 | Memoria=34386\n",
      "[DQN] Episodio 232/10000 | Recompensa: 800.0 | ε=0.304 | Memoria=34533\n",
      "[DQN] Episodio 233/10000 | Recompensa: 580.0 | ε=0.301 | Memoria=34680\n",
      "[DQN] Episodio 234/10000 | Recompensa: 580.0 | ε=0.298 | Memoria=34806\n",
      "[DQN] Episodio 235/10000 | Recompensa: 970.0 | ε=0.295 | Memoria=34994\n",
      "[DQN] Episodio 236/10000 | Recompensa: 1390.0 | ε=0.292 | Memoria=35379\n",
      "[DQN] Episodio 237/10000 | Recompensa: 860.0 | ε=0.289 | Memoria=35564\n",
      "[DQN] Episodio 238/10000 | Recompensa: 1170.0 | ε=0.286 | Memoria=35863\n",
      "[DQN] Episodio 239/10000 | Recompensa: 840.0 | ε=0.283 | Memoria=36011\n",
      "[DQN] Episodio 240/10000 | Recompensa: 480.0 | ε=0.280 | Memoria=36113\n",
      "[DQN] Episodio 241/10000 | Recompensa: 530.0 | ε=0.277 | Memoria=36206\n",
      "[DQN] Episodio 242/10000 | Recompensa: 300.0 | ε=0.274 | Memoria=36291\n",
      "[DQN] Episodio 243/10000 | Recompensa: 1610.0 | ε=0.271 | Memoria=36617\n",
      "[DQN] Episodio 244/10000 | Recompensa: 1000.0 | ε=0.268 | Memoria=36836\n",
      "[DQN] Episodio 245/10000 | Recompensa: 1340.0 | ε=0.265 | Memoria=37058\n",
      "[DQN] Episodio 246/10000 | Recompensa: 270.0 | ε=0.262 | Memoria=37129\n",
      "[DQN] Episodio 247/10000 | Recompensa: 680.0 | ε=0.259 | Memoria=37249\n",
      "[DQN] Episodio 248/10000 | Recompensa: 930.0 | ε=0.256 | Memoria=37503\n",
      "[DQN] Episodio 249/10000 | Recompensa: 750.0 | ε=0.253 | Memoria=37641\n",
      "[DQN] Episodio 250/10000 | Recompensa: 420.0 | ε=0.250 | Memoria=37718\n",
      "[DQN] Episodio 251/10000 | Recompensa: 400.0 | ε=0.247 | Memoria=37837\n",
      "[DQN] Episodio 252/10000 | Recompensa: 450.0 | ε=0.244 | Memoria=37932\n",
      "[DQN] Episodio 253/10000 | Recompensa: 660.0 | ε=0.241 | Memoria=38078\n",
      "[DQN] Episodio 254/10000 | Recompensa: 870.0 | ε=0.238 | Memoria=38257\n",
      "[DQN] Episodio 255/10000 | Recompensa: 1020.0 | ε=0.235 | Memoria=38395\n",
      "[DQN] Episodio 256/10000 | Recompensa: 440.0 | ε=0.232 | Memoria=38477\n",
      "[DQN] Episodio 257/10000 | Recompensa: 740.0 | ε=0.229 | Memoria=38609\n",
      "[DQN] Episodio 258/10000 | Recompensa: 1430.0 | ε=0.226 | Memoria=38811\n",
      "[DQN] Episodio 259/10000 | Recompensa: 920.0 | ε=0.223 | Memoria=38944\n",
      "[DQN] Episodio 260/10000 | Recompensa: 790.0 | ε=0.220 | Memoria=39130\n",
      "[DQN] Episodio 261/10000 | Recompensa: 1020.0 | ε=0.217 | Memoria=39278\n",
      "[DQN] Episodio 262/10000 | Recompensa: 850.0 | ε=0.214 | Memoria=39396\n",
      "[DQN] Episodio 263/10000 | Recompensa: 1220.0 | ε=0.211 | Memoria=39608\n",
      "[DQN] Episodio 264/10000 | Recompensa: 540.0 | ε=0.208 | Memoria=39690\n",
      "[DQN] Episodio 265/10000 | Recompensa: 270.0 | ε=0.205 | Memoria=39755\n",
      "[DQN] Episodio 266/10000 | Recompensa: 1410.0 | ε=0.202 | Memoria=40042\n",
      "[DQN] Episodio 267/10000 | Recompensa: 820.0 | ε=0.199 | Memoria=40205\n",
      "[DQN] Episodio 268/10000 | Recompensa: 950.0 | ε=0.196 | Memoria=40347\n",
      "[DQN] Episodio 269/10000 | Recompensa: 510.0 | ε=0.193 | Memoria=40470\n",
      "[DQN] Episodio 270/10000 | Recompensa: 950.0 | ε=0.190 | Memoria=40653\n",
      "[DQN] Episodio 271/10000 | Recompensa: 430.0 | ε=0.187 | Memoria=40794\n",
      "[DQN] Episodio 272/10000 | Recompensa: 900.0 | ε=0.184 | Memoria=40960\n",
      "[DQN] Episodio 273/10000 | Recompensa: 900.0 | ε=0.181 | Memoria=41128\n",
      "[DQN] Episodio 274/10000 | Recompensa: 520.0 | ε=0.178 | Memoria=41214\n",
      "[DQN] Episodio 275/10000 | Recompensa: 680.0 | ε=0.175 | Memoria=41321\n",
      "[DQN] Episodio 276/10000 | Recompensa: 310.0 | ε=0.172 | Memoria=41393\n",
      "[DQN] Episodio 277/10000 | Recompensa: 1200.0 | ε=0.169 | Memoria=41610\n",
      "[DQN] Episodio 278/10000 | Recompensa: 1020.0 | ε=0.166 | Memoria=41728\n",
      "[DQN] Episodio 279/10000 | Recompensa: 1310.0 | ε=0.163 | Memoria=42037\n",
      "[DQN] Episodio 280/10000 | Recompensa: 890.0 | ε=0.160 | Memoria=42234\n",
      "[DQN] Episodio 281/10000 | Recompensa: 1220.0 | ε=0.157 | Memoria=42471\n",
      "[DQN] Episodio 282/10000 | Recompensa: 1060.0 | ε=0.154 | Memoria=42634\n",
      "[DQN] Episodio 283/10000 | Recompensa: 570.0 | ε=0.151 | Memoria=42729\n",
      "[DQN] Episodio 284/10000 | Recompensa: 830.0 | ε=0.148 | Memoria=42855\n",
      "[DQN] Episodio 285/10000 | Recompensa: 1210.0 | ε=0.145 | Memoria=43075\n",
      "[DQN] Episodio 286/10000 | Recompensa: 1220.0 | ε=0.142 | Memoria=43287\n",
      "[DQN] Episodio 287/10000 | Recompensa: 480.0 | ε=0.139 | Memoria=43422\n",
      "[DQN] Episodio 288/10000 | Recompensa: 750.0 | ε=0.136 | Memoria=43540\n",
      "[DQN] Episodio 289/10000 | Recompensa: 1320.0 | ε=0.133 | Memoria=43756\n",
      "[DQN] Episodio 290/10000 | Recompensa: 270.0 | ε=0.130 | Memoria=43853\n",
      "[DQN] Episodio 291/10000 | Recompensa: 580.0 | ε=0.127 | Memoria=43973\n",
      "[DQN] Episodio 292/10000 | Recompensa: 730.0 | ε=0.124 | Memoria=44144\n",
      "[DQN] Episodio 293/10000 | Recompensa: 530.0 | ε=0.121 | Memoria=44232\n",
      "[DQN] Episodio 294/10000 | Recompensa: 700.0 | ε=0.118 | Memoria=44407\n",
      "[DQN] Episodio 295/10000 | Recompensa: 880.0 | ε=0.115 | Memoria=44520\n",
      "[DQN] Episodio 296/10000 | Recompensa: 650.0 | ε=0.112 | Memoria=44625\n",
      "[DQN] Episodio 297/10000 | Recompensa: 1910.0 | ε=0.109 | Memoria=44950\n",
      "[DQN] Episodio 298/10000 | Recompensa: 330.0 | ε=0.106 | Memoria=45028\n",
      "[DQN] Episodio 299/10000 | Recompensa: 1040.0 | ε=0.103 | Memoria=45236\n",
      "[DQN] Episodio 300/10000 | Recompensa: 1450.0 | ε=0.100 | Memoria=45475\n",
      "[DQN] Episodio 301/10000 | Recompensa: 520.0 | ε=0.100 | Memoria=45571\n",
      "[DQN] Episodio 302/10000 | Recompensa: 1050.0 | ε=0.100 | Memoria=45748\n",
      "[DQN] Episodio 303/10000 | Recompensa: 620.0 | ε=0.100 | Memoria=45883\n",
      "[DQN] Episodio 304/10000 | Recompensa: 920.0 | ε=0.100 | Memoria=46101\n",
      "[DQN] Episodio 305/10000 | Recompensa: 920.0 | ε=0.100 | Memoria=46248\n",
      "[DQN] Episodio 306/10000 | Recompensa: 490.0 | ε=0.100 | Memoria=46340\n",
      "[DQN] Episodio 307/10000 | Recompensa: 600.0 | ε=0.100 | Memoria=46443\n",
      "[DQN] Episodio 308/10000 | Recompensa: 2470.0 | ε=0.100 | Memoria=46667\n",
      "[DQN] Episodio 309/10000 | Recompensa: 1550.0 | ε=0.100 | Memoria=46982\n",
      "[DQN] Episodio 310/10000 | Recompensa: 990.0 | ε=0.100 | Memoria=47180\n",
      "[DQN] Episodio 311/10000 | Recompensa: 780.0 | ε=0.100 | Memoria=47351\n",
      "[DQN] Episodio 312/10000 | Recompensa: 920.0 | ε=0.100 | Memoria=47501\n",
      "[DQN] Episodio 313/10000 | Recompensa: 900.0 | ε=0.100 | Memoria=47641\n",
      "[DQN] Episodio 314/10000 | Recompensa: 420.0 | ε=0.100 | Memoria=47742\n",
      "[DQN] Episodio 315/10000 | Recompensa: 560.0 | ε=0.100 | Memoria=47837\n",
      "[DQN] Episodio 316/10000 | Recompensa: 990.0 | ε=0.100 | Memoria=48055\n",
      "[DQN] Episodio 317/10000 | Recompensa: 270.0 | ε=0.100 | Memoria=48126\n",
      "[DQN] Episodio 318/10000 | Recompensa: 850.0 | ε=0.100 | Memoria=48278\n",
      "[DQN] Episodio 319/10000 | Recompensa: 1130.0 | ε=0.100 | Memoria=48445\n",
      "[DQN] Episodio 320/10000 | Recompensa: 400.0 | ε=0.100 | Memoria=48570\n",
      "[DQN] Episodio 321/10000 | Recompensa: 210.0 | ε=0.100 | Memoria=48642\n",
      "[DQN] Episodio 322/10000 | Recompensa: 970.0 | ε=0.100 | Memoria=48846\n",
      "[DQN] Episodio 323/10000 | Recompensa: 310.0 | ε=0.100 | Memoria=48914\n",
      "[DQN] Episodio 324/10000 | Recompensa: 930.0 | ε=0.100 | Memoria=49082\n",
      "[DQN] Episodio 325/10000 | Recompensa: 930.0 | ε=0.100 | Memoria=49230\n",
      "[DQN] Episodio 326/10000 | Recompensa: 820.0 | ε=0.100 | Memoria=49383\n",
      "[DQN] Episodio 327/10000 | Recompensa: 1290.0 | ε=0.100 | Memoria=49586\n",
      "[DQN] Episodio 328/10000 | Recompensa: 1360.0 | ε=0.100 | Memoria=49822\n",
      "[DQN] Episodio 329/10000 | Recompensa: 650.0 | ε=0.100 | Memoria=49928\n",
      "[DQN] Episodio 330/10000 | Recompensa: 720.0 | ε=0.100 | Memoria=50038\n",
      "[DQN] Episodio 331/10000 | Recompensa: 1080.0 | ε=0.100 | Memoria=50193\n",
      "[DQN] Episodio 332/10000 | Recompensa: 1770.0 | ε=0.100 | Memoria=50488\n",
      "[DQN] Episodio 333/10000 | Recompensa: 210.0 | ε=0.100 | Memoria=50560\n",
      "[DQN] Episodio 334/10000 | Recompensa: 720.0 | ε=0.100 | Memoria=50713\n",
      "[DQN] Episodio 335/10000 | Recompensa: 1130.0 | ε=0.100 | Memoria=50940\n",
      "[DQN] Episodio 336/10000 | Recompensa: 1280.0 | ε=0.100 | Memoria=51161\n",
      "[DQN] Episodio 337/10000 | Recompensa: 1090.0 | ε=0.100 | Memoria=51375\n",
      "[DQN] Episodio 338/10000 | Recompensa: 1170.0 | ε=0.100 | Memoria=51578\n",
      "[DQN] Episodio 339/10000 | Recompensa: 1570.0 | ε=0.100 | Memoria=51930\n",
      "[DQN] Episodio 340/10000 | Recompensa: 930.0 | ε=0.100 | Memoria=52121\n",
      "[DQN] Episodio 341/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=52366\n",
      "[DQN] Episodio 342/10000 | Recompensa: 560.0 | ε=0.100 | Memoria=52476\n",
      "[DQN] Episodio 343/10000 | Recompensa: 590.0 | ε=0.100 | Memoria=52591\n",
      "[DQN] Episodio 344/10000 | Recompensa: 500.0 | ε=0.100 | Memoria=52677\n",
      "[DQN] Episodio 345/10000 | Recompensa: 820.0 | ε=0.100 | Memoria=52844\n",
      "[DQN] Episodio 346/10000 | Recompensa: 1270.0 | ε=0.100 | Memoria=52998\n",
      "[DQN] Episodio 347/10000 | Recompensa: 1080.0 | ε=0.100 | Memoria=53187\n",
      "[DQN] Episodio 348/10000 | Recompensa: 1300.0 | ε=0.100 | Memoria=53511\n",
      "[DQN] Episodio 349/10000 | Recompensa: 720.0 | ε=0.100 | Memoria=53623\n",
      "[DQN] Episodio 350/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=53869\n",
      "[DQN] Episodio 351/10000 | Recompensa: 840.0 | ε=0.100 | Memoria=54069\n",
      "[DQN] Episodio 352/10000 | Recompensa: 1100.0 | ε=0.100 | Memoria=54312\n",
      "[DQN] Episodio 353/10000 | Recompensa: 450.0 | ε=0.100 | Memoria=54390\n",
      "[DQN] Episodio 354/10000 | Recompensa: 1200.0 | ε=0.100 | Memoria=54587\n",
      "[DQN] Episodio 355/10000 | Recompensa: 670.0 | ε=0.100 | Memoria=54675\n",
      "[DQN] Episodio 356/10000 | Recompensa: 610.0 | ε=0.100 | Memoria=54761\n",
      "[DQN] Episodio 357/10000 | Recompensa: 1390.0 | ε=0.100 | Memoria=54982\n",
      "[DQN] Episodio 358/10000 | Recompensa: 1270.0 | ε=0.100 | Memoria=55307\n",
      "[DQN] Episodio 359/10000 | Recompensa: 920.0 | ε=0.100 | Memoria=55472\n",
      "[DQN] Episodio 360/10000 | Recompensa: 1400.0 | ε=0.100 | Memoria=55737\n",
      "[DQN] Episodio 361/10000 | Recompensa: 980.0 | ε=0.100 | Memoria=55894\n",
      "[DQN] Episodio 362/10000 | Recompensa: 1180.0 | ε=0.100 | Memoria=56133\n",
      "[DQN] Episodio 363/10000 | Recompensa: 1270.0 | ε=0.100 | Memoria=56328\n",
      "[DQN] Episodio 364/10000 | Recompensa: 1140.0 | ε=0.100 | Memoria=56596\n",
      "[DQN] Episodio 365/10000 | Recompensa: 1510.0 | ε=0.100 | Memoria=56860\n",
      "[DQN] Episodio 366/10000 | Recompensa: 1330.0 | ε=0.100 | Memoria=57057\n",
      "[DQN] Episodio 367/10000 | Recompensa: 790.0 | ε=0.100 | Memoria=57178\n",
      "[DQN] Episodio 368/10000 | Recompensa: 970.0 | ε=0.100 | Memoria=57339\n",
      "[DQN] Episodio 369/10000 | Recompensa: 690.0 | ε=0.100 | Memoria=57447\n",
      "[DQN] Episodio 370/10000 | Recompensa: 1980.0 | ε=0.100 | Memoria=57780\n",
      "[DQN] Episodio 371/10000 | Recompensa: 1410.0 | ε=0.100 | Memoria=58006\n",
      "[DQN] Episodio 372/10000 | Recompensa: 740.0 | ε=0.100 | Memoria=58127\n",
      "[DQN] Episodio 373/10000 | Recompensa: 1360.0 | ε=0.100 | Memoria=58403\n",
      "[DQN] Episodio 374/10000 | Recompensa: 1630.0 | ε=0.100 | Memoria=58658\n",
      "[DQN] Episodio 375/10000 | Recompensa: 530.0 | ε=0.100 | Memoria=58777\n",
      "[DQN] Episodio 376/10000 | Recompensa: 1070.0 | ε=0.100 | Memoria=58928\n",
      "[DQN] Episodio 377/10000 | Recompensa: 630.0 | ε=0.100 | Memoria=59018\n",
      "[DQN] Episodio 378/10000 | Recompensa: 1080.0 | ε=0.100 | Memoria=59212\n",
      "[DQN] Episodio 379/10000 | Recompensa: 750.0 | ε=0.100 | Memoria=59361\n",
      "[DQN] Episodio 380/10000 | Recompensa: 820.0 | ε=0.100 | Memoria=59485\n",
      "[DQN] Episodio 381/10000 | Recompensa: 740.0 | ε=0.100 | Memoria=59601\n",
      "[DQN] Episodio 382/10000 | Recompensa: 630.0 | ε=0.100 | Memoria=59686\n",
      "[DQN] Episodio 383/10000 | Recompensa: 420.0 | ε=0.100 | Memoria=59799\n",
      "[DQN] Episodio 384/10000 | Recompensa: 880.0 | ε=0.100 | Memoria=59943\n",
      "[DQN] Episodio 385/10000 | Recompensa: 770.0 | ε=0.100 | Memoria=60088\n",
      "[DQN] Episodio 386/10000 | Recompensa: 730.0 | ε=0.100 | Memoria=60262\n",
      "[DQN] Episodio 387/10000 | Recompensa: 1230.0 | ε=0.100 | Memoria=60493\n",
      "[DQN] Episodio 388/10000 | Recompensa: 620.0 | ε=0.100 | Memoria=60621\n",
      "[DQN] Episodio 389/10000 | Recompensa: 990.0 | ε=0.100 | Memoria=60818\n",
      "[DQN] Episodio 390/10000 | Recompensa: 970.0 | ε=0.100 | Memoria=60978\n",
      "[DQN] Episodio 391/10000 | Recompensa: 560.0 | ε=0.100 | Memoria=61126\n",
      "[DQN] Episodio 392/10000 | Recompensa: 460.0 | ε=0.100 | Memoria=61204\n",
      "[DQN] Episodio 393/10000 | Recompensa: 870.0 | ε=0.100 | Memoria=61411\n",
      "[DQN] Episodio 394/10000 | Recompensa: 1120.0 | ε=0.100 | Memoria=61604\n",
      "[DQN] Episodio 395/10000 | Recompensa: 980.0 | ε=0.100 | Memoria=61747\n",
      "[DQN] Episodio 396/10000 | Recompensa: 990.0 | ε=0.100 | Memoria=61875\n",
      "[DQN] Episodio 397/10000 | Recompensa: 710.0 | ε=0.100 | Memoria=62001\n",
      "[DQN] Episodio 398/10000 | Recompensa: 280.0 | ε=0.100 | Memoria=62092\n",
      "[DQN] Episodio 399/10000 | Recompensa: 1190.0 | ε=0.100 | Memoria=62281\n",
      "[DQN] Episodio 400/10000 | Recompensa: 870.0 | ε=0.100 | Memoria=62496\n",
      "[DQN] Episodio 401/10000 | Recompensa: 2040.0 | ε=0.100 | Memoria=62809\n",
      "[DQN] Episodio 402/10000 | Recompensa: 310.0 | ε=0.100 | Memoria=62881\n",
      "[DQN] Episodio 403/10000 | Recompensa: 1500.0 | ε=0.100 | Memoria=63116\n",
      "[DQN] Episodio 404/10000 | Recompensa: 820.0 | ε=0.100 | Memoria=63253\n",
      "[DQN] Episodio 405/10000 | Recompensa: 570.0 | ε=0.100 | Memoria=63381\n",
      "[DQN] Episodio 406/10000 | Recompensa: 760.0 | ε=0.100 | Memoria=63544\n",
      "[DQN] Episodio 407/10000 | Recompensa: 700.0 | ε=0.100 | Memoria=63708\n",
      "[DQN] Episodio 408/10000 | Recompensa: 560.0 | ε=0.100 | Memoria=63799\n",
      "[DQN] Episodio 409/10000 | Recompensa: 590.0 | ε=0.100 | Memoria=63900\n",
      "[DQN] Episodio 410/10000 | Recompensa: 390.0 | ε=0.100 | Memoria=63981\n",
      "[DQN] Episodio 411/10000 | Recompensa: 1210.0 | ε=0.100 | Memoria=64185\n",
      "[DQN] Episodio 412/10000 | Recompensa: 750.0 | ε=0.100 | Memoria=64334\n",
      "[DQN] Episodio 413/10000 | Recompensa: 710.0 | ε=0.100 | Memoria=64459\n",
      "[DQN] Episodio 414/10000 | Recompensa: 940.0 | ε=0.100 | Memoria=64656\n",
      "[DQN] Episodio 415/10000 | Recompensa: 600.0 | ε=0.100 | Memoria=64735\n",
      "[DQN] Episodio 416/10000 | Recompensa: 960.0 | ε=0.100 | Memoria=64895\n",
      "[DQN] Episodio 417/10000 | Recompensa: 480.0 | ε=0.100 | Memoria=64972\n",
      "[DQN] Episodio 418/10000 | Recompensa: 630.0 | ε=0.100 | Memoria=65107\n",
      "[DQN] Episodio 419/10000 | Recompensa: 1060.0 | ε=0.100 | Memoria=65353\n",
      "[DQN] Episodio 420/10000 | Recompensa: 920.0 | ε=0.100 | Memoria=65469\n",
      "[DQN] Episodio 421/10000 | Recompensa: 600.0 | ε=0.100 | Memoria=65635\n",
      "[DQN] Episodio 422/10000 | Recompensa: 870.0 | ε=0.100 | Memoria=65776\n",
      "[DQN] Episodio 423/10000 | Recompensa: 1600.0 | ε=0.100 | Memoria=66055\n",
      "[DQN] Episodio 424/10000 | Recompensa: 1390.0 | ε=0.100 | Memoria=66287\n",
      "[DQN] Episodio 425/10000 | Recompensa: 560.0 | ε=0.100 | Memoria=66381\n",
      "[DQN] Episodio 426/10000 | Recompensa: 610.0 | ε=0.100 | Memoria=66460\n",
      "[DQN] Episodio 427/10000 | Recompensa: 1320.0 | ε=0.100 | Memoria=66696\n",
      "[DQN] Episodio 428/10000 | Recompensa: 740.0 | ε=0.100 | Memoria=66808\n",
      "[DQN] Episodio 429/10000 | Recompensa: 480.0 | ε=0.100 | Memoria=66905\n",
      "[DQN] Episodio 430/10000 | Recompensa: 1560.0 | ε=0.100 | Memoria=67232\n",
      "[DQN] Episodio 431/10000 | Recompensa: 910.0 | ε=0.100 | Memoria=67352\n",
      "[DQN] Episodio 432/10000 | Recompensa: 1570.0 | ε=0.100 | Memoria=67671\n",
      "[DQN] Episodio 433/10000 | Recompensa: 1270.0 | ε=0.100 | Memoria=67918\n",
      "[DQN] Episodio 434/10000 | Recompensa: 660.0 | ε=0.100 | Memoria=68026\n",
      "[DQN] Episodio 435/10000 | Recompensa: 2920.0 | ε=0.100 | Memoria=68435\n",
      "[DQN] Episodio 436/10000 | Recompensa: 480.0 | ε=0.100 | Memoria=68565\n",
      "[DQN] Episodio 437/10000 | Recompensa: 680.0 | ε=0.100 | Memoria=68675\n",
      "[DQN] Episodio 438/10000 | Recompensa: 570.0 | ε=0.100 | Memoria=68767\n",
      "[DQN] Episodio 439/10000 | Recompensa: 670.0 | ε=0.100 | Memoria=68918\n",
      "[DQN] Episodio 440/10000 | Recompensa: 480.0 | ε=0.100 | Memoria=68991\n",
      "[DQN] Episodio 441/10000 | Recompensa: 470.0 | ε=0.100 | Memoria=69076\n",
      "[DQN] Episodio 442/10000 | Recompensa: 550.0 | ε=0.100 | Memoria=69172\n",
      "[DQN] Episodio 443/10000 | Recompensa: 500.0 | ε=0.100 | Memoria=69249\n",
      "[DQN] Episodio 444/10000 | Recompensa: 1010.0 | ε=0.100 | Memoria=69399\n",
      "[DQN] Episodio 445/10000 | Recompensa: 830.0 | ε=0.100 | Memoria=69517\n",
      "[DQN] Episodio 446/10000 | Recompensa: 420.0 | ε=0.100 | Memoria=69626\n",
      "[DQN] Episodio 447/10000 | Recompensa: 540.0 | ε=0.100 | Memoria=69720\n",
      "[DQN] Episodio 448/10000 | Recompensa: 880.0 | ε=0.100 | Memoria=69853\n",
      "[DQN] Episodio 449/10000 | Recompensa: 1050.0 | ε=0.100 | Memoria=70042\n",
      "[DQN] Episodio 450/10000 | Recompensa: 390.0 | ε=0.100 | Memoria=70128\n",
      "[DQN] Episodio 451/10000 | Recompensa: 1250.0 | ε=0.100 | Memoria=70322\n",
      "[DQN] Episodio 452/10000 | Recompensa: 1260.0 | ε=0.100 | Memoria=70493\n",
      "[DQN] Episodio 453/10000 | Recompensa: 450.0 | ε=0.100 | Memoria=70583\n",
      "[DQN] Episodio 454/10000 | Recompensa: 390.0 | ε=0.100 | Memoria=70668\n",
      "[DQN] Episodio 455/10000 | Recompensa: 540.0 | ε=0.100 | Memoria=70795\n",
      "[DQN] Episodio 456/10000 | Recompensa: 490.0 | ε=0.100 | Memoria=70888\n",
      "[DQN] Episodio 457/10000 | Recompensa: 750.0 | ε=0.100 | Memoria=71039\n",
      "[DQN] Episodio 458/10000 | Recompensa: 820.0 | ε=0.100 | Memoria=71260\n",
      "[DQN] Episodio 459/10000 | Recompensa: 820.0 | ε=0.100 | Memoria=71420\n",
      "[DQN] Episodio 460/10000 | Recompensa: 690.0 | ε=0.100 | Memoria=71530\n",
      "[DQN] Episodio 461/10000 | Recompensa: 1120.0 | ε=0.100 | Memoria=71786\n",
      "[DQN] Episodio 462/10000 | Recompensa: 620.0 | ε=0.100 | Memoria=71953\n",
      "[DQN] Episodio 463/10000 | Recompensa: 870.0 | ε=0.100 | Memoria=72084\n",
      "[DQN] Episodio 464/10000 | Recompensa: 1330.0 | ε=0.100 | Memoria=72369\n",
      "[DQN] Episodio 465/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=72574\n",
      "[DQN] Episodio 466/10000 | Recompensa: 1070.0 | ε=0.100 | Memoria=72760\n",
      "[DQN] Episodio 467/10000 | Recompensa: 640.0 | ε=0.100 | Memoria=72868\n",
      "[DQN] Episodio 468/10000 | Recompensa: 610.0 | ε=0.100 | Memoria=72972\n",
      "[DQN] Episodio 469/10000 | Recompensa: 1250.0 | ε=0.100 | Memoria=73270\n",
      "[DQN] Episodio 470/10000 | Recompensa: 980.0 | ε=0.100 | Memoria=73462\n",
      "[DQN] Episodio 471/10000 | Recompensa: 670.0 | ε=0.100 | Memoria=73597\n",
      "[DQN] Episodio 472/10000 | Recompensa: 1340.0 | ε=0.100 | Memoria=73840\n",
      "[DQN] Episodio 473/10000 | Recompensa: 1260.0 | ε=0.100 | Memoria=74088\n",
      "[DQN] Episodio 474/10000 | Recompensa: 600.0 | ε=0.100 | Memoria=74196\n",
      "[DQN] Episodio 475/10000 | Recompensa: 900.0 | ε=0.100 | Memoria=74387\n",
      "[DQN] Episodio 476/10000 | Recompensa: 370.0 | ε=0.100 | Memoria=74497\n",
      "[DQN] Episodio 477/10000 | Recompensa: 900.0 | ε=0.100 | Memoria=74614\n",
      "[DQN] Episodio 478/10000 | Recompensa: 1170.0 | ε=0.100 | Memoria=74799\n",
      "[DQN] Episodio 479/10000 | Recompensa: 550.0 | ε=0.100 | Memoria=74926\n",
      "[DQN] Episodio 480/10000 | Recompensa: 870.0 | ε=0.100 | Memoria=75133\n",
      "[DQN] Episodio 481/10000 | Recompensa: 2280.0 | ε=0.100 | Memoria=75400\n",
      "[DQN] Episodio 482/10000 | Recompensa: 1190.0 | ε=0.100 | Memoria=75669\n",
      "[DQN] Episodio 483/10000 | Recompensa: 790.0 | ε=0.100 | Memoria=75864\n",
      "[DQN] Episodio 484/10000 | Recompensa: 880.0 | ε=0.100 | Memoria=76034\n",
      "[DQN] Episodio 485/10000 | Recompensa: 1580.0 | ε=0.100 | Memoria=76311\n",
      "[DQN] Episodio 486/10000 | Recompensa: 1870.0 | ε=0.100 | Memoria=76582\n",
      "[DQN] Episodio 487/10000 | Recompensa: 1140.0 | ε=0.100 | Memoria=76750\n",
      "[DQN] Episodio 488/10000 | Recompensa: 1150.0 | ε=0.100 | Memoria=76972\n",
      "[DQN] Episodio 489/10000 | Recompensa: 950.0 | ε=0.100 | Memoria=77172\n",
      "[DQN] Episodio 490/10000 | Recompensa: 950.0 | ε=0.100 | Memoria=77318\n",
      "[DQN] Episodio 491/10000 | Recompensa: 1040.0 | ε=0.100 | Memoria=77534\n",
      "[DQN] Episodio 492/10000 | Recompensa: 1070.0 | ε=0.100 | Memoria=77751\n",
      "[DQN] Episodio 493/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=77971\n",
      "[DQN] Episodio 494/10000 | Recompensa: 910.0 | ε=0.100 | Memoria=78146\n",
      "[DQN] Episodio 495/10000 | Recompensa: 870.0 | ε=0.100 | Memoria=78268\n",
      "[DQN] Episodio 496/10000 | Recompensa: 420.0 | ε=0.100 | Memoria=78363\n",
      "[DQN] Episodio 497/10000 | Recompensa: 820.0 | ε=0.100 | Memoria=78470\n",
      "[DQN] Episodio 498/10000 | Recompensa: 1130.0 | ε=0.100 | Memoria=78712\n",
      "[DQN] Episodio 499/10000 | Recompensa: 860.0 | ε=0.100 | Memoria=78889\n",
      "[DQN] Episodio 500/10000 | Recompensa: 290.0 | ε=0.100 | Memoria=78981\n",
      "[CHECKPOINT] Guardado en: /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento/dqn/dqn_galaxian_ep500.pth\n",
      "[REGISTRO] Gráfica y CSV guardados: /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento/dqn/recompensas_ep500.png / /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento/dqn/registro_recompensas.csv\n",
      "[DQN] Episodio 501/10000 | Recompensa: 990.0 | ε=0.100 | Memoria=79175\n",
      "[DQN] Episodio 502/10000 | Recompensa: 1450.0 | ε=0.100 | Memoria=79428\n",
      "[DQN] Episodio 503/10000 | Recompensa: 1150.0 | ε=0.100 | Memoria=79659\n",
      "[DQN] Episodio 504/10000 | Recompensa: 770.0 | ε=0.100 | Memoria=79844\n",
      "[DQN] Episodio 505/10000 | Recompensa: 1500.0 | ε=0.100 | Memoria=80127\n",
      "[DQN] Episodio 506/10000 | Recompensa: 1040.0 | ε=0.100 | Memoria=80310\n",
      "[DQN] Episodio 507/10000 | Recompensa: 800.0 | ε=0.100 | Memoria=80426\n",
      "[DQN] Episodio 508/10000 | Recompensa: 850.0 | ε=0.100 | Memoria=80578\n",
      "[DQN] Episodio 509/10000 | Recompensa: 1500.0 | ε=0.100 | Memoria=80954\n",
      "[DQN] Episodio 510/10000 | Recompensa: 550.0 | ε=0.100 | Memoria=81070\n",
      "[DQN] Episodio 511/10000 | Recompensa: 920.0 | ε=0.100 | Memoria=81185\n",
      "[DQN] Episodio 512/10000 | Recompensa: 1440.0 | ε=0.100 | Memoria=81618\n",
      "[DQN] Episodio 513/10000 | Recompensa: 610.0 | ε=0.100 | Memoria=81730\n",
      "[DQN] Episodio 514/10000 | Recompensa: 360.0 | ε=0.100 | Memoria=81794\n",
      "[DQN] Episodio 515/10000 | Recompensa: 720.0 | ε=0.100 | Memoria=81943\n",
      "[DQN] Episodio 516/10000 | Recompensa: 830.0 | ε=0.100 | Memoria=82102\n",
      "[DQN] Episodio 517/10000 | Recompensa: 1150.0 | ε=0.100 | Memoria=82258\n",
      "[DQN] Episodio 518/10000 | Recompensa: 980.0 | ε=0.100 | Memoria=82420\n",
      "[DQN] Episodio 519/10000 | Recompensa: 1020.0 | ε=0.100 | Memoria=82619\n",
      "[DQN] Episodio 520/10000 | Recompensa: 240.0 | ε=0.100 | Memoria=82702\n",
      "[DQN] Episodio 521/10000 | Recompensa: 510.0 | ε=0.100 | Memoria=82780\n",
      "[DQN] Episodio 522/10000 | Recompensa: 340.0 | ε=0.100 | Memoria=82864\n",
      "[DQN] Episodio 523/10000 | Recompensa: 790.0 | ε=0.100 | Memoria=83050\n",
      "[DQN] Episodio 524/10000 | Recompensa: 900.0 | ε=0.100 | Memoria=83228\n",
      "[DQN] Episodio 525/10000 | Recompensa: 1070.0 | ε=0.100 | Memoria=83479\n",
      "[DQN] Episodio 526/10000 | Recompensa: 470.0 | ε=0.100 | Memoria=83557\n",
      "[DQN] Episodio 527/10000 | Recompensa: 960.0 | ε=0.100 | Memoria=83684\n",
      "[DQN] Episodio 528/10000 | Recompensa: 660.0 | ε=0.100 | Memoria=83803\n",
      "[DQN] Episodio 529/10000 | Recompensa: 350.0 | ε=0.100 | Memoria=83893\n",
      "[DQN] Episodio 530/10000 | Recompensa: 660.0 | ε=0.100 | Memoria=84019\n",
      "[DQN] Episodio 531/10000 | Recompensa: 850.0 | ε=0.100 | Memoria=84222\n",
      "[DQN] Episodio 532/10000 | Recompensa: 300.0 | ε=0.100 | Memoria=84291\n",
      "[DQN] Episodio 533/10000 | Recompensa: 480.0 | ε=0.100 | Memoria=84380\n",
      "[DQN] Episodio 534/10000 | Recompensa: 1260.0 | ε=0.100 | Memoria=84705\n",
      "[DQN] Episodio 535/10000 | Recompensa: 580.0 | ε=0.100 | Memoria=84791\n",
      "[DQN] Episodio 536/10000 | Recompensa: 810.0 | ε=0.100 | Memoria=84933\n",
      "[DQN] Episodio 537/10000 | Recompensa: 880.0 | ε=0.100 | Memoria=85105\n",
      "[DQN] Episodio 538/10000 | Recompensa: 930.0 | ε=0.100 | Memoria=85302\n",
      "[DQN] Episodio 539/10000 | Recompensa: 440.0 | ε=0.100 | Memoria=85384\n",
      "[DQN] Episodio 540/10000 | Recompensa: 1110.0 | ε=0.100 | Memoria=85585\n",
      "[DQN] Episodio 541/10000 | Recompensa: 880.0 | ε=0.100 | Memoria=85767\n",
      "[DQN] Episodio 542/10000 | Recompensa: 1120.0 | ε=0.100 | Memoria=85980\n",
      "[DQN] Episodio 543/10000 | Recompensa: 610.0 | ε=0.100 | Memoria=86135\n",
      "[DQN] Episodio 544/10000 | Recompensa: 600.0 | ε=0.100 | Memoria=86240\n",
      "[DQN] Episodio 545/10000 | Recompensa: 1390.0 | ε=0.100 | Memoria=86450\n",
      "[DQN] Episodio 546/10000 | Recompensa: 1300.0 | ε=0.100 | Memoria=86729\n",
      "[DQN] Episodio 547/10000 | Recompensa: 660.0 | ε=0.100 | Memoria=86865\n",
      "[DQN] Episodio 548/10000 | Recompensa: 990.0 | ε=0.100 | Memoria=87081\n",
      "[DQN] Episodio 549/10000 | Recompensa: 820.0 | ε=0.100 | Memoria=87235\n",
      "[DQN] Episodio 550/10000 | Recompensa: 970.0 | ε=0.100 | Memoria=87414\n",
      "[DQN] Episodio 551/10000 | Recompensa: 1080.0 | ε=0.100 | Memoria=87586\n",
      "[DQN] Episodio 552/10000 | Recompensa: 420.0 | ε=0.100 | Memoria=87674\n",
      "[DQN] Episodio 553/10000 | Recompensa: 360.0 | ε=0.100 | Memoria=87755\n",
      "[DQN] Episodio 554/10000 | Recompensa: 770.0 | ε=0.100 | Memoria=87890\n",
      "[DQN] Episodio 555/10000 | Recompensa: 490.0 | ε=0.100 | Memoria=87968\n",
      "[DQN] Episodio 556/10000 | Recompensa: 840.0 | ε=0.100 | Memoria=88082\n",
      "[DQN] Episodio 557/10000 | Recompensa: 820.0 | ε=0.100 | Memoria=88230\n",
      "[DQN] Episodio 558/10000 | Recompensa: 870.0 | ε=0.100 | Memoria=88386\n",
      "[DQN] Episodio 559/10000 | Recompensa: 1130.0 | ε=0.100 | Memoria=88635\n",
      "[DQN] Episodio 560/10000 | Recompensa: 1380.0 | ε=0.100 | Memoria=89058\n",
      "[DQN] Episodio 561/10000 | Recompensa: 970.0 | ε=0.100 | Memoria=89314\n",
      "[DQN] Episodio 562/10000 | Recompensa: 710.0 | ε=0.100 | Memoria=89435\n",
      "[DQN] Episodio 563/10000 | Recompensa: 1130.0 | ε=0.100 | Memoria=89598\n",
      "[DQN] Episodio 564/10000 | Recompensa: 1120.0 | ε=0.100 | Memoria=89804\n",
      "[DQN] Episodio 565/10000 | Recompensa: 1210.0 | ε=0.100 | Memoria=90217\n",
      "[DQN] Episodio 566/10000 | Recompensa: 550.0 | ε=0.100 | Memoria=90345\n",
      "[DQN] Episodio 567/10000 | Recompensa: 550.0 | ε=0.100 | Memoria=90438\n",
      "[DQN] Episodio 568/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=90686\n",
      "[DQN] Episodio 569/10000 | Recompensa: 580.0 | ε=0.100 | Memoria=90820\n",
      "[DQN] Episodio 570/10000 | Recompensa: 630.0 | ε=0.100 | Memoria=90948\n",
      "[DQN] Episodio 571/10000 | Recompensa: 1100.0 | ε=0.100 | Memoria=91115\n",
      "[DQN] Episodio 572/10000 | Recompensa: 430.0 | ε=0.100 | Memoria=91211\n",
      "[DQN] Episodio 573/10000 | Recompensa: 940.0 | ε=0.100 | Memoria=91373\n",
      "[DQN] Episodio 574/10000 | Recompensa: 1780.0 | ε=0.100 | Memoria=91807\n",
      "[DQN] Episodio 575/10000 | Recompensa: 920.0 | ε=0.100 | Memoria=91927\n",
      "[DQN] Episodio 576/10000 | Recompensa: 550.0 | ε=0.100 | Memoria=92019\n",
      "[DQN] Episodio 577/10000 | Recompensa: 360.0 | ε=0.100 | Memoria=92104\n",
      "[DQN] Episodio 578/10000 | Recompensa: 1390.0 | ε=0.100 | Memoria=92332\n",
      "[DQN] Episodio 579/10000 | Recompensa: 1230.0 | ε=0.100 | Memoria=92538\n",
      "[DQN] Episodio 580/10000 | Recompensa: 380.0 | ε=0.100 | Memoria=92609\n",
      "[DQN] Episodio 581/10000 | Recompensa: 560.0 | ε=0.100 | Memoria=92714\n",
      "[DQN] Episodio 582/10000 | Recompensa: 1730.0 | ε=0.100 | Memoria=92991\n",
      "[DQN] Episodio 583/10000 | Recompensa: 530.0 | ε=0.100 | Memoria=93069\n",
      "[DQN] Episodio 584/10000 | Recompensa: 970.0 | ε=0.100 | Memoria=93325\n",
      "[DQN] Episodio 585/10000 | Recompensa: 430.0 | ε=0.100 | Memoria=93453\n",
      "[DQN] Episodio 586/10000 | Recompensa: 790.0 | ε=0.100 | Memoria=93587\n",
      "[DQN] Episodio 587/10000 | Recompensa: 1300.0 | ε=0.100 | Memoria=94044\n",
      "[DQN] Episodio 588/10000 | Recompensa: 800.0 | ε=0.100 | Memoria=94242\n",
      "[DQN] Episodio 589/10000 | Recompensa: 950.0 | ε=0.100 | Memoria=94388\n",
      "[DQN] Episodio 590/10000 | Recompensa: 1260.0 | ε=0.100 | Memoria=94666\n",
      "[DQN] Episodio 591/10000 | Recompensa: 570.0 | ε=0.100 | Memoria=94767\n",
      "[DQN] Episodio 592/10000 | Recompensa: 300.0 | ε=0.100 | Memoria=94851\n",
      "[DQN] Episodio 593/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=95191\n",
      "[DQN] Episodio 594/10000 | Recompensa: 940.0 | ε=0.100 | Memoria=95408\n",
      "[DQN] Episodio 595/10000 | Recompensa: 1690.0 | ε=0.100 | Memoria=95784\n",
      "[DQN] Episodio 596/10000 | Recompensa: 2810.0 | ε=0.100 | Memoria=96339\n",
      "[DQN] Episodio 597/10000 | Recompensa: 780.0 | ε=0.100 | Memoria=96461\n",
      "[DQN] Episodio 598/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=96736\n",
      "[DQN] Episodio 599/10000 | Recompensa: 1590.0 | ε=0.100 | Memoria=97059\n",
      "[DQN] Episodio 600/10000 | Recompensa: 460.0 | ε=0.100 | Memoria=97156\n",
      "[DQN] Episodio 601/10000 | Recompensa: 960.0 | ε=0.100 | Memoria=97372\n",
      "[DQN] Episodio 602/10000 | Recompensa: 1520.0 | ε=0.100 | Memoria=97676\n",
      "[DQN] Episodio 603/10000 | Recompensa: 850.0 | ε=0.100 | Memoria=97840\n",
      "[DQN] Episodio 604/10000 | Recompensa: 410.0 | ε=0.100 | Memoria=97943\n",
      "[DQN] Episodio 605/10000 | Recompensa: 580.0 | ε=0.100 | Memoria=98083\n",
      "[DQN] Episodio 606/10000 | Recompensa: 1320.0 | ε=0.100 | Memoria=98283\n",
      "[DQN] Episodio 607/10000 | Recompensa: 310.0 | ε=0.100 | Memoria=98389\n",
      "[DQN] Episodio 608/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=98675\n",
      "[DQN] Episodio 609/10000 | Recompensa: 750.0 | ε=0.100 | Memoria=98810\n",
      "[DQN] Episodio 610/10000 | Recompensa: 1490.0 | ε=0.100 | Memoria=99042\n",
      "[DQN] Episodio 611/10000 | Recompensa: 2430.0 | ε=0.100 | Memoria=99289\n",
      "[DQN] Episodio 612/10000 | Recompensa: 1230.0 | ε=0.100 | Memoria=99460\n",
      "[DQN] Episodio 613/10000 | Recompensa: 1700.0 | ε=0.100 | Memoria=99722\n",
      "[DQN] Episodio 614/10000 | Recompensa: 1260.0 | ε=0.100 | Memoria=99949\n",
      "[DQN] Episodio 615/10000 | Recompensa: 1010.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 616/10000 | Recompensa: 340.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 617/10000 | Recompensa: 410.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 618/10000 | Recompensa: 560.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 619/10000 | Recompensa: 590.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 620/10000 | Recompensa: 1930.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 621/10000 | Recompensa: 930.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 622/10000 | Recompensa: 720.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 623/10000 | Recompensa: 790.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 624/10000 | Recompensa: 440.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 625/10000 | Recompensa: 2170.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 626/10000 | Recompensa: 620.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 627/10000 | Recompensa: 820.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 628/10000 | Recompensa: 1380.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 629/10000 | Recompensa: 1160.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 630/10000 | Recompensa: 1500.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 631/10000 | Recompensa: 1450.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 632/10000 | Recompensa: 1020.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 633/10000 | Recompensa: 1580.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 634/10000 | Recompensa: 820.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 635/10000 | Recompensa: 1450.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 636/10000 | Recompensa: 1170.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 637/10000 | Recompensa: 1770.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 638/10000 | Recompensa: 610.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 639/10000 | Recompensa: 730.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 640/10000 | Recompensa: 1050.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 641/10000 | Recompensa: 270.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 642/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 643/10000 | Recompensa: 990.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 644/10000 | Recompensa: 1390.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 645/10000 | Recompensa: 880.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 646/10000 | Recompensa: 1070.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 647/10000 | Recompensa: 1050.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 648/10000 | Recompensa: 1460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 649/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 650/10000 | Recompensa: 1280.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 651/10000 | Recompensa: 1200.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 652/10000 | Recompensa: 680.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 653/10000 | Recompensa: 640.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 654/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 655/10000 | Recompensa: 1980.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 656/10000 | Recompensa: 1110.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 657/10000 | Recompensa: 870.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 658/10000 | Recompensa: 1240.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 659/10000 | Recompensa: 870.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 660/10000 | Recompensa: 920.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 661/10000 | Recompensa: 1400.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 662/10000 | Recompensa: 1340.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 663/10000 | Recompensa: 270.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 664/10000 | Recompensa: 600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 665/10000 | Recompensa: 1400.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 666/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 667/10000 | Recompensa: 870.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 668/10000 | Recompensa: 1280.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 669/10000 | Recompensa: 1580.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 670/10000 | Recompensa: 490.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 671/10000 | Recompensa: 750.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 672/10000 | Recompensa: 560.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 673/10000 | Recompensa: 1220.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 674/10000 | Recompensa: 640.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 675/10000 | Recompensa: 580.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 676/10000 | Recompensa: 1150.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 677/10000 | Recompensa: 1660.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 678/10000 | Recompensa: 610.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 679/10000 | Recompensa: 2860.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 680/10000 | Recompensa: 850.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 681/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 682/10000 | Recompensa: 2240.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 683/10000 | Recompensa: 1140.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 684/10000 | Recompensa: 1380.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 685/10000 | Recompensa: 1290.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 686/10000 | Recompensa: 1440.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 687/10000 | Recompensa: 1700.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 688/10000 | Recompensa: 490.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 689/10000 | Recompensa: 1950.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 690/10000 | Recompensa: 1590.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 691/10000 | Recompensa: 1640.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 692/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 693/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 694/10000 | Recompensa: 1130.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 695/10000 | Recompensa: 560.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 696/10000 | Recompensa: 1200.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 697/10000 | Recompensa: 1310.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 698/10000 | Recompensa: 1210.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 699/10000 | Recompensa: 1270.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 700/10000 | Recompensa: 1550.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 701/10000 | Recompensa: 950.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 702/10000 | Recompensa: 1090.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 703/10000 | Recompensa: 940.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 704/10000 | Recompensa: 1450.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 705/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 706/10000 | Recompensa: 1250.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 707/10000 | Recompensa: 1640.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 708/10000 | Recompensa: 1460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 709/10000 | Recompensa: 780.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 710/10000 | Recompensa: 500.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 711/10000 | Recompensa: 450.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 712/10000 | Recompensa: 1340.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 713/10000 | Recompensa: 810.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 714/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 715/10000 | Recompensa: 450.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 716/10000 | Recompensa: 930.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 717/10000 | Recompensa: 1620.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 718/10000 | Recompensa: 1170.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 719/10000 | Recompensa: 540.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 720/10000 | Recompensa: 970.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 721/10000 | Recompensa: 1640.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 722/10000 | Recompensa: 1490.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 723/10000 | Recompensa: 1330.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 724/10000 | Recompensa: 850.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 725/10000 | Recompensa: 690.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 726/10000 | Recompensa: 1560.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 727/10000 | Recompensa: 1730.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 728/10000 | Recompensa: 1080.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 729/10000 | Recompensa: 1190.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 730/10000 | Recompensa: 1560.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 731/10000 | Recompensa: 1570.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 732/10000 | Recompensa: 1760.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 733/10000 | Recompensa: 1630.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 734/10000 | Recompensa: 1260.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 735/10000 | Recompensa: 1400.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 736/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 737/10000 | Recompensa: 1060.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 738/10000 | Recompensa: 1060.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 739/10000 | Recompensa: 3020.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 740/10000 | Recompensa: 2610.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 741/10000 | Recompensa: 1650.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 742/10000 | Recompensa: 2380.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 743/10000 | Recompensa: 1410.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 744/10000 | Recompensa: 1590.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 745/10000 | Recompensa: 2950.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 746/10000 | Recompensa: 1480.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 747/10000 | Recompensa: 950.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 748/10000 | Recompensa: 1680.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 749/10000 | Recompensa: 1000.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 750/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 751/10000 | Recompensa: 1250.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 752/10000 | Recompensa: 2190.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 753/10000 | Recompensa: 2430.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 754/10000 | Recompensa: 2410.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 755/10000 | Recompensa: 930.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 756/10000 | Recompensa: 2020.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 757/10000 | Recompensa: 1350.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 758/10000 | Recompensa: 1590.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 759/10000 | Recompensa: 820.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 760/10000 | Recompensa: 1310.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 761/10000 | Recompensa: 1230.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 762/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 763/10000 | Recompensa: 2500.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 764/10000 | Recompensa: 1690.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 765/10000 | Recompensa: 1620.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 766/10000 | Recompensa: 1280.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 767/10000 | Recompensa: 560.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 768/10000 | Recompensa: 1260.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 769/10000 | Recompensa: 1030.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 770/10000 | Recompensa: 1410.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 771/10000 | Recompensa: 1890.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 772/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 773/10000 | Recompensa: 1070.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 774/10000 | Recompensa: 1320.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 775/10000 | Recompensa: 1300.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 776/10000 | Recompensa: 430.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 777/10000 | Recompensa: 1330.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 778/10000 | Recompensa: 1540.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 779/10000 | Recompensa: 1390.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 780/10000 | Recompensa: 2070.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 781/10000 | Recompensa: 1200.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 782/10000 | Recompensa: 1120.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 783/10000 | Recompensa: 760.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 784/10000 | Recompensa: 580.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 785/10000 | Recompensa: 1170.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 786/10000 | Recompensa: 1970.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 787/10000 | Recompensa: 1080.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 788/10000 | Recompensa: 1410.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 789/10000 | Recompensa: 1680.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 790/10000 | Recompensa: 2830.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 791/10000 | Recompensa: 1000.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 792/10000 | Recompensa: 1640.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 793/10000 | Recompensa: 1380.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 794/10000 | Recompensa: 1960.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 795/10000 | Recompensa: 2210.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 796/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 797/10000 | Recompensa: 2480.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 798/10000 | Recompensa: 1480.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 799/10000 | Recompensa: 2730.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 800/10000 | Recompensa: 920.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 801/10000 | Recompensa: 2820.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 802/10000 | Recompensa: 2240.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 803/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 804/10000 | Recompensa: 1630.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 805/10000 | Recompensa: 1080.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 806/10000 | Recompensa: 1390.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 807/10000 | Recompensa: 1440.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 808/10000 | Recompensa: 1540.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 809/10000 | Recompensa: 1020.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 810/10000 | Recompensa: 1480.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 811/10000 | Recompensa: 1210.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 812/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 813/10000 | Recompensa: 1290.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 814/10000 | Recompensa: 2150.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 815/10000 | Recompensa: 990.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 816/10000 | Recompensa: 970.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 817/10000 | Recompensa: 1280.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 818/10000 | Recompensa: 1430.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 819/10000 | Recompensa: 740.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 820/10000 | Recompensa: 1410.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 821/10000 | Recompensa: 1120.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 822/10000 | Recompensa: 1930.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 823/10000 | Recompensa: 2600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 824/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 825/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 826/10000 | Recompensa: 3010.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 827/10000 | Recompensa: 1570.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 828/10000 | Recompensa: 1840.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 829/10000 | Recompensa: 1280.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 830/10000 | Recompensa: 2590.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 831/10000 | Recompensa: 1240.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 832/10000 | Recompensa: 2340.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 833/10000 | Recompensa: 1670.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 834/10000 | Recompensa: 1690.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 835/10000 | Recompensa: 2220.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 836/10000 | Recompensa: 410.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 837/10000 | Recompensa: 1460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 838/10000 | Recompensa: 1380.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 839/10000 | Recompensa: 1120.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 840/10000 | Recompensa: 1460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 841/10000 | Recompensa: 1780.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 842/10000 | Recompensa: 1430.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 843/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 844/10000 | Recompensa: 2940.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 845/10000 | Recompensa: 2660.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 846/10000 | Recompensa: 1350.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 847/10000 | Recompensa: 1260.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 848/10000 | Recompensa: 1600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 849/10000 | Recompensa: 3550.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 850/10000 | Recompensa: 2210.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 851/10000 | Recompensa: 1220.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 852/10000 | Recompensa: 1040.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 853/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 854/10000 | Recompensa: 3440.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 855/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 856/10000 | Recompensa: 990.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 857/10000 | Recompensa: 1260.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 858/10000 | Recompensa: 1080.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 859/10000 | Recompensa: 1260.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 860/10000 | Recompensa: 690.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 861/10000 | Recompensa: 1180.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 862/10000 | Recompensa: 1600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 863/10000 | Recompensa: 1490.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 864/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 865/10000 | Recompensa: 1350.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 866/10000 | Recompensa: 1820.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 867/10000 | Recompensa: 1400.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 868/10000 | Recompensa: 1730.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 869/10000 | Recompensa: 980.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 870/10000 | Recompensa: 1410.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 871/10000 | Recompensa: 1290.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 872/10000 | Recompensa: 1780.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 873/10000 | Recompensa: 850.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 874/10000 | Recompensa: 780.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 875/10000 | Recompensa: 2170.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 876/10000 | Recompensa: 1130.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 877/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 878/10000 | Recompensa: 2850.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 879/10000 | Recompensa: 3290.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 880/10000 | Recompensa: 910.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 881/10000 | Recompensa: 2940.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 882/10000 | Recompensa: 1030.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 883/10000 | Recompensa: 1570.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 884/10000 | Recompensa: 3500.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 885/10000 | Recompensa: 2400.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 886/10000 | Recompensa: 1750.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 887/10000 | Recompensa: 1590.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 888/10000 | Recompensa: 810.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 889/10000 | Recompensa: 490.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 890/10000 | Recompensa: 3100.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 891/10000 | Recompensa: 2130.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 892/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 893/10000 | Recompensa: 1310.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 894/10000 | Recompensa: 2720.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 895/10000 | Recompensa: 2660.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 896/10000 | Recompensa: 1230.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 897/10000 | Recompensa: 1100.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 898/10000 | Recompensa: 2590.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 899/10000 | Recompensa: 1460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 900/10000 | Recompensa: 1700.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 901/10000 | Recompensa: 1400.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 902/10000 | Recompensa: 3360.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 903/10000 | Recompensa: 1380.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 904/10000 | Recompensa: 1300.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 905/10000 | Recompensa: 2280.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 906/10000 | Recompensa: 900.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 907/10000 | Recompensa: 2930.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 908/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 909/10000 | Recompensa: 3050.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 910/10000 | Recompensa: 2460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 911/10000 | Recompensa: 1260.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 912/10000 | Recompensa: 1770.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 913/10000 | Recompensa: 1620.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 914/10000 | Recompensa: 1460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 915/10000 | Recompensa: 1240.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 916/10000 | Recompensa: 1950.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 917/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 918/10000 | Recompensa: 1330.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 919/10000 | Recompensa: 2080.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 920/10000 | Recompensa: 1180.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 921/10000 | Recompensa: 1120.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 922/10000 | Recompensa: 1260.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 923/10000 | Recompensa: 2740.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 924/10000 | Recompensa: 2940.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 925/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 926/10000 | Recompensa: 1150.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 927/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 928/10000 | Recompensa: 690.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 929/10000 | Recompensa: 1550.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 930/10000 | Recompensa: 1850.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 931/10000 | Recompensa: 1560.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 932/10000 | Recompensa: 2520.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 933/10000 | Recompensa: 1450.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 934/10000 | Recompensa: 1460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 935/10000 | Recompensa: 2600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 936/10000 | Recompensa: 1110.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 937/10000 | Recompensa: 930.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 938/10000 | Recompensa: 1800.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 939/10000 | Recompensa: 1230.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 940/10000 | Recompensa: 2760.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 941/10000 | Recompensa: 4380.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 942/10000 | Recompensa: 1580.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 943/10000 | Recompensa: 1570.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 944/10000 | Recompensa: 1090.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 945/10000 | Recompensa: 1830.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 946/10000 | Recompensa: 3060.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 947/10000 | Recompensa: 1020.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 948/10000 | Recompensa: 910.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 949/10000 | Recompensa: 2960.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 950/10000 | Recompensa: 2230.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 951/10000 | Recompensa: 1480.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 952/10000 | Recompensa: 1410.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 953/10000 | Recompensa: 1550.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 954/10000 | Recompensa: 870.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 955/10000 | Recompensa: 780.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 956/10000 | Recompensa: 1190.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 957/10000 | Recompensa: 1550.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 958/10000 | Recompensa: 1360.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 959/10000 | Recompensa: 2220.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 960/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 961/10000 | Recompensa: 2340.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 962/10000 | Recompensa: 1940.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 963/10000 | Recompensa: 3450.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 964/10000 | Recompensa: 1100.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 965/10000 | Recompensa: 3290.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 966/10000 | Recompensa: 1310.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 967/10000 | Recompensa: 2230.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 968/10000 | Recompensa: 1280.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 969/10000 | Recompensa: 1620.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 970/10000 | Recompensa: 1570.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 971/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 972/10000 | Recompensa: 920.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 973/10000 | Recompensa: 3850.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 974/10000 | Recompensa: 1670.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 975/10000 | Recompensa: 1530.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 976/10000 | Recompensa: 2200.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 977/10000 | Recompensa: 1700.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 978/10000 | Recompensa: 2350.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 979/10000 | Recompensa: 3410.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 980/10000 | Recompensa: 1500.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 981/10000 | Recompensa: 1650.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 982/10000 | Recompensa: 4080.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 983/10000 | Recompensa: 2450.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 984/10000 | Recompensa: 2300.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 985/10000 | Recompensa: 3020.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 986/10000 | Recompensa: 1870.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 987/10000 | Recompensa: 2500.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 988/10000 | Recompensa: 1050.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 989/10000 | Recompensa: 1490.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 990/10000 | Recompensa: 4470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 991/10000 | Recompensa: 1400.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 992/10000 | Recompensa: 1990.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 993/10000 | Recompensa: 1160.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 994/10000 | Recompensa: 1880.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 995/10000 | Recompensa: 1540.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 996/10000 | Recompensa: 1650.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 997/10000 | Recompensa: 1490.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 998/10000 | Recompensa: 1510.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 999/10000 | Recompensa: 1230.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1000/10000 | Recompensa: 1570.0 | ε=0.100 | Memoria=100000\n",
      "[CHECKPOINT] Guardado en: /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento/dqn/dqn_galaxian_ep1000.pth\n",
      "[REGISTRO] Gráfica y CSV guardados: /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento/dqn/recompensas_ep1000.png / /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento/dqn/registro_recompensas.csv\n",
      "[DQN] Episodio 1001/10000 | Recompensa: 1600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1002/10000 | Recompensa: 1800.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1003/10000 | Recompensa: 1690.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1004/10000 | Recompensa: 1670.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1005/10000 | Recompensa: 1180.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1006/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1007/10000 | Recompensa: 2580.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1008/10000 | Recompensa: 1170.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1009/10000 | Recompensa: 1120.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1010/10000 | Recompensa: 2090.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1011/10000 | Recompensa: 1620.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1012/10000 | Recompensa: 1040.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1013/10000 | Recompensa: 1890.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1014/10000 | Recompensa: 1160.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1015/10000 | Recompensa: 1490.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1016/10000 | Recompensa: 3200.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1017/10000 | Recompensa: 850.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1018/10000 | Recompensa: 1360.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1019/10000 | Recompensa: 1130.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1020/10000 | Recompensa: 3020.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1021/10000 | Recompensa: 2560.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1022/10000 | Recompensa: 1140.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1023/10000 | Recompensa: 1690.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1024/10000 | Recompensa: 1840.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1025/10000 | Recompensa: 1220.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1026/10000 | Recompensa: 1100.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1027/10000 | Recompensa: 2960.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1028/10000 | Recompensa: 1570.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1029/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1030/10000 | Recompensa: 730.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1031/10000 | Recompensa: 2570.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1032/10000 | Recompensa: 1070.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1033/10000 | Recompensa: 2620.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1034/10000 | Recompensa: 1300.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1035/10000 | Recompensa: 1320.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1036/10000 | Recompensa: 1330.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1037/10000 | Recompensa: 1640.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1038/10000 | Recompensa: 1310.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1039/10000 | Recompensa: 1980.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1040/10000 | Recompensa: 1480.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1041/10000 | Recompensa: 1720.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1042/10000 | Recompensa: 2600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1043/10000 | Recompensa: 3360.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1044/10000 | Recompensa: 420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1045/10000 | Recompensa: 2800.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1046/10000 | Recompensa: 1170.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1047/10000 | Recompensa: 4240.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1048/10000 | Recompensa: 1270.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1049/10000 | Recompensa: 1390.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1050/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1051/10000 | Recompensa: 2320.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1052/10000 | Recompensa: 2690.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1053/10000 | Recompensa: 1630.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1054/10000 | Recompensa: 2840.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1055/10000 | Recompensa: 3000.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1056/10000 | Recompensa: 1560.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1057/10000 | Recompensa: 490.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1058/10000 | Recompensa: 2280.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1059/10000 | Recompensa: 1060.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1060/10000 | Recompensa: 2150.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1061/10000 | Recompensa: 1020.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1062/10000 | Recompensa: 1360.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1063/10000 | Recompensa: 1440.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1064/10000 | Recompensa: 2910.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1065/10000 | Recompensa: 2170.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1066/10000 | Recompensa: 1390.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1067/10000 | Recompensa: 1270.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1068/10000 | Recompensa: 280.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1069/10000 | Recompensa: 1540.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1070/10000 | Recompensa: 2540.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1071/10000 | Recompensa: 1250.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1072/10000 | Recompensa: 1840.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1073/10000 | Recompensa: 1500.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1074/10000 | Recompensa: 1500.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1075/10000 | Recompensa: 920.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1076/10000 | Recompensa: 1350.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1077/10000 | Recompensa: 1950.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1078/10000 | Recompensa: 1390.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1079/10000 | Recompensa: 1460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1080/10000 | Recompensa: 1130.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1081/10000 | Recompensa: 830.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1082/10000 | Recompensa: 1520.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1083/10000 | Recompensa: 1000.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1084/10000 | Recompensa: 640.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1085/10000 | Recompensa: 2430.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1086/10000 | Recompensa: 1740.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1087/10000 | Recompensa: 860.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1088/10000 | Recompensa: 1170.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1089/10000 | Recompensa: 1330.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1090/10000 | Recompensa: 1650.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1091/10000 | Recompensa: 750.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1092/10000 | Recompensa: 3000.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1093/10000 | Recompensa: 2070.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1094/10000 | Recompensa: 2850.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1095/10000 | Recompensa: 1250.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1096/10000 | Recompensa: 2090.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1097/10000 | Recompensa: 2320.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1098/10000 | Recompensa: 1490.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1099/10000 | Recompensa: 1010.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1100/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1101/10000 | Recompensa: 1460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1102/10000 | Recompensa: 2000.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1103/10000 | Recompensa: 1330.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1104/10000 | Recompensa: 1880.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1105/10000 | Recompensa: 1460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1106/10000 | Recompensa: 2730.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1107/10000 | Recompensa: 960.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1108/10000 | Recompensa: 3800.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1109/10000 | Recompensa: 1250.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1110/10000 | Recompensa: 2920.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1111/10000 | Recompensa: 1820.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1112/10000 | Recompensa: 1460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1113/10000 | Recompensa: 2340.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1114/10000 | Recompensa: 1680.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1115/10000 | Recompensa: 1130.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1116/10000 | Recompensa: 3250.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1117/10000 | Recompensa: 1730.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1118/10000 | Recompensa: 1490.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1119/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1120/10000 | Recompensa: 3300.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1121/10000 | Recompensa: 2170.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1122/10000 | Recompensa: 2860.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1123/10000 | Recompensa: 2960.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1124/10000 | Recompensa: 3510.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1125/10000 | Recompensa: 1600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1126/10000 | Recompensa: 3170.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1127/10000 | Recompensa: 1150.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1128/10000 | Recompensa: 3010.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1129/10000 | Recompensa: 950.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1130/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1131/10000 | Recompensa: 2880.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1132/10000 | Recompensa: 1590.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1133/10000 | Recompensa: 550.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1134/10000 | Recompensa: 1850.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1135/10000 | Recompensa: 1430.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1136/10000 | Recompensa: 1990.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1137/10000 | Recompensa: 2410.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1138/10000 | Recompensa: 1270.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1139/10000 | Recompensa: 1500.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1140/10000 | Recompensa: 1600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1141/10000 | Recompensa: 3230.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1142/10000 | Recompensa: 2010.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1143/10000 | Recompensa: 850.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1144/10000 | Recompensa: 1550.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1145/10000 | Recompensa: 1750.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1146/10000 | Recompensa: 3270.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1147/10000 | Recompensa: 1130.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1148/10000 | Recompensa: 1190.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1149/10000 | Recompensa: 1260.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1150/10000 | Recompensa: 1410.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1151/10000 | Recompensa: 1610.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1152/10000 | Recompensa: 2870.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1153/10000 | Recompensa: 1540.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1154/10000 | Recompensa: 1000.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1155/10000 | Recompensa: 3550.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1156/10000 | Recompensa: 950.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1157/10000 | Recompensa: 1790.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1158/10000 | Recompensa: 1460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1159/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1160/10000 | Recompensa: 1690.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1161/10000 | Recompensa: 1310.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1162/10000 | Recompensa: 1680.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1163/10000 | Recompensa: 1800.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1164/10000 | Recompensa: 1410.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1165/10000 | Recompensa: 1220.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1166/10000 | Recompensa: 2960.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1167/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1168/10000 | Recompensa: 1960.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1169/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1170/10000 | Recompensa: 3290.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1171/10000 | Recompensa: 2940.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1172/10000 | Recompensa: 350.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1173/10000 | Recompensa: 1480.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1174/10000 | Recompensa: 2740.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1175/10000 | Recompensa: 1440.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1176/10000 | Recompensa: 3850.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1177/10000 | Recompensa: 2720.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1178/10000 | Recompensa: 1340.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1179/10000 | Recompensa: 3090.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1180/10000 | Recompensa: 2550.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1181/10000 | Recompensa: 3060.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1182/10000 | Recompensa: 1600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1183/10000 | Recompensa: 580.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1184/10000 | Recompensa: 2970.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1185/10000 | Recompensa: 2850.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1186/10000 | Recompensa: 1590.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1187/10000 | Recompensa: 980.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1188/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1189/10000 | Recompensa: 3260.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1190/10000 | Recompensa: 1690.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1191/10000 | Recompensa: 2750.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1192/10000 | Recompensa: 2700.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1193/10000 | Recompensa: 1500.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1194/10000 | Recompensa: 2600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1195/10000 | Recompensa: 2200.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1196/10000 | Recompensa: 2630.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1197/10000 | Recompensa: 1530.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1198/10000 | Recompensa: 1450.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1199/10000 | Recompensa: 3720.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1200/10000 | Recompensa: 2610.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1201/10000 | Recompensa: 4320.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1202/10000 | Recompensa: 2060.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1203/10000 | Recompensa: 1670.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1204/10000 | Recompensa: 1730.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1205/10000 | Recompensa: 2280.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1206/10000 | Recompensa: 3390.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1207/10000 | Recompensa: 1390.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1208/10000 | Recompensa: 1540.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1209/10000 | Recompensa: 1970.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1210/10000 | Recompensa: 1080.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1211/10000 | Recompensa: 1360.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1212/10000 | Recompensa: 1620.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1213/10000 | Recompensa: 2640.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1214/10000 | Recompensa: 3030.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1215/10000 | Recompensa: 2560.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1216/10000 | Recompensa: 2890.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1217/10000 | Recompensa: 1010.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1218/10000 | Recompensa: 1500.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1219/10000 | Recompensa: 2550.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1220/10000 | Recompensa: 2260.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1221/10000 | Recompensa: 1110.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1222/10000 | Recompensa: 880.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1223/10000 | Recompensa: 1160.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1224/10000 | Recompensa: 1460.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1225/10000 | Recompensa: 1510.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1226/10000 | Recompensa: 910.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1227/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1228/10000 | Recompensa: 1340.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1229/10000 | Recompensa: 1500.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1230/10000 | Recompensa: 1520.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1231/10000 | Recompensa: 900.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1232/10000 | Recompensa: 420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1233/10000 | Recompensa: 4570.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1234/10000 | Recompensa: 3100.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1235/10000 | Recompensa: 2700.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1236/10000 | Recompensa: 2220.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1237/10000 | Recompensa: 3000.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1238/10000 | Recompensa: 2390.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1239/10000 | Recompensa: 4070.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1240/10000 | Recompensa: 2290.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1241/10000 | Recompensa: 2770.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1242/10000 | Recompensa: 1630.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1243/10000 | Recompensa: 1600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1244/10000 | Recompensa: 2310.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1245/10000 | Recompensa: 2930.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1246/10000 | Recompensa: 2260.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1247/10000 | Recompensa: 1270.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1248/10000 | Recompensa: 2730.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1249/10000 | Recompensa: 2200.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1250/10000 | Recompensa: 2280.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1251/10000 | Recompensa: 2820.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1252/10000 | Recompensa: 1370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1253/10000 | Recompensa: 1400.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1254/10000 | Recompensa: 1180.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1255/10000 | Recompensa: 1760.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1256/10000 | Recompensa: 2470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1257/10000 | Recompensa: 2600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1258/10000 | Recompensa: 1160.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1259/10000 | Recompensa: 3450.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1260/10000 | Recompensa: 1080.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1261/10000 | Recompensa: 3280.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1262/10000 | Recompensa: 2130.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1263/10000 | Recompensa: 3350.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1264/10000 | Recompensa: 1600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1265/10000 | Recompensa: 3570.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1266/10000 | Recompensa: 1590.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1267/10000 | Recompensa: 830.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1268/10000 | Recompensa: 3450.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1269/10000 | Recompensa: 1560.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1270/10000 | Recompensa: 2060.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1271/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1272/10000 | Recompensa: 1730.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1273/10000 | Recompensa: 2110.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1274/10000 | Recompensa: 1180.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1275/10000 | Recompensa: 2570.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1276/10000 | Recompensa: 1870.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1277/10000 | Recompensa: 1080.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1278/10000 | Recompensa: 1870.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1279/10000 | Recompensa: 3510.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1280/10000 | Recompensa: 2030.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1281/10000 | Recompensa: 1600.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1282/10000 | Recompensa: 1880.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1283/10000 | Recompensa: 1710.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1284/10000 | Recompensa: 2690.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1285/10000 | Recompensa: 3110.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1286/10000 | Recompensa: 1790.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1287/10000 | Recompensa: 2480.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1288/10000 | Recompensa: 3020.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1289/10000 | Recompensa: 1160.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1290/10000 | Recompensa: 1140.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1291/10000 | Recompensa: 3110.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1292/10000 | Recompensa: 3190.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1293/10000 | Recompensa: 1680.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1294/10000 | Recompensa: 5630.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1295/10000 | Recompensa: 2370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1296/10000 | Recompensa: 1820.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1297/10000 | Recompensa: 2230.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1298/10000 | Recompensa: 2500.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1299/10000 | Recompensa: 3420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1300/10000 | Recompensa: 1280.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1301/10000 | Recompensa: 3080.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1302/10000 | Recompensa: 960.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1303/10000 | Recompensa: 4130.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1304/10000 | Recompensa: 1620.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1305/10000 | Recompensa: 3150.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1306/10000 | Recompensa: 1190.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1307/10000 | Recompensa: 1420.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1308/10000 | Recompensa: 1470.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1309/10000 | Recompensa: 1540.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1310/10000 | Recompensa: 1490.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1311/10000 | Recompensa: 2370.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1312/10000 | Recompensa: 1740.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1313/10000 | Recompensa: 4440.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1314/10000 | Recompensa: 3180.0 | ε=0.100 | Memoria=100000\n",
      "[DQN] Episodio 1315/10000 | Recompensa: 2270.0 | ε=0.100 | Memoria=100000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Entrenar DQN con configuración local\u001b[39;00m\n\u001b[1;32m      2\u001b[0m carpeta_dqn \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DIRECTORIO_BASE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdqn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m red_q_entrenada \u001b[38;5;241m=\u001b[39m \u001b[43mentrenar_dqn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectorio_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcarpeta_dqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodios_totales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervalo_guardado\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervalo_graficas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 273\u001b[0m, in \u001b[0;36mentrenar_dqn\u001b[0;34m(directorio_checkpoints, episodios_totales, capacidad_replay, tam_lote, factor_descuento, tasa_aprendizaje, epsilon_inicial, epsilon_final, episodios_decaimiento_eps, intervalo_actualizacion_target, pasos_inicio_entrenamiento, intervalo_guardado, intervalo_graficas, pasos_maximos_por_episodio, semilla_aleatoria)\u001b[0m\n\u001b[1;32m    271\u001b[0m     perdida\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    272\u001b[0m     nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(red_q_principal\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m10.0\u001b[39m)\n\u001b[0;32m--> 273\u001b[0m     \u001b[43moptimizador\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# Actualizar red objetivo\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pasos_globales \u001b[38;5;241m%\u001b[39m intervalo_actualizacion_target \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Galaxian/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:517\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    514\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m             )\n\u001b[0;32m--> 517\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Galaxian/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:82\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 82\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Desktop/Galaxian/.venv/lib/python3.10/site-packages/torch/optim/adam.py:247\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    235\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    237\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    238\u001b[0m         group,\n\u001b[1;32m    239\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m         state_steps,\n\u001b[1;32m    245\u001b[0m     )\n\u001b[0;32m--> 247\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Desktop/Galaxian/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:150\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Galaxian/.venv/lib/python3.10/site-packages/torch/optim/adam.py:953\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 953\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Galaxian/.venv/lib/python3.10/site-packages/torch/optim/adam.py:466\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    462\u001b[0m         exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(\n\u001b[1;32m    463\u001b[0m             grad, grad, value\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    464\u001b[0m         )\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    469\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Entrenar DQN con configuración local\n",
    "carpeta_dqn = os.path.join(DIRECTORIO_BASE, \"dqn\")\n",
    "red_q_entrenada = entrenar_dqn(\n",
    "    directorio_checkpoints=carpeta_dqn, \n",
    "    episodios_totales=10000, \n",
    "    intervalo_guardado=500, \n",
    "    intervalo_graficas=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de A2C (Advantage Actor-Critic) desde cero\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Arquitectura Actor-Crítico\n",
    "# ============================================================\n",
    "class RedA2C(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal con arquitectura Actor-Crítico.\n",
    "    Backbone convolucional compartido con dos cabezas:\n",
    "    - Actor: predice distribución de acciones (logits)\n",
    "    - Crítico: predice valor del estado V(s)\n",
    "    \"\"\"\n",
    "    def __init__(self, forma_entrada: Tuple[int, int, int], num_acciones: int):\n",
    "        super().__init__()\n",
    "        canales, alto, ancho = forma_entrada\n",
    "        self.canales_esperados = canales\n",
    "\n",
    "        self.extractor_caracteristicas = nn.Sequential(\n",
    "            nn.Conv2d(canales, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tensor_prueba = torch.zeros(1, canales, alto, ancho)\n",
    "            tam_aplanado = self.extractor_caracteristicas(tensor_prueba).shape[1]\n",
    "\n",
    "        self.cabeza_actor = nn.Sequential(\n",
    "            nn.Linear(tam_aplanado, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_acciones)\n",
    "        )\n",
    "        self.cabeza_critico = nn.Sequential(\n",
    "            nn.Linear(tam_aplanado, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor_entrada: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Retorna:\n",
    "          - logits de política: (B, num_acciones)\n",
    "          - valores de estado V(s): (B,)\n",
    "        \"\"\"\n",
    "        if tensor_entrada.ndim != 4:\n",
    "            raise ValueError(f\"Esperado tensor 4D, recibido tensor_entrada.ndim={tensor_entrada.ndim}\")\n",
    "\n",
    "        if tensor_entrada.shape[1] != self.canales_esperados and tensor_entrada.shape[-1] == self.canales_esperados:\n",
    "            tensor_entrada = tensor_entrada.permute(0, 3, 1, 2)\n",
    "\n",
    "        tensor_entrada = tensor_entrada.float() / 255.0\n",
    "        caracteristicas = self.extractor_caracteristicas(tensor_entrada)\n",
    "        logits_politica = self.cabeza_actor(caracteristicas)\n",
    "        valores_estado = self.cabeza_critico(caracteristicas).squeeze(-1)\n",
    "        return logits_politica, valores_estado\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Política de Evaluación A2C\n",
    "# ============================================================\n",
    "class PoliticaA2C:\n",
    "    def __init__(self, red_ac: 'RedA2C'):\n",
    "        self.red_ac = red_ac.to(dispositivo)\n",
    "        self.red_ac.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, observacion: np.ndarray, info: dict) -> int:\n",
    "        \"\"\"Estrategia greedy para evaluación\"\"\"\n",
    "        if observacion.ndim != 3:\n",
    "            raise ValueError(f\"Esperado obs 3D, recibido observacion.ndim={observacion.ndim}\")\n",
    "\n",
    "        obs_lote = np.expand_dims(observacion, axis=0)\n",
    "        obs_tensor = torch.from_numpy(obs_lote).to(dispositivo)\n",
    "        logits, _ = self.red_ac(obs_tensor)\n",
    "        probabilidades = torch.softmax(logits, dim=-1)\n",
    "        accion = torch.argmax(probabilidades, dim=-1).item()\n",
    "        return int(accion)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Función de Entrenamiento A2C\n",
    "# ============================================================\n",
    "def entrenar_a2c(\n",
    "    directorio_checkpoints: str,\n",
    "    episodios_totales: int = 500,\n",
    "    factor_descuento: float = 0.99,\n",
    "    tasa_aprendizaje: float = 2.5e-4,\n",
    "    coef_entropia: float = 0.01,\n",
    "    coef_valor: float = 0.5,\n",
    "    longitud_rollout: int = 5,\n",
    "    intervalo_guardado: int = 50,\n",
    "    pasos_maximos_por_episodio: int | None = None,\n",
    "    semilla_aleatoria: int = 123,\n",
    "    lambda_gae: float = 0.95,\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un agente A2C para Galaxian con rollouts n-step y GAE(λ).\n",
    "    \"\"\"\n",
    "    os.makedirs(directorio_checkpoints, exist_ok=True)\n",
    "\n",
    "    entorno_juego = crear_entorno_galaxian(semilla=semilla_aleatoria, modo_render=None)\n",
    "\n",
    "    observacion, _ = entorno_juego.reset()\n",
    "    if observacion.ndim != 3:\n",
    "        entorno_juego.close()\n",
    "        raise ValueError(f\"Observación inesperada: ndim={observacion.ndim}\")\n",
    "\n",
    "    if observacion.shape[0] in (1, 3, 4):\n",
    "        forma_entrada = (observacion.shape[0], observacion.shape[1], observacion.shape[2])\n",
    "    else:\n",
    "        forma_entrada = (observacion.shape[2], observacion.shape[0], observacion.shape[1])\n",
    "\n",
    "    num_acciones = entorno_juego.action_space.n\n",
    "\n",
    "    red_ac = RedA2C(forma_entrada, num_acciones).to(dispositivo)\n",
    "    optimizador = optim.RMSprop(red_ac.parameters(), lr=tasa_aprendizaje, eps=1e-5)\n",
    "\n",
    "    indice_episodio = 0\n",
    "    registro_recompensas: List[float] = []\n",
    "\n",
    "    while indice_episodio < episodios_totales:\n",
    "        observacion, _ = entorno_juego.reset()\n",
    "        finalizado = False\n",
    "        recompensa_episodio = 0.0\n",
    "        pasos_en_episodio = 0\n",
    "\n",
    "        while not finalizado:\n",
    "            # Recolectar rollout\n",
    "            log_probs_lista = []\n",
    "            valores_lista = []\n",
    "            recompensas_lista = []\n",
    "            finalizados_lista = []\n",
    "            entropias_lista = []\n",
    "\n",
    "            for _ in range(longitud_rollout):\n",
    "                if finalizado:\n",
    "                    break\n",
    "\n",
    "                obs_lote = np.expand_dims(observacion, axis=0)\n",
    "                obs_tensor = torch.from_numpy(obs_lote).to(dispositivo)\n",
    "\n",
    "                logits, valor = red_ac(obs_tensor)\n",
    "                probabilidades = torch.softmax(logits, dim=-1)\n",
    "                distribucion = torch.distributions.Categorical(probabilidades)\n",
    "\n",
    "                accion_muestreada = distribucion.sample()\n",
    "                entropia = distribucion.entropy().mean()\n",
    "\n",
    "                siguiente_obs, recompensa_step, terminado, truncado, _ = entorno_juego.step(accion_muestreada.item())\n",
    "                finalizado = terminado or truncado\n",
    "\n",
    "                log_probs_lista.append(distribucion.log_prob(accion_muestreada).squeeze(0))\n",
    "                valores_lista.append(valor.squeeze(0))\n",
    "                recompensas_lista.append(torch.tensor(recompensa_step, dtype=torch.float32, device=dispositivo))\n",
    "                finalizados_lista.append(torch.tensor(float(finalizado), device=dispositivo))\n",
    "                entropias_lista.append(entropia)\n",
    "\n",
    "                observacion = siguiente_obs\n",
    "                recompensa_episodio += float(recompensa_step)\n",
    "                pasos_en_episodio += 1\n",
    "\n",
    "                if pasos_maximos_por_episodio is not None and pasos_en_episodio >= pasos_maximos_por_episodio:\n",
    "                    finalizado = True\n",
    "                    break\n",
    "\n",
    "            # Bootstrap del valor siguiente\n",
    "            if finalizado:\n",
    "                valor_siguiente = torch.zeros(1, device=dispositivo)\n",
    "            else:\n",
    "                obs_siguiente_lote = np.expand_dims(observacion, axis=0)\n",
    "                obs_siguiente_tensor = torch.from_numpy(obs_siguiente_lote).to(dispositivo)\n",
    "                _, valor_siguiente = red_ac(obs_siguiente_tensor)\n",
    "                valor_siguiente = valor_siguiente.detach()\n",
    "\n",
    "            # Calcular retornos con GAE(λ)\n",
    "            retornos_lista = []\n",
    "            ventaja_acumulada = 0\n",
    "            valor_futuro = valor_siguiente\n",
    "            for recomp, terminal, val in zip(reversed(recompensas_lista), reversed(finalizados_lista), reversed(valores_lista)):\n",
    "                valor_futuro = valor_futuro * (1.0 - terminal)\n",
    "                delta_temporal = recomp + factor_descuento * valor_futuro - val\n",
    "                ventaja_acumulada = delta_temporal + factor_descuento * lambda_gae * (1.0 - terminal) * ventaja_acumulada\n",
    "                valor_futuro = val\n",
    "                retornos_lista.insert(0, ventaja_acumulada + val)\n",
    "\n",
    "            retornos_tensor = torch.stack(retornos_lista)\n",
    "            valores_tensor = torch.stack(valores_lista)\n",
    "            log_probs_tensor = torch.stack(log_probs_lista)\n",
    "            entropias_tensor = torch.stack(entropias_lista) if len(entropias_lista) > 0 else torch.tensor(0.0, device=dispositivo)\n",
    "\n",
    "            ventajas = retornos_tensor - valores_tensor\n",
    "\n",
    "            perdida_politica = -(log_probs_tensor * ventajas.detach()).mean()\n",
    "            perdida_valor = ventajas.pow(2).mean()\n",
    "            perdida_entropia = entropias_tensor.mean() if entropias_tensor.ndim > 0 else entropias_tensor\n",
    "\n",
    "            perdida_total = perdida_politica + coef_valor * perdida_valor - coef_entropia * perdida_entropia\n",
    "\n",
    "            optimizador.zero_grad()\n",
    "            perdida_total.backward()\n",
    "            nn.utils.clip_grad_norm_(red_ac.parameters(), 0.5)\n",
    "            optimizador.step()\n",
    "\n",
    "            if finalizado:\n",
    "                break\n",
    "\n",
    "        indice_episodio += 1\n",
    "        registro_recompensas.append(recompensa_episodio)\n",
    "        print(f\"[A2C] Episodio {indice_episodio}/{episodios_totales} | Recompensa: {recompensa_episodio:.1f}\")\n",
    "\n",
    "        # Guardar checkpoint\n",
    "        if indice_episodio % intervalo_guardado == 0:\n",
    "            ruta_ckpt = os.path.join(directorio_checkpoints, f\"a2c_galaxian_ep{indice_episodio}.pth\")\n",
    "            torch.save({\n",
    "                \"red\": red_ac.state_dict(),\n",
    "                \"optimizador\": optimizador.state_dict(),\n",
    "                \"episodio\": indice_episodio,\n",
    "                \"recompensas\": registro_recompensas,\n",
    "                \"forma_entrada\": forma_entrada,\n",
    "                \"num_acciones\": num_acciones,\n",
    "            }, ruta_ckpt)\n",
    "            print(f\"[CHECKPOINT] Guardado en: {ruta_ckpt}\")\n",
    "\n",
    "    entorno_juego.close()\n",
    "\n",
    "    ruta_final = os.path.join(directorio_checkpoints, \"a2c_galaxian_final.pth\")\n",
    "    torch.save(red_ac.state_dict(), ruta_final)\n",
    "    print(f\"[FINALIZADO] Modelo final guardado en: {ruta_final}\")\n",
    "\n",
    "    return red_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar A2C con configuración local\n",
    "carpeta_a2c = os.path.join(DIRECTORIO_BASE, \"a2c\")\n",
    "red_a2c_entrenada = entrenar_a2c(\n",
    "    directorio_checkpoints=carpeta_a2c, \n",
    "    episodios_totales=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dueling-ddqn-per-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de Dueling Double DQN con Prioritized Experience Replay (PER)\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Árboles de Segmentos para PER\n",
    "# ===========================\n",
    "class ArbolSegmentos:\n",
    "    def __init__(self, capacidad, funcion_reduccion):\n",
    "        assert capacidad > 0 and (capacidad & (capacidad - 1)) == 0, \\\n",
    "            \"Capacidad debe ser potencia de 2\"\n",
    "        self.capacidad = capacidad\n",
    "        self.arbol = np.zeros(2 * capacidad, dtype=np.float32)\n",
    "        self.funcion_reduccion = funcion_reduccion\n",
    "\n",
    "    def actualizar(self, indice, valor):\n",
    "        i = indice + self.capacidad\n",
    "        self.arbol[i] = valor\n",
    "        i //= 2\n",
    "        while i >= 1:\n",
    "            self.arbol[i] = self.funcion_reduccion(self.arbol[2 * i], self.arbol[2 * i + 1])\n",
    "            i //= 2\n",
    "\n",
    "    def reducir(self, inicio, fin):\n",
    "        resultado_izq = None\n",
    "        resultado_der = None\n",
    "        inicio += self.capacidad\n",
    "        fin += self.capacidad\n",
    "        while inicio <= fin:\n",
    "            if (inicio % 2) == 1:\n",
    "                resultado_izq = self.arbol[inicio] if resultado_izq is None else self.funcion_reduccion(resultado_izq, self.arbol[inicio])\n",
    "                inicio += 1\n",
    "            if (fin % 2) == 0:\n",
    "                resultado_der = self.arbol[fin] if resultado_der is None else self.funcion_reduccion(self.arbol[fin], resultado_der)\n",
    "                fin -= 1\n",
    "            inicio //= 2\n",
    "            fin //= 2\n",
    "        if resultado_izq is None:\n",
    "            return resultado_der\n",
    "        if resultado_der is None:\n",
    "            return resultado_izq\n",
    "        return self.funcion_reduccion(resultado_izq, resultado_der)\n",
    "\n",
    "    def __getitem__(self, indice):\n",
    "        return self.arbol[indice + self.capacidad]\n",
    "\n",
    "\n",
    "class ArbolSuma(ArbolSegmentos):\n",
    "    def __init__(self, capacidad):\n",
    "        super().__init__(capacidad, fn=lambda a, b: a + b)\n",
    "\n",
    "    def suma_total(self):\n",
    "        return self.arbol[1]\n",
    "\n",
    "    def encontrar_suma_prefijo(self, suma_objetivo):\n",
    "        \"\"\"Encuentra índice i tal que sum(0..i) >= suma_objetivo\"\"\"\n",
    "        idx = 1\n",
    "        while idx < self.capacidad:\n",
    "            izquierda = 2 * idx\n",
    "            if self.arbol[izquierda] >= suma_objetivo:\n",
    "                idx = izquierda\n",
    "            else:\n",
    "                suma_objetivo -= self.arbol[izquierda]\n",
    "                idx = izquierda + 1\n",
    "        return idx - self.capacidad\n",
    "\n",
    "\n",
    "class ArbolMinimo(ArbolSegmentos):\n",
    "    def __init__(self, capacidad):\n",
    "        super().__init__(capacidad, fn=min)\n",
    "\n",
    "    def minimo_total(self):\n",
    "        return self.arbol[1]\n",
    "\n",
    "\n",
    "# =======================================\n",
    "#  Buffer de Replay Priorizado\n",
    "# =======================================\n",
    "class BufferReplayPriorizado:\n",
    "    def __init__(self, capacidad_maxima: int, alfa_prioridad: float = 0.6, epsilon_per: float = 1e-6):\n",
    "        potencia_2 = 1\n",
    "        while potencia_2 < capacidad_maxima:\n",
    "            potencia_2 *= 2\n",
    "        self.capacidad = potencia_2\n",
    "        self.alfa_prioridad = alfa_prioridad\n",
    "        self.epsilon_per = epsilon_per\n",
    "\n",
    "        self.posicion = 0\n",
    "        self.tamanio = 0\n",
    "\n",
    "        self.estados = [None] * self.capacidad\n",
    "        self.acciones = np.zeros(self.capacidad, dtype=np.int64)\n",
    "        self.recompensas = np.zeros(self.capacidad, dtype=np.float32)\n",
    "        self.estados_siguientes = [None] * self.capacidad\n",
    "        self.terminados = np.zeros(self.capacidad, dtype=np.bool_)\n",
    "\n",
    "        self.arbol_suma = ArbolSuma(self.capacidad)\n",
    "        self.arbol_minimo = ArbolMinimo(self.capacidad)\n",
    "        self.prioridad_maxima = 1.0\n",
    "\n",
    "        for i in range(self.capacidad):\n",
    "            self.arbol_suma.actualizar(i, 0.0)\n",
    "            self.arbol_minimo.actualizar(i, float(\"inf\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tamanio\n",
    "\n",
    "    def agregar(self, estado, accion, recompensa, estado_sig, terminal):\n",
    "        idx = self.posicion\n",
    "        self.estados[idx] = estado\n",
    "        self.acciones[idx] = accion\n",
    "        self.recompensas[idx] = recompensa\n",
    "        self.estados_siguientes[idx] = estado_sig\n",
    "        self.terminados[idx] = terminal\n",
    "\n",
    "        prioridad = (self.prioridad_maxima + self.epsilon_per) ** self.alfa_prioridad\n",
    "        self.arbol_suma.actualizar(idx, prioridad)\n",
    "        self.arbol_minimo.actualizar(idx, prioridad)\n",
    "\n",
    "        self.posicion = (self.posicion + 1) % self.capacidad\n",
    "        self.tamanio = min(self.tamanio + 1, self.capacidad)\n",
    "\n",
    "    def muestrear(self, tam_lote: int, beta_importancia: float = 0.4):\n",
    "        \"\"\"Devuelve (indices, pesos_is, batch)\"\"\"\n",
    "        indices_salida = []\n",
    "        estados_salida = []\n",
    "        acciones_salida = np.empty(tam_lote, dtype=np.int64)\n",
    "        recompensas_salida = np.empty(tam_lote, dtype=np.float32)\n",
    "        estados_sig_salida = []\n",
    "        terminados_salida = np.empty(tam_lote, dtype=np.float32)\n",
    "\n",
    "        suma_total = self.arbol_suma.suma_total()\n",
    "        segmento = suma_total / tam_lote\n",
    "        probabilidad_minima = self.arbol_minimo.minimo_total() / suma_total\n",
    "        peso_maximo = (probabilidad_minima * self.tamanio) ** (-beta_importancia)\n",
    "\n",
    "        for i in range(tam_lote):\n",
    "            a = segmento * i\n",
    "            b = segmento * (i + 1)\n",
    "            masa = random.random() * (b - a) + a\n",
    "            idx = self.arbol_suma.encontrar_suma_prefijo(masa)\n",
    "            indices_salida.append(idx)\n",
    "            estados_salida.append(self.estados[idx])\n",
    "            acciones_salida[i] = self.acciones[idx]\n",
    "            recompensas_salida[i] = self.recompensas[idx]\n",
    "            estados_sig_salida.append(self.estados_siguientes[idx])\n",
    "            terminados_salida[i] = float(self.terminados[idx])\n",
    "\n",
    "        # Pesos de importance sampling\n",
    "        probabilidades = np.array([self.arbol_suma[idx] / suma_total for idx in indices_salida], dtype=np.float32)\n",
    "        pesos_is = (probabilidades * self.tamanio) ** (-beta_importancia)\n",
    "        pesos_is = pesos_is / peso_maximo\n",
    "        pesos_is = pesos_is.astype(np.float32)\n",
    "\n",
    "        return np.array(indices_salida), pesos_is, (np.array(estados_salida), acciones_salida, recompensas_salida, np.array(estados_sig_salida), terminados_salida)\n",
    "\n",
    "    def actualizar_prioridades(self, indices, prioridades):\n",
    "        for idx, prioridad in zip(indices, prioridades):\n",
    "            prioridad = float(prioridad + self.epsilon_per)\n",
    "            self.arbol_suma.actualizar(idx, prioridad ** self.alfa_prioridad)\n",
    "            self.arbol_minimo.actualizar(idx, prioridad ** self.alfa_prioridad)\n",
    "            self.prioridad_maxima = max(self.prioridad_maxima, prioridad)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Arquitectura Dueling DQN\n",
    "# ===========================\n",
    "class RedDuelingDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Arquitectura Dueling DQN:\n",
    "    Q(s,a) = V(s) + (A(s,a) - mean(A))\n",
    "    Dos streams separados para Valor y Ventaja.\n",
    "    \"\"\"\n",
    "    def __init__(self, forma_entrada: Tuple[int, int, int], num_acciones: int):\n",
    "        super().__init__()\n",
    "        canales, alto, ancho = forma_entrada\n",
    "        self.canales_esperados = canales\n",
    "\n",
    "        self.extractor_caracteristicas = nn.Sequential(\n",
    "            nn.Conv2d(canales, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            tam_aplanado = self.extractor_caracteristicas(torch.zeros(1, canales, alto, ancho)).shape[1]\n",
    "\n",
    "        self.stream_valor = nn.Sequential(\n",
    "            nn.Linear(tam_aplanado, 512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        self.stream_ventaja = nn.Sequential(\n",
    "            nn.Linear(tam_aplanado, 512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_acciones)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor_entrada):\n",
    "        if tensor_entrada.ndim != 4:\n",
    "            raise ValueError(f\"tensor_entrada.ndim={tensor_entrada.ndim}, esperado 4\")\n",
    "        if tensor_entrada.shape[1] != self.canales_esperados and tensor_entrada.shape[-1] == self.canales_esperados:\n",
    "            tensor_entrada = tensor_entrada.permute(0, 3, 1, 2)\n",
    "        tensor_entrada = tensor_entrada.float() / 255.0\n",
    "        caracteristicas = self.extractor_caracteristicas(tensor_entrada)\n",
    "        valor_estado = self.stream_valor(caracteristicas)\n",
    "        ventaja_acciones = self.stream_ventaja(caracteristicas)\n",
    "        valores_q = valor_estado + (ventaja_acciones - ventaja_acciones.mean(dim=1, keepdim=True))\n",
    "        return valores_q\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Política de Evaluación\n",
    "# ===========================\n",
    "class PoliticaDQN:\n",
    "    def __init__(self, red_q: 'RedDuelingDQN'):\n",
    "        self.red_q = red_q.to(dispositivo)\n",
    "        self.red_q.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, observacion: np.ndarray, info: dict) -> int:\n",
    "        if observacion.ndim != 3:\n",
    "            raise ValueError(\"obs debe ser 3D\")\n",
    "        obs_lote = np.expand_dims(observacion, axis=0)\n",
    "        obs_tensor = torch.from_numpy(obs_lote).to(dispositivo)\n",
    "        valores_q = self.red_q(obs_tensor)\n",
    "        return int(torch.argmax(valores_q, dim=1).item())\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Utilidades de logging\n",
    "# ===========================\n",
    "def _calcular_media_movil(valores: List[float], ventana: int = 100):\n",
    "    if len(valores) == 0:\n",
    "        return []\n",
    "    resultado = []\n",
    "    suma_acum = 0.0\n",
    "    cola = []\n",
    "    for v in valores:\n",
    "        cola.append(v)\n",
    "        suma_acum += v\n",
    "        if len(cola) > ventana:\n",
    "            suma_acum -= cola.pop(0)\n",
    "        resultado.append(suma_acum / len(cola))\n",
    "    return resultado\n",
    "\n",
    "def _guardar_grafica_y_csv(dir_checkpoints: str, recompensas: List[float], episodio: int):\n",
    "    os.makedirs(dir_checkpoints, exist_ok=True)\n",
    "    \n",
    "    # CSV\n",
    "    ruta_csv = os.path.join(dir_checkpoints, \"registro_recompensas.csv\")\n",
    "    archivo_nuevo = not os.path.exists(ruta_csv)\n",
    "    with open(ruta_csv, \"a\", newline=\"\") as archivo:\n",
    "        escritor = csv.writer(archivo)\n",
    "        if archivo_nuevo:\n",
    "            escritor.writerow([\"episodio\", \"recompensa\"])\n",
    "        escritor.writerow([episodio, recompensas[-1]])\n",
    "\n",
    "    # Gráfica PNG con nuevos colores\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(recompensas, label=\"Recompensa\", color=\"#FF6B35\", linewidth=1.2, alpha=0.7)\n",
    "    media_movil = _calcular_media_movil(recompensas, ventana=100)\n",
    "    if len(media_movil) > 0:\n",
    "        plt.plot(media_movil, label=\"Media Móvil (100)\", color=\"#00A896\", linewidth=2.5)\n",
    "    plt.xlabel(\"Episodio\", fontsize=12)\n",
    "    plt.ylabel(\"Recompensa Total\", fontsize=12)\n",
    "    plt.title(\"Progreso de Entrenamiento - Dueling DDQN + PER\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    ruta_png = os.path.join(dir_checkpoints, f\"recompensas_ep{episodio}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ruta_png, dpi=120)\n",
    "    plt.close()\n",
    "    print(f\"[REGISTRO] Gráfica y CSV guardados: {ruta_png} / {ruta_csv}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Función de Entrenamiento\n",
    "# ===========================\n",
    "def entrenar_dueling_ddqn_per(\n",
    "    directorio_checkpoints: str,\n",
    "    episodios_totales: int = 50000,\n",
    "    capacidad_buffer: int = 100_000,\n",
    "    tam_lote: int = 32,\n",
    "    factor_descuento: float = 0.99,\n",
    "    tasa_aprendizaje: float = 1e-4,\n",
    "    epsilon_inicial: float = 1.0,\n",
    "    epsilon_final: float = 0.1,\n",
    "    episodios_decaimiento_eps: int = 30000,\n",
    "    intervalo_actualizacion_target: int = 1000,\n",
    "    pasos_inicio_entrenamiento: int = 10_000,\n",
    "    alfa_per: float = 0.6,\n",
    "    beta_per_inicial: float = 0.4,\n",
    "    beta_per_final: float = 1.0,\n",
    "    episodios_annealing_beta: int = 50000,\n",
    "    epsilon_per: float = 1e-6,\n",
    "    intervalo_guardado: int = 500,\n",
    "    intervalo_graficas: int = 200,\n",
    "    pasos_maximos_por_episodio: int | None = None,\n",
    "    semilla_aleatoria: int = 42,\n",
    "):\n",
    "    os.makedirs(directorio_checkpoints, exist_ok=True)\n",
    "\n",
    "    entorno_juego = crear_entorno_galaxian(semilla=semilla_aleatoria, modo_render=None)\n",
    "\n",
    "    observacion, _ = entorno_juego.reset()\n",
    "    if observacion.ndim != 3:\n",
    "        entorno_juego.close()\n",
    "        raise ValueError(\"obs.ndim inesperado\")\n",
    "    if observacion.shape[0] in (1, 3, 4):\n",
    "        forma_entrada = (observacion.shape[0], observacion.shape[1], observacion.shape[2])\n",
    "    else:\n",
    "        forma_entrada = (observacion.shape[2], observacion.shape[0], observacion.shape[1])\n",
    "    num_acciones = entorno_juego.action_space.n\n",
    "\n",
    "    red_q_principal = RedDuelingDQN(forma_entrada, num_acciones).to(dispositivo)\n",
    "    red_q_objetivo = RedDuelingDQN(forma_entrada, num_acciones).to(dispositivo)\n",
    "    red_q_objetivo.load_state_dict(red_q_principal.state_dict())\n",
    "    red_q_objetivo.eval()\n",
    "\n",
    "    optimizador = optim.Adam(red_q_principal.parameters(), lr=tasa_aprendizaje)\n",
    "    memoria_experiencias = BufferReplayPriorizado(capacidad_buffer, alfa_prioridad=alfa_per, epsilon_per=epsilon_per)\n",
    "\n",
    "    registro_recompensas: List[float] = []\n",
    "    pasos_globales = 0\n",
    "\n",
    "    def calcular_epsilon(ep):\n",
    "        if ep >= episodios_decaimiento_eps:\n",
    "            return epsilon_final\n",
    "        fraccion = ep / float(episodios_decaimiento_eps)\n",
    "        return epsilon_inicial + fraccion * (epsilon_final - epsilon_inicial)\n",
    "\n",
    "    def calcular_beta(ep):\n",
    "        fraccion = min(1.0, ep / float(episodios_annealing_beta))\n",
    "        return beta_per_inicial + fraccion * (beta_per_final - beta_per_inicial)\n",
    "\n",
    "    for episodio_actual in range(1, episodios_totales + 1):\n",
    "        observacion, _ = entorno_juego.reset()\n",
    "        finalizado = False\n",
    "        recompensa_total = 0.0\n",
    "        pasos_en_episodio = 0\n",
    "\n",
    "        epsilon_actual = calcular_epsilon(episodio_actual)\n",
    "        beta_actual = calcular_beta(episodio_actual)\n",
    "\n",
    "        while not finalizado:\n",
    "            pasos_globales += 1\n",
    "            pasos_en_episodio += 1\n",
    "\n",
    "            # Política ε-greedy\n",
    "            if random.random() < epsilon_actual:\n",
    "                accion_elegida = entorno_juego.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    obs_lote = np.expand_dims(observacion, axis=0)\n",
    "                    obs_tensor = torch.from_numpy(obs_lote).to(dispositivo)\n",
    "                    valores_q = red_q_principal(obs_tensor)\n",
    "                    accion_elegida = int(torch.argmax(valores_q, dim=1).item())\n",
    "\n",
    "            siguiente_obs, recompensa_step, terminado, truncado, _ = entorno_juego.step(accion_elegida)\n",
    "            finalizado = terminado or truncado\n",
    "            recompensa_total += float(recompensa_step)\n",
    "\n",
    "            memoria_experiencias.agregar(observacion, accion_elegida, recompensa_step, siguiente_obs, finalizado)\n",
    "            observacion = siguiente_obs\n",
    "\n",
    "            # Entrenamiento\n",
    "            if len(memoria_experiencias) >= pasos_inicio_entrenamiento:\n",
    "                indices, pesos_is, lote = memoria_experiencias.muestrear(tam_lote, beta_importancia=beta_actual)\n",
    "                estados, acciones, recompensas, estados_sig, terminados = lote\n",
    "\n",
    "                estados_t = torch.from_numpy(estados).to(dispositivo)\n",
    "                estados_sig_t = torch.from_numpy(estados_sig).to(dispositivo)\n",
    "                acciones_t = torch.from_numpy(acciones).long().to(dispositivo)\n",
    "                recompensas_t = torch.from_numpy(recompensas).float().to(dispositivo)\n",
    "                terminados_t = torch.from_numpy(terminados).float().to(dispositivo)\n",
    "                pesos_is_t = torch.from_numpy(pesos_is).float().to(dispositivo)\n",
    "\n",
    "                # Q(s,a) actual\n",
    "                valores_q = red_q_principal(estados_t).gather(1, acciones_t.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Double DQN: selección con red principal, evaluación con red objetivo\n",
    "                with torch.no_grad():\n",
    "                    q_principal_sig = red_q_principal(estados_sig_t)\n",
    "                    acciones_optimas = torch.argmax(q_principal_sig, dim=1, keepdim=True)\n",
    "\n",
    "                    q_objetivo_sig = red_q_objetivo(estados_sig_t)\n",
    "                    q_siguientes = q_objetivo_sig.gather(1, acciones_optimas).squeeze(1)\n",
    "\n",
    "                    q_objetivo_valores = recompensas_t + factor_descuento * q_siguientes * (1.0 - terminados_t)\n",
    "\n",
    "                errores_td = q_objetivo_valores - valores_q\n",
    "                perdida = (pesos_is_t * errores_td.pow(2)).mean()\n",
    "\n",
    "                optimizador.zero_grad()\n",
    "                perdida.backward()\n",
    "                nn.utils.clip_grad_norm_(red_q_principal.parameters(), 10.0)\n",
    "                optimizador.step()\n",
    "\n",
    "                # Actualizar prioridades\n",
    "                nuevas_prioridades = errores_td.detach().abs().cpu().numpy() + epsilon_per\n",
    "                memoria_experiencias.actualizar_prioridades(indices, nuevas_prioridades)\n",
    "\n",
    "            # Sincronizar red objetivo\n",
    "            if pasos_globales % intervalo_actualizacion_target == 0:\n",
    "                red_q_objetivo.load_state_dict(red_q_principal.state_dict())\n",
    "\n",
    "            if pasos_maximos_por_episodio is not None and pasos_en_episodio >= pasos_maximos_por_episodio:\n",
    "                break\n",
    "\n",
    "        registro_recompensas.append(recompensa_total)\n",
    "        print(f\"[Dueling-DDQN+PER] Ep {episodio_actual}/{episodios_totales} | R: {recompensa_total:.1f} | ε={epsilon_actual:.3f} | β={beta_actual:.3f} | Memoria={len(memoria_experiencias)}\")\n",
    "\n",
    "        # Guardar checkpoint\n",
    "        if episodio_actual % intervalo_guardado == 0:\n",
    "            ruta_ckpt = os.path.join(directorio_checkpoints, f\"dueling_ddqn_per_ep{episodio_actual}.pth\")\n",
    "            torch.save({\n",
    "                \"red_q\": red_q_principal.state_dict(),\n",
    "                \"red_objetivo\": red_q_objetivo.state_dict(),\n",
    "                \"optimizador\": optimizador.state_dict(),\n",
    "                \"episodio\": episodio_actual,\n",
    "                \"pasos_globales\": pasos_globales,\n",
    "                \"recompensas\": registro_recompensas,\n",
    "                \"forma_entrada\": forma_entrada,\n",
    "                \"num_acciones\": num_acciones,\n",
    "            }, ruta_ckpt)\n",
    "            print(f\"[CHECKPOINT] Guardado: {ruta_ckpt}\")\n",
    "\n",
    "        # Guardar gráficas\n",
    "        if episodio_actual % intervalo_graficas == 0:\n",
    "            _guardar_grafica_y_csv(directorio_checkpoints, registro_recompensas, episodio_actual)\n",
    "\n",
    "    entorno_juego.close()\n",
    "\n",
    "    ruta_final = os.path.join(directorio_checkpoints, \"dueling_ddqn_per_final.pth\")\n",
    "    torch.save(red_q_principal.state_dict(), ruta_final)\n",
    "    print(f\"[FINALIZADO] Modelo final guardado en: {ruta_final}\")\n",
    "    _guardar_grafica_y_csv(directorio_checkpoints, registro_recompensas, episodios_totales)\n",
    "\n",
    "    return red_q_principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-dueling-ddqn-per",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar Dueling DDQN + PER con configuración local\n",
    "carpeta_dueling = os.path.join(DIRECTORIO_BASE, \"dueling_ddqn_per\")\n",
    "modelo_dueling = entrenar_dueling_ddqn_per(\n",
    "    directorio_checkpoints=carpeta_dueling,\n",
    "    episodios_totales=100000,\n",
    "    intervalo_guardado=500,\n",
    "    intervalo_graficas=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resume-training-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Reanudar entrenamiento desde checkpoint (opcional)\n",
    "# Descomenta y modifica según necesites\n",
    "\n",
    "# ruta_checkpoint = os.path.join(carpeta_dueling, \"dueling_ddqn_per_ep9500.pth\")\n",
    "# \n",
    "# # Cargar checkpoint\n",
    "# checkpoint = torch.load(ruta_checkpoint)\n",
    "# \n",
    "# # Crear red y cargar pesos\n",
    "# forma_entrada = tuple(checkpoint['forma_entrada'])\n",
    "# num_acciones = checkpoint['num_acciones']\n",
    "# red_reanudada = RedDuelingDQN(forma_entrada, num_acciones).to(dispositivo)\n",
    "# red_reanudada.load_state_dict(checkpoint['red_q'])\n",
    "# \n",
    "# episodio_inicio = checkpoint['episodio']\n",
    "# print(f\"Checkpoint cargado desde episodio: {episodio_inicio}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
