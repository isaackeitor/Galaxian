{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "config-local-paths",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio base configurado: /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento\n",
      "Los resultados se guardarán en: /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DIRECTORIO_BASE = os.path.join(os.getcwd(), \"resultados_entrenamiento\")\n",
    "os.makedirs(DIRECTORIO_BASE, exist_ok=True)\n",
    "print(\"Directorio base configurado:\", DIRECTORIO_BASE)\n",
    "print(\"Los resultados se guardarán en:\", os.path.abspath(DIRECTORIO_BASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "environment-wrappers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidades para preprocesamiento del entorno Atari\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class EnvolturaPreprocesamiento(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Preprocesamiento personalizado de Atari:\n",
    "    - Frame skipping (salto de cuadros)\n",
    "    - Conversión a escala de grises\n",
    "    - Reescalado a 84x84 píxeles\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, entorno_base, salto_cuadros=4, tam_pantalla=84, escala_grises=True):\n",
    "        super().__init__(entorno_base)\n",
    "        self.salto_cuadros = salto_cuadros\n",
    "        self.tam_pantalla = tam_pantalla\n",
    "        self.escala_grises = escala_grises\n",
    "\n",
    "        forma_obs = (tam_pantalla, tam_pantalla)\n",
    "        if not escala_grises:\n",
    "            forma_obs += (3,)\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=forma_obs, dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def procesar_cuadro(self, cuadro):\n",
    "        \"\"\"Convierte y reescala el cuadro\"\"\"\n",
    "        if self.escala_grises:\n",
    "            cuadro = cv2.cvtColor(cuadro, cv2.COLOR_RGB2GRAY)\n",
    "            cuadro = cv2.resize(cuadro, (self.tam_pantalla, self.tam_pantalla), interpolation=cv2.INTER_AREA)\n",
    "            return cuadro\n",
    "        else:\n",
    "            cuadro = cv2.resize(cuadro, (self.tam_pantalla, self.tam_pantalla), interpolation=cv2.INTER_AREA)\n",
    "            return cuadro\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"Ejecuta múltiples pasos y acumula recompensas\"\"\"\n",
    "        recompensa_acumulada = 0.0\n",
    "        terminado = truncado = False\n",
    "        for _ in range(self.salto_cuadros):\n",
    "            observacion_raw, recomp, term, trunc, informacion = self.env.step(accion)\n",
    "            recompensa_acumulada += recomp\n",
    "            terminado |= term\n",
    "            truncado |= trunc\n",
    "            if terminado or truncado:\n",
    "                break\n",
    "        cuadro_procesado = self.procesar_cuadro(observacion_raw)\n",
    "        return cuadro_procesado, recompensa_acumulada, terminado, truncado, informacion\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observacion_raw, informacion = self.env.reset(**kwargs)\n",
    "        cuadro_procesado = self.procesar_cuadro(observacion_raw)\n",
    "        return cuadro_procesado, informacion\n",
    "\n",
    "\n",
    "class EnvolturaApilamiento(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Apila los últimos N cuadros para capturar dinámica temporal.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, entorno_base, num_apilar=4):\n",
    "        super().__init__(entorno_base)\n",
    "        self.num_apilar = num_apilar\n",
    "        self.cuadros_memoria = deque([], maxlen=num_apilar)\n",
    "        bajo = np.repeat(entorno_base.observation_space.low[np.newaxis, ...], num_apilar, axis=0)\n",
    "        alto = np.repeat(entorno_base.observation_space.high[np.newaxis, ...], num_apilar, axis=0)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=bajo.min(), high=alto.max(), dtype=entorno_base.observation_space.dtype, \n",
    "            shape=(num_apilar, *entorno_base.observation_space.shape)\n",
    "        )\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observacion, informacion = self.env.reset(**kwargs)\n",
    "        for _ in range(self.num_apilar):\n",
    "            self.cuadros_memoria.append(observacion)\n",
    "        return self._obtener_obs(), informacion\n",
    "\n",
    "    def step(self, accion):\n",
    "        observacion, recompensa, terminado, truncado, informacion = self.env.step(accion)\n",
    "        self.cuadros_memoria.append(observacion)\n",
    "        return self._obtener_obs(), recompensa, terminado, truncado, informacion\n",
    "\n",
    "    def _obtener_obs(self):\n",
    "        return np.stack(self.cuadros_memoria, axis=0)\n",
    "\n",
    "\n",
    "def crear_entorno_galaxian(semilla: int | None = None, modo_render: str | None = None):\n",
    "    \"\"\"\n",
    "    Crea el entorno ALE/Galaxian-v5 con preprocesamiento completo.\n",
    "    - Reescalado a 84x84\n",
    "    - Escala de grises\n",
    "    - Frame skip = 4\n",
    "    - Frame stack = 4\n",
    "    \"\"\"\n",
    "    entorno = gym.make(\"ALE/Galaxian-v5\", render_mode=modo_render)\n",
    "    if semilla is not None:\n",
    "        entorno.reset(seed=semilla)\n",
    "\n",
    "    entorno = EnvolturaPreprocesamiento(entorno, salto_cuadros=4, tam_pantalla=84, escala_grises=True)\n",
    "    entorno = EnvolturaApilamiento(entorno, num_apilar=4)\n",
    "\n",
    "    return entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dqn-implementation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Usando GPU de Apple Silicon (MPS)\n"
     ]
    }
   ],
   "source": [
    "# Implementación de DQN (Deep Q-Network) desde cero\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import Tuple, Deque, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # backend sin interfaz gráfica para guardar PNG\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Detectar dispositivo: MPS (Mac M1/M2/M3/M4) > CUDA > CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    dispositivo = torch.device(\"mps\")\n",
    "    print(\"✅ Usando GPU de Apple Silicon (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    dispositivo = torch.device(\"cuda\")\n",
    "    print(\"✅ Usando GPU CUDA\")\n",
    "else:\n",
    "    dispositivo = torch.device(\"cpu\")\n",
    "    print(\"⚠️ Usando CPU (entrenamiento será más lento)\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Arquitectura de Red Neuronal DQN\n",
    "# ============================================================\n",
    "class RedDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Red convolucional profunda para aproximar Q(s,a).\n",
    "    Arquitectura estilo Nature DQN.\n",
    "    Formato esperado: (Batch, Canales, Alto, Ancho)\n",
    "    \"\"\"\n",
    "    def __init__(self, forma_entrada: Tuple[int, int, int], num_acciones: int):\n",
    "        super().__init__()\n",
    "        canales, alto, ancho = forma_entrada\n",
    "        self.canales_esperados = canales\n",
    "\n",
    "        self.extractor_caracteristicas = nn.Sequential(\n",
    "            nn.Conv2d(canales, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Inferir tamaño del vector aplanado\n",
    "        with torch.no_grad():\n",
    "            tensor_prueba = torch.zeros(1, canales, alto, ancho)\n",
    "            tam_aplanado = self.extractor_caracteristicas(tensor_prueba).shape[1]\n",
    "\n",
    "        self.cabeza_valores_q = nn.Sequential(\n",
    "            nn.Linear(tam_aplanado, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_acciones)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor_entrada: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        tensor_entrada: (B,C,H,W) o (B,H,W,C)\n",
    "        Retorna: Q(s,·) de forma (B, num_acciones)\n",
    "        \"\"\"\n",
    "        if tensor_entrada.ndim != 4:\n",
    "            raise ValueError(f\"Esperado tensor 4D, recibido tensor_entrada.ndim={tensor_entrada.ndim}\")\n",
    "\n",
    "        # Auto-detectar formato y permutar si es necesario\n",
    "        if tensor_entrada.shape[1] != self.canales_esperados and tensor_entrada.shape[-1] == self.canales_esperados:\n",
    "            tensor_entrada = tensor_entrada.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Normalizar a rango [0,1]\n",
    "        tensor_entrada = tensor_entrada.float() / 255.0\n",
    "        caracteristicas = self.extractor_caracteristicas(tensor_entrada)\n",
    "        return self.cabeza_valores_q(caracteristicas)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Buffer de Replay (Memoria de Experiencias)\n",
    "# ============================================================\n",
    "class BufferReplay:\n",
    "    def __init__(self, capacidad_maxima: int):\n",
    "        self.almacen: Deque = deque(maxlen=capacidad_maxima)\n",
    "\n",
    "    def agregar(self, estado, accion, recompensa, estado_sig, terminado):\n",
    "        self.almacen.append((estado, accion, recompensa, estado_sig, terminado))\n",
    "\n",
    "    def muestrear(self, tam_lote: int):\n",
    "        lote = random.sample(self.almacen, tam_lote)\n",
    "        estados, acciones, recompensas, estados_sig, terminados = map(np.array, zip(*lote))\n",
    "        return estados, acciones, recompensas, estados_sig, terminados\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.almacen)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Política de Evaluación DQN\n",
    "# ============================================================\n",
    "class PoliticaDQN:\n",
    "    def __init__(self, red_q: 'RedDQN'):\n",
    "        self.red_q = red_q.to(dispositivo)\n",
    "        self.red_q.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, observacion: np.ndarray, info: dict) -> int:\n",
    "        \"\"\"Selecciona acción greedy (argmax Q)\"\"\"\n",
    "        if observacion.ndim != 3:\n",
    "            raise ValueError(f\"Esperado obs 3D, recibido observacion.ndim={observacion.ndim}\")\n",
    "\n",
    "        obs_lote = np.expand_dims(observacion, axis=0)\n",
    "        obs_tensor = torch.from_numpy(obs_lote).to(dispositivo)\n",
    "        valores_q = self.red_q(obs_tensor)\n",
    "        return int(torch.argmax(valores_q, dim=1).item())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Funciones de Logging y Visualización\n",
    "# ============================================================\n",
    "def _calcular_media_movil(valores: List[float], ventana: int = 100):\n",
    "    \"\"\"Calcula media móvil con ventana deslizante\"\"\"\n",
    "    if len(valores) == 0:\n",
    "        return []\n",
    "    resultado = []\n",
    "    suma_acum = 0.0\n",
    "    cola = []\n",
    "    for v in valores:\n",
    "        cola.append(v)\n",
    "        suma_acum += v\n",
    "        if len(cola) > ventana:\n",
    "            suma_acum -= cola.pop(0)\n",
    "        resultado.append(suma_acum / len(cola))\n",
    "    return resultado\n",
    "\n",
    "def _guardar_grafica_y_csv(dir_checkpoints: str, recompensas: List[float], episodio: int):\n",
    "    \"\"\"Guarda gráfica PNG y registro CSV de recompensas\"\"\"\n",
    "    os.makedirs(dir_checkpoints, exist_ok=True)\n",
    "\n",
    "    # Guardar registro CSV\n",
    "    ruta_csv = os.path.join(dir_checkpoints, \"registro_recompensas.csv\")\n",
    "    archivo_nuevo = not os.path.exists(ruta_csv)\n",
    "    with open(ruta_csv, \"a\", newline=\"\") as archivo:\n",
    "        escritor = csv.writer(archivo)\n",
    "        if archivo_nuevo:\n",
    "            escritor.writerow([\"episodio\", \"recompensa\"])\n",
    "        escritor.writerow([episodio, recompensas[-1]])\n",
    "\n",
    "    # Generar gráfica con nuevos colores\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(recompensas, label=\"Recompensa por episodio\", color=\"#FF6B35\", linewidth=1.2, alpha=0.7)\n",
    "    media_movil = _calcular_media_movil(recompensas, ventana=100)\n",
    "    if len(media_movil) > 0:\n",
    "        plt.plot(media_movil, label=\"Media Móvil (100 eps)\", color=\"#00A896\", linewidth=2.5)\n",
    "    plt.xlabel(\"Episodio\", fontsize=12)\n",
    "    plt.ylabel(\"Recompensa Total\", fontsize=12)\n",
    "    plt.title(\"Progreso de Entrenamiento - DQN\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    ruta_png = os.path.join(dir_checkpoints, f\"recompensas_ep{episodio}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ruta_png, dpi=120)\n",
    "    plt.close()\n",
    "    print(f\"[REGISTRO] Gráfica y CSV guardados: {ruta_png} / {ruta_csv}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Función de Entrenamiento DQN\n",
    "# ============================================================\n",
    "def entrenar_dqn(\n",
    "    directorio_checkpoints: str,\n",
    "    episodios_totales: int = 500,\n",
    "    capacidad_replay: int = 100_000,\n",
    "    tam_lote: int = 32,\n",
    "    factor_descuento: float = 0.99,\n",
    "    tasa_aprendizaje: float = 1e-4,\n",
    "    epsilon_inicial: float = 1.0,\n",
    "    epsilon_final: float = 0.1,\n",
    "    episodios_decaimiento_eps: int = 300,\n",
    "    intervalo_actualizacion_target: int = 1_000,\n",
    "    pasos_inicio_entrenamiento: int = 10_000,\n",
    "    intervalo_guardado: int = 50,\n",
    "    intervalo_graficas: int = 50,\n",
    "    pasos_maximos_por_episodio: int | None = None,\n",
    "    semilla_aleatoria: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un agente DQN para Galaxian.\n",
    "    Guarda checkpoints y gráficas en directorio_checkpoints.\n",
    "    \"\"\"\n",
    "    os.makedirs(directorio_checkpoints, exist_ok=True)\n",
    "\n",
    "    # Crear entorno con preprocesamiento\n",
    "    entorno_juego = crear_entorno_galaxian(semilla=semilla_aleatoria, modo_render=None)\n",
    "\n",
    "    # Detectar forma de observación\n",
    "    observacion, _ = entorno_juego.reset()\n",
    "    if observacion.ndim != 3:\n",
    "        entorno_juego.close()\n",
    "        raise ValueError(f\"Observación inesperada: ndim={observacion.ndim}\")\n",
    "\n",
    "    if observacion.shape[0] in (1, 3, 4):\n",
    "        forma_entrada = (observacion.shape[0], observacion.shape[1], observacion.shape[2])\n",
    "    else:\n",
    "        forma_entrada = (observacion.shape[2], observacion.shape[0], observacion.shape[1])\n",
    "\n",
    "    num_acciones = entorno_juego.action_space.n\n",
    "\n",
    "    # Inicializar redes\n",
    "    red_q_principal = RedDQN(forma_entrada, num_acciones).to(dispositivo)\n",
    "    red_q_objetivo = RedDQN(forma_entrada, num_acciones).to(dispositivo)\n",
    "    red_q_objetivo.load_state_dict(red_q_principal.state_dict())\n",
    "    red_q_objetivo.eval()\n",
    "\n",
    "    optimizador = optim.Adam(red_q_principal.parameters(), lr=tasa_aprendizaje)\n",
    "    memoria_experiencias = BufferReplay(capacidad_replay)\n",
    "\n",
    "    pasos_globales = 0\n",
    "\n",
    "    def calcular_epsilon(ep: int) -> float:\n",
    "        if ep >= episodios_decaimiento_eps:\n",
    "            return epsilon_final\n",
    "        fraccion = ep / float(episodios_decaimiento_eps)\n",
    "        return epsilon_inicial + fraccion * (epsilon_final - epsilon_inicial)\n",
    "\n",
    "    registro_recompensas: List[float] = []\n",
    "\n",
    "    for episodio_actual in range(1, episodios_totales + 1):\n",
    "        observacion, _ = entorno_juego.reset()\n",
    "        finalizado = False\n",
    "        recompensa_total = 0.0\n",
    "        epsilon_actual = calcular_epsilon(episodio_actual)\n",
    "        pasos_en_episodio = 0\n",
    "\n",
    "        while not finalizado:\n",
    "            pasos_globales += 1\n",
    "            pasos_en_episodio += 1\n",
    "\n",
    "            # Política epsilon-greedy\n",
    "            if random.random() < epsilon_actual:\n",
    "                accion_elegida = entorno_juego.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    obs_lote = np.expand_dims(observacion, axis=0)\n",
    "                    obs_tensor = torch.from_numpy(obs_lote).to(dispositivo)\n",
    "                    valores_q = red_q_principal(obs_tensor)\n",
    "                    accion_elegida = int(torch.argmax(valores_q, dim=1).item())\n",
    "\n",
    "            siguiente_obs, recompensa_step, terminado, truncado, _ = entorno_juego.step(accion_elegida)\n",
    "            finalizado = terminado or truncado\n",
    "            recompensa_total += float(recompensa_step)\n",
    "\n",
    "            memoria_experiencias.agregar(observacion, accion_elegida, recompensa_step, siguiente_obs, finalizado)\n",
    "            observacion = siguiente_obs\n",
    "\n",
    "            # Entrenamiento cuando hay suficientes experiencias\n",
    "            if len(memoria_experiencias) >= pasos_inicio_entrenamiento:\n",
    "                estados, acciones, recompensas, estados_sig, terminados = memoria_experiencias.muestrear(tam_lote)\n",
    "\n",
    "                estados_t = torch.from_numpy(estados).to(dispositivo)\n",
    "                estados_sig_t = torch.from_numpy(estados_sig).to(dispositivo)\n",
    "                acciones_t = torch.from_numpy(acciones).long().to(dispositivo)\n",
    "                recompensas_t = torch.from_numpy(recompensas).float().to(dispositivo)\n",
    "                terminados_t = torch.from_numpy(terminados.astype(np.float32)).to(dispositivo)\n",
    "\n",
    "                # Calcular Q(s,a) actual\n",
    "                valores_q = red_q_principal(estados_t)\n",
    "                q_valores_accion = valores_q.gather(1, acciones_t.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Calcular target: r + gamma * max Q_target(s',a')\n",
    "                with torch.no_grad():\n",
    "                    q_siguientes = red_q_objetivo(estados_sig_t).max(1)[0]\n",
    "                    q_objetivo = recompensas_t + factor_descuento * q_siguientes * (1.0 - terminados_t)\n",
    "\n",
    "                perdida = nn.functional.mse_loss(q_valores_accion, q_objetivo)\n",
    "\n",
    "                optimizador.zero_grad()\n",
    "                perdida.backward()\n",
    "                nn.utils.clip_grad_norm_(red_q_principal.parameters(), 10.0)\n",
    "                optimizador.step()\n",
    "\n",
    "            # Actualizar red objetivo\n",
    "            if pasos_globales % intervalo_actualizacion_target == 0:\n",
    "                red_q_objetivo.load_state_dict(red_q_principal.state_dict())\n",
    "\n",
    "            if pasos_maximos_por_episodio is not None and pasos_en_episodio >= pasos_maximos_por_episodio:\n",
    "                break\n",
    "\n",
    "        registro_recompensas.append(recompensa_total)\n",
    "        print(f\"[DQN] Episodio {episodio_actual}/{episodios_totales} | Recompensa: {recompensa_total:.1f} | ε={epsilon_actual:.3f} | Memoria={len(memoria_experiencias)}\")\n",
    "\n",
    "        # Guardar checkpoint\n",
    "        if episodio_actual % intervalo_guardado == 0:\n",
    "            ruta_ckpt = os.path.join(directorio_checkpoints, f\"dqn_galaxian_ep{episodio_actual}.pth\")\n",
    "            torch.save({\n",
    "                \"red_q\": red_q_principal.state_dict(),\n",
    "                \"red_objetivo\": red_q_objetivo.state_dict(),\n",
    "                \"optimizador\": optimizador.state_dict(),\n",
    "                \"episodio\": episodio_actual,\n",
    "                \"pasos_globales\": pasos_globales,\n",
    "                \"recompensas\": registro_recompensas,\n",
    "                \"forma_entrada\": forma_entrada,\n",
    "                \"num_acciones\": num_acciones,\n",
    "            }, ruta_ckpt)\n",
    "            print(f\"[CHECKPOINT] Guardado en: {ruta_ckpt}\")\n",
    "\n",
    "        # Guardar gráficas\n",
    "        if episodio_actual % intervalo_graficas == 0:\n",
    "            _guardar_grafica_y_csv(directorio_checkpoints, registro_recompensas, episodio_actual)\n",
    "\n",
    "    entorno_juego.close()\n",
    "\n",
    "    # Guardar modelo final\n",
    "    ruta_final = os.path.join(directorio_checkpoints, \"dqn_galaxian_final.pth\")\n",
    "    torch.save(red_q_principal.state_dict(), ruta_final)\n",
    "    print(f\"[FINALIZADO] Modelo final guardado en: {ruta_final}\")\n",
    "\n",
    "    return red_q_principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "train-dqn",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DQN] Episodio 1/10000 | Recompensa: 760.0 | ε=0.997 | Memoria=179\n",
      "[DQN] Episodio 2/10000 | Recompensa: 1140.0 | ε=0.994 | Memoria=418\n",
      "[DQN] Episodio 3/10000 | Recompensa: 300.0 | ε=0.991 | Memoria=521\n",
      "[DQN] Episodio 2/10000 | Recompensa: 1140.0 | ε=0.994 | Memoria=418\n",
      "[DQN] Episodio 3/10000 | Recompensa: 300.0 | ε=0.991 | Memoria=521\n",
      "[DQN] Episodio 4/10000 | Recompensa: 700.0 | ε=0.988 | Memoria=701\n",
      "[DQN] Episodio 4/10000 | Recompensa: 700.0 | ε=0.988 | Memoria=701\n",
      "[DQN] Episodio 5/10000 | Recompensa: 1390.0 | ε=0.985 | Memoria=990\n",
      "[DQN] Episodio 6/10000 | Recompensa: 730.0 | ε=0.982 | Memoria=1114\n",
      "[DQN] Episodio 5/10000 | Recompensa: 1390.0 | ε=0.985 | Memoria=990\n",
      "[DQN] Episodio 6/10000 | Recompensa: 730.0 | ε=0.982 | Memoria=1114\n",
      "[DQN] Episodio 7/10000 | Recompensa: 740.0 | ε=0.979 | Memoria=1324\n",
      "[DQN] Episodio 8/10000 | Recompensa: 580.0 | ε=0.976 | Memoria=1490\n",
      "[DQN] Episodio 7/10000 | Recompensa: 740.0 | ε=0.979 | Memoria=1324\n",
      "[DQN] Episodio 8/10000 | Recompensa: 580.0 | ε=0.976 | Memoria=1490\n",
      "[DQN] Episodio 9/10000 | Recompensa: 600.0 | ε=0.973 | Memoria=1647\n",
      "[DQN] Episodio 9/10000 | Recompensa: 600.0 | ε=0.973 | Memoria=1647\n",
      "[DQN] Episodio 10/10000 | Recompensa: 1130.0 | ε=0.970 | Memoria=1909\n",
      "[DQN] Episodio 11/10000 | Recompensa: 60.0 | ε=0.967 | Memoria=1984\n",
      "[DQN] Episodio 12/10000 | Recompensa: 560.0 | ε=0.964 | Memoria=2079\n",
      "[DQN] Episodio 10/10000 | Recompensa: 1130.0 | ε=0.970 | Memoria=1909\n",
      "[DQN] Episodio 11/10000 | Recompensa: 60.0 | ε=0.967 | Memoria=1984\n",
      "[DQN] Episodio 12/10000 | Recompensa: 560.0 | ε=0.964 | Memoria=2079\n",
      "[DQN] Episodio 13/10000 | Recompensa: 680.0 | ε=0.961 | Memoria=2252\n",
      "[DQN] Episodio 13/10000 | Recompensa: 680.0 | ε=0.961 | Memoria=2252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Entrenar DQN con configuración local\u001b[39;00m\n\u001b[1;32m      2\u001b[0m carpeta_dqn \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DIRECTORIO_BASE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdqn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m red_q_entrenada \u001b[38;5;241m=\u001b[39m \u001b[43mentrenar_dqn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectorio_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcarpeta_dqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodios_totales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervalo_guardado\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervalo_graficas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 251\u001b[0m, in \u001b[0;36mentrenar_dqn\u001b[0;34m(directorio_checkpoints, episodios_totales, capacidad_replay, tam_lote, factor_descuento, tasa_aprendizaje, epsilon_inicial, epsilon_final, episodios_decaimiento_eps, intervalo_actualizacion_target, pasos_inicio_entrenamiento, intervalo_guardado, intervalo_graficas, pasos_maximos_por_episodio, semilla_aleatoria)\u001b[0m\n\u001b[1;32m    248\u001b[0m         valores_q \u001b[38;5;241m=\u001b[39m red_q_principal(obs_tensor)\n\u001b[1;32m    249\u001b[0m         accion_elegida \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(torch\u001b[38;5;241m.\u001b[39margmax(valores_q, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m--> 251\u001b[0m siguiente_obs, recompensa_step, terminado, truncado, _ \u001b[38;5;241m=\u001b[39m \u001b[43mentorno_juego\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccion_elegida\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m finalizado \u001b[38;5;241m=\u001b[39m terminado \u001b[38;5;129;01mor\u001b[39;00m truncado\n\u001b[1;32m    253\u001b[0m recompensa_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(recompensa_step)\n",
      "Cell \u001b[0;32mIn[2], line 84\u001b[0m, in \u001b[0;36mEnvolturaApilamiento.step\u001b[0;34m(self, accion)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, accion):\n\u001b[0;32m---> 84\u001b[0m     observacion, recompensa, terminado, truncado, informacion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuadros_memoria\u001b[38;5;241m.\u001b[39mappend(observacion)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obtener_obs(), recompensa, terminado, truncado, informacion\n",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m, in \u001b[0;36mEnvolturaPreprocesamiento.step\u001b[0;34m(self, accion)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminado \u001b[38;5;129;01mor\u001b[39;00m truncado:\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m cuadro_procesado \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocesar_cuadro\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservacion_raw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cuadro_procesado, recompensa_acumulada, terminado, truncado, informacion\n",
      "Cell \u001b[0;32mIn[2], line 35\u001b[0m, in \u001b[0;36mEnvolturaPreprocesamiento.procesar_cuadro\u001b[0;34m(self, cuadro)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mescala_grises:\n\u001b[1;32m     34\u001b[0m     cuadro \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(cuadro, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2GRAY)\n\u001b[0;32m---> 35\u001b[0m     cuadro \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcuadro\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtam_pantalla\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtam_pantalla\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINTER_AREA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cuadro\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Entrenar DQN con configuración local\n",
    "carpeta_dqn = os.path.join(DIRECTORIO_BASE, \"dqn\")\n",
    "red_q_entrenada = entrenar_dqn(\n",
    "    directorio_checkpoints=carpeta_dqn, \n",
    "    episodios_totales=10000, \n",
    "    intervalo_guardado=500, \n",
    "    intervalo_graficas=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c-implementation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Usando GPU de Apple Silicon (MPS)\n"
     ]
    }
   ],
   "source": [
    "# Implementación de A2C (Advantage Actor-Critic) desde cero\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Detectar dispositivo: MPS (Mac M1/M2/M3/M4) > CUDA > CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    dispositivo = torch.device(\"mps\")\n",
    "    print(\"✅ Usando GPU de Apple Silicon (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    dispositivo = torch.device(\"cuda\")\n",
    "    print(\"✅ Usando GPU CUDA\")\n",
    "else:\n",
    "    dispositivo = torch.device(\"cpu\")\n",
    "    print(\"⚠️ Usando CPU (entrenamiento será más lento)\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Arquitectura Actor-Crítico\n",
    "# ============================================================\n",
    "class RedA2C(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal con arquitectura Actor-Crítico.\n",
    "    Backbone convolucional compartido con dos cabezas:\n",
    "    - Actor: predice distribución de acciones (logits)\n",
    "    - Crítico: predice valor del estado V(s)\n",
    "    \"\"\"\n",
    "    def __init__(self, forma_entrada: Tuple[int, int, int], num_acciones: int):\n",
    "        super().__init__()\n",
    "        canales, alto, ancho = forma_entrada\n",
    "        self.canales_esperados = canales\n",
    "\n",
    "        self.extractor_caracteristicas = nn.Sequential(\n",
    "            nn.Conv2d(canales, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tensor_prueba = torch.zeros(1, canales, alto, ancho)\n",
    "            tam_aplanado = self.extractor_caracteristicas(tensor_prueba).shape[1]\n",
    "\n",
    "        self.cabeza_actor = nn.Sequential(\n",
    "            nn.Linear(tam_aplanado, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_acciones)\n",
    "        )\n",
    "        self.cabeza_critico = nn.Sequential(\n",
    "            nn.Linear(tam_aplanado, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor_entrada: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Retorna:\n",
    "          - logits de política: (B, num_acciones)\n",
    "          - valores de estado V(s): (B,)\n",
    "        \"\"\"\n",
    "        if tensor_entrada.ndim != 4:\n",
    "            raise ValueError(f\"Esperado tensor 4D, recibido tensor_entrada.ndim={tensor_entrada.ndim}\")\n",
    "\n",
    "        if tensor_entrada.shape[1] != self.canales_esperados and tensor_entrada.shape[-1] == self.canales_esperados:\n",
    "            tensor_entrada = tensor_entrada.permute(0, 3, 1, 2)\n",
    "\n",
    "        tensor_entrada = tensor_entrada.float() / 255.0\n",
    "        caracteristicas = self.extractor_caracteristicas(tensor_entrada)\n",
    "        logits_politica = self.cabeza_actor(caracteristicas)\n",
    "        valores_estado = self.cabeza_critico(caracteristicas).squeeze(-1)\n",
    "        return logits_politica, valores_estado\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Política de Evaluación A2C\n",
    "# ============================================================\n",
    "class PoliticaA2C:\n",
    "    def __init__(self, red_ac: 'RedA2C'):\n",
    "        self.red_ac = red_ac.to(dispositivo)\n",
    "        self.red_ac.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, observacion: np.ndarray, info: dict) -> int:\n",
    "        \"\"\"Estrategia greedy para evaluación\"\"\"\n",
    "        if observacion.ndim != 3:\n",
    "            raise ValueError(f\"Esperado obs 3D, recibido observacion.ndim={observacion.ndim}\")\n",
    "\n",
    "        obs_lote = np.expand_dims(observacion, axis=0)\n",
    "        obs_tensor = torch.from_numpy(obs_lote).to(dispositivo)\n",
    "        logits, _ = self.red_ac(obs_tensor)\n",
    "        probabilidades = torch.softmax(logits, dim=-1)\n",
    "        accion = torch.argmax(probabilidades, dim=-1).item()\n",
    "        return int(accion)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Función de Entrenamiento A2C\n",
    "# ============================================================\n",
    "def entrenar_a2c(\n",
    "    directorio_checkpoints: str,\n",
    "    episodios_totales: int = 500,\n",
    "    factor_descuento: float = 0.99,\n",
    "    tasa_aprendizaje: float = 2.5e-4,\n",
    "    coef_entropia: float = 0.01,\n",
    "    coef_valor: float = 0.5,\n",
    "    longitud_rollout: int = 5,\n",
    "    intervalo_guardado: int = 50,\n",
    "    pasos_maximos_por_episodio: int | None = None,\n",
    "    semilla_aleatoria: int = 123,\n",
    "    lambda_gae: float = 0.95,\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un agente A2C para Galaxian con rollouts n-step y GAE(λ).\n",
    "    \"\"\"\n",
    "    os.makedirs(directorio_checkpoints, exist_ok=True)\n",
    "\n",
    "    entorno_juego = crear_entorno_galaxian(semilla=semilla_aleatoria, modo_render=None)\n",
    "\n",
    "    observacion, _ = entorno_juego.reset()\n",
    "    if observacion.ndim != 3:\n",
    "        entorno_juego.close()\n",
    "        raise ValueError(f\"Observación inesperada: ndim={observacion.ndim}\")\n",
    "\n",
    "    if observacion.shape[0] in (1, 3, 4):\n",
    "        forma_entrada = (observacion.shape[0], observacion.shape[1], observacion.shape[2])\n",
    "    else:\n",
    "        forma_entrada = (observacion.shape[2], observacion.shape[0], observacion.shape[1])\n",
    "\n",
    "    num_acciones = entorno_juego.action_space.n\n",
    "\n",
    "    red_ac = RedA2C(forma_entrada, num_acciones).to(dispositivo)\n",
    "    optimizador = optim.RMSprop(red_ac.parameters(), lr=tasa_aprendizaje, eps=1e-5)\n",
    "\n",
    "    indice_episodio = 0\n",
    "    registro_recompensas: List[float] = []\n",
    "\n",
    "    while indice_episodio < episodios_totales:\n",
    "        observacion, _ = entorno_juego.reset()\n",
    "        finalizado = False\n",
    "        recompensa_episodio = 0.0\n",
    "        pasos_en_episodio = 0\n",
    "\n",
    "        while not finalizado:\n",
    "            # Recolectar rollout\n",
    "            log_probs_lista = []\n",
    "            valores_lista = []\n",
    "            recompensas_lista = []\n",
    "            finalizados_lista = []\n",
    "            entropias_lista = []\n",
    "\n",
    "            for _ in range(longitud_rollout):\n",
    "                if finalizado:\n",
    "                    break\n",
    "\n",
    "                obs_lote = np.expand_dims(observacion, axis=0)\n",
    "                obs_tensor = torch.from_numpy(obs_lote).to(dispositivo)\n",
    "\n",
    "                logits, valor = red_ac(obs_tensor)\n",
    "                probabilidades = torch.softmax(logits, dim=-1)\n",
    "                distribucion = torch.distributions.Categorical(probabilidades)\n",
    "\n",
    "                accion_muestreada = distribucion.sample()\n",
    "                entropia = distribucion.entropy().mean()\n",
    "\n",
    "                siguiente_obs, recompensa_step, terminado, truncado, _ = entorno_juego.step(accion_muestreada.item())\n",
    "                finalizado = terminado or truncado\n",
    "\n",
    "                log_probs_lista.append(distribucion.log_prob(accion_muestreada).squeeze(0))\n",
    "                valores_lista.append(valor.squeeze(0))\n",
    "                recompensas_lista.append(torch.tensor(recompensa_step, dtype=torch.float32, device=dispositivo))\n",
    "                finalizados_lista.append(torch.tensor(float(finalizado), device=dispositivo))\n",
    "                entropias_lista.append(entropia)\n",
    "\n",
    "                observacion = siguiente_obs\n",
    "                recompensa_episodio += float(recompensa_step)\n",
    "                pasos_en_episodio += 1\n",
    "\n",
    "                if pasos_maximos_por_episodio is not None and pasos_en_episodio >= pasos_maximos_por_episodio:\n",
    "                    finalizado = True\n",
    "                    break\n",
    "\n",
    "            # Bootstrap del valor siguiente\n",
    "            if finalizado:\n",
    "                valor_siguiente = torch.zeros(1, device=dispositivo)\n",
    "            else:\n",
    "                obs_siguiente_lote = np.expand_dims(observacion, axis=0)\n",
    "                obs_siguiente_tensor = torch.from_numpy(obs_siguiente_lote).to(dispositivo)\n",
    "                _, valor_siguiente = red_ac(obs_siguiente_tensor)\n",
    "                valor_siguiente = valor_siguiente.detach()\n",
    "\n",
    "            # Calcular retornos con GAE(λ)\n",
    "            retornos_lista = []\n",
    "            ventaja_acumulada = 0\n",
    "            valor_futuro = valor_siguiente\n",
    "            for recomp, terminal, val in zip(reversed(recompensas_lista), reversed(finalizados_lista), reversed(valores_lista)):\n",
    "                valor_futuro = valor_futuro * (1.0 - terminal)\n",
    "                delta_temporal = recomp + factor_descuento * valor_futuro - val\n",
    "                ventaja_acumulada = delta_temporal + factor_descuento * lambda_gae * (1.0 - terminal) * ventaja_acumulada\n",
    "                valor_futuro = val\n",
    "                retornos_lista.insert(0, ventaja_acumulada + val)\n",
    "\n",
    "            retornos_tensor = torch.stack(retornos_lista)\n",
    "            valores_tensor = torch.stack(valores_lista)\n",
    "            log_probs_tensor = torch.stack(log_probs_lista)\n",
    "            entropias_tensor = torch.stack(entropias_lista) if len(entropias_lista) > 0 else torch.tensor(0.0, device=dispositivo)\n",
    "\n",
    "            ventajas = retornos_tensor - valores_tensor\n",
    "\n",
    "            perdida_politica = -(log_probs_tensor * ventajas.detach()).mean()\n",
    "            perdida_valor = ventajas.pow(2).mean()\n",
    "            perdida_entropia = entropias_tensor.mean() if entropias_tensor.ndim > 0 else entropias_tensor\n",
    "\n",
    "            perdida_total = perdida_politica + coef_valor * perdida_valor - coef_entropia * perdida_entropia\n",
    "\n",
    "            optimizador.zero_grad()\n",
    "            perdida_total.backward()\n",
    "            nn.utils.clip_grad_norm_(red_ac.parameters(), 0.5)\n",
    "            optimizador.step()\n",
    "\n",
    "            if finalizado:\n",
    "                break\n",
    "\n",
    "        indice_episodio += 1\n",
    "        registro_recompensas.append(recompensa_episodio)\n",
    "        print(f\"[A2C] Episodio {indice_episodio}/{episodios_totales} | Recompensa: {recompensa_episodio:.1f}\")\n",
    "\n",
    "        # Guardar checkpoint\n",
    "        if indice_episodio % intervalo_guardado == 0:\n",
    "            ruta_ckpt = os.path.join(directorio_checkpoints, f\"a2c_galaxian_ep{indice_episodio}.pth\")\n",
    "            torch.save({\n",
    "                \"red\": red_ac.state_dict(),\n",
    "                \"optimizador\": optimizador.state_dict(),\n",
    "                \"episodio\": indice_episodio,\n",
    "                \"recompensas\": registro_recompensas,\n",
    "                \"forma_entrada\": forma_entrada,\n",
    "                \"num_acciones\": num_acciones,\n",
    "            }, ruta_ckpt)\n",
    "            print(f\"[CHECKPOINT] Guardado en: {ruta_ckpt}\")\n",
    "\n",
    "    entorno_juego.close()\n",
    "\n",
    "    ruta_final = os.path.join(directorio_checkpoints, \"a2c_galaxian_final.pth\")\n",
    "    torch.save(red_ac.state_dict(), ruta_final)\n",
    "    print(f\"[FINALIZADO] Modelo final guardado en: {ruta_final}\")\n",
    "\n",
    "    return red_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "train-a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A2C] Episodio 1/50 | Recompensa: 570.0\n",
      "[A2C] Episodio 2/50 | Recompensa: 390.0\n",
      "[A2C] Episodio 2/50 | Recompensa: 390.0\n",
      "[A2C] Episodio 3/50 | Recompensa: 210.0\n",
      "[A2C] Episodio 3/50 | Recompensa: 210.0\n",
      "[A2C] Episodio 4/50 | Recompensa: 880.0\n",
      "[A2C] Episodio 4/50 | Recompensa: 880.0\n",
      "[A2C] Episodio 5/50 | Recompensa: 590.0\n",
      "[A2C] Episodio 5/50 | Recompensa: 590.0\n",
      "[A2C] Episodio 6/50 | Recompensa: 970.0\n",
      "[A2C] Episodio 6/50 | Recompensa: 970.0\n",
      "[A2C] Episodio 7/50 | Recompensa: 740.0\n",
      "[A2C] Episodio 7/50 | Recompensa: 740.0\n",
      "[A2C] Episodio 8/50 | Recompensa: 300.0\n",
      "[A2C] Episodio 8/50 | Recompensa: 300.0\n",
      "[A2C] Episodio 9/50 | Recompensa: 1090.0\n",
      "[A2C] Episodio 9/50 | Recompensa: 1090.0\n",
      "[A2C] Episodio 10/50 | Recompensa: 1270.0\n",
      "[A2C] Episodio 10/50 | Recompensa: 1270.0\n",
      "[A2C] Episodio 11/50 | Recompensa: 780.0\n",
      "[A2C] Episodio 11/50 | Recompensa: 780.0\n",
      "[A2C] Episodio 12/50 | Recompensa: 1000.0\n",
      "[A2C] Episodio 12/50 | Recompensa: 1000.0\n",
      "[A2C] Episodio 13/50 | Recompensa: 730.0\n",
      "[A2C] Episodio 13/50 | Recompensa: 730.0\n",
      "[A2C] Episodio 14/50 | Recompensa: 240.0\n",
      "[A2C] Episodio 14/50 | Recompensa: 240.0\n",
      "[A2C] Episodio 15/50 | Recompensa: 560.0\n",
      "[A2C] Episodio 15/50 | Recompensa: 560.0\n",
      "[A2C] Episodio 16/50 | Recompensa: 270.0\n",
      "[A2C] Episodio 16/50 | Recompensa: 270.0\n",
      "[A2C] Episodio 17/50 | Recompensa: 480.0\n",
      "[A2C] Episodio 17/50 | Recompensa: 480.0\n",
      "[A2C] Episodio 18/50 | Recompensa: 120.0\n",
      "[A2C] Episodio 18/50 | Recompensa: 120.0\n",
      "[A2C] Episodio 19/50 | Recompensa: 540.0\n",
      "[A2C] Episodio 19/50 | Recompensa: 540.0\n",
      "[A2C] Episodio 20/50 | Recompensa: 1040.0\n",
      "[A2C] Episodio 20/50 | Recompensa: 1040.0\n",
      "[A2C] Episodio 21/50 | Recompensa: 570.0\n",
      "[A2C] Episodio 21/50 | Recompensa: 570.0\n",
      "[A2C] Episodio 22/50 | Recompensa: 1120.0\n",
      "[A2C] Episodio 22/50 | Recompensa: 1120.0\n",
      "[A2C] Episodio 23/50 | Recompensa: 890.0\n",
      "[A2C] Episodio 23/50 | Recompensa: 890.0\n",
      "[A2C] Episodio 24/50 | Recompensa: 770.0\n",
      "[A2C] Episodio 24/50 | Recompensa: 770.0\n",
      "[A2C] Episodio 25/50 | Recompensa: 1070.0\n",
      "[A2C] Episodio 25/50 | Recompensa: 1070.0\n",
      "[A2C] Episodio 26/50 | Recompensa: 920.0\n",
      "[A2C] Episodio 26/50 | Recompensa: 920.0\n",
      "[A2C] Episodio 27/50 | Recompensa: 180.0\n",
      "[A2C] Episodio 27/50 | Recompensa: 180.0\n",
      "[A2C] Episodio 28/50 | Recompensa: 1040.0\n",
      "[A2C] Episodio 28/50 | Recompensa: 1040.0\n",
      "[A2C] Episodio 29/50 | Recompensa: 1110.0\n",
      "[A2C] Episodio 29/50 | Recompensa: 1110.0\n",
      "[A2C] Episodio 30/50 | Recompensa: 1520.0\n",
      "[A2C] Episodio 30/50 | Recompensa: 1520.0\n",
      "[A2C] Episodio 31/50 | Recompensa: 330.0\n",
      "[A2C] Episodio 31/50 | Recompensa: 330.0\n",
      "[A2C] Episodio 32/50 | Recompensa: 970.0\n",
      "[A2C] Episodio 32/50 | Recompensa: 970.0\n",
      "[A2C] Episodio 33/50 | Recompensa: 1010.0\n",
      "[A2C] Episodio 33/50 | Recompensa: 1010.0\n",
      "[A2C] Episodio 34/50 | Recompensa: 280.0\n",
      "[A2C] Episodio 34/50 | Recompensa: 280.0\n",
      "[A2C] Episodio 35/50 | Recompensa: 240.0\n",
      "[A2C] Episodio 35/50 | Recompensa: 240.0\n",
      "[A2C] Episodio 36/50 | Recompensa: 620.0\n",
      "[A2C] Episodio 36/50 | Recompensa: 620.0\n",
      "[A2C] Episodio 37/50 | Recompensa: 610.0\n",
      "[A2C] Episodio 37/50 | Recompensa: 610.0\n",
      "[A2C] Episodio 38/50 | Recompensa: 390.0\n",
      "[A2C] Episodio 38/50 | Recompensa: 390.0\n",
      "[A2C] Episodio 39/50 | Recompensa: 1100.0\n",
      "[A2C] Episodio 39/50 | Recompensa: 1100.0\n",
      "[A2C] Episodio 40/50 | Recompensa: 1070.0\n",
      "[A2C] Episodio 40/50 | Recompensa: 1070.0\n",
      "[A2C] Episodio 41/50 | Recompensa: 510.0\n",
      "[A2C] Episodio 41/50 | Recompensa: 510.0\n",
      "[A2C] Episodio 42/50 | Recompensa: 840.0\n",
      "[A2C] Episodio 42/50 | Recompensa: 840.0\n",
      "[A2C] Episodio 43/50 | Recompensa: 910.0\n",
      "[A2C] Episodio 43/50 | Recompensa: 910.0\n",
      "[A2C] Episodio 44/50 | Recompensa: 660.0\n",
      "[A2C] Episodio 44/50 | Recompensa: 660.0\n",
      "[A2C] Episodio 45/50 | Recompensa: 1070.0\n",
      "[A2C] Episodio 45/50 | Recompensa: 1070.0\n",
      "[A2C] Episodio 46/50 | Recompensa: 840.0\n",
      "[A2C] Episodio 46/50 | Recompensa: 840.0\n",
      "[A2C] Episodio 47/50 | Recompensa: 390.0\n",
      "[A2C] Episodio 47/50 | Recompensa: 390.0\n",
      "[A2C] Episodio 48/50 | Recompensa: 520.0\n",
      "[A2C] Episodio 48/50 | Recompensa: 520.0\n",
      "[A2C] Episodio 49/50 | Recompensa: 1160.0\n",
      "[A2C] Episodio 49/50 | Recompensa: 1160.0\n",
      "[A2C] Episodio 50/50 | Recompensa: 980.0\n",
      "[CHECKPOINT] Guardado en: /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento/a2c/a2c_galaxian_ep50.pth\n",
      "[FINALIZADO] Modelo final guardado en: /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento/a2c/a2c_galaxian_final.pth\n",
      "[A2C] Episodio 50/50 | Recompensa: 980.0\n",
      "[CHECKPOINT] Guardado en: /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento/a2c/a2c_galaxian_ep50.pth\n",
      "[FINALIZADO] Modelo final guardado en: /Users/isaackeitor/Desktop/Galaxian/resultados_entrenamiento/a2c/a2c_galaxian_final.pth\n"
     ]
    }
   ],
   "source": [
    "# Entrenar A2C con configuración local\n",
    "carpeta_a2c = os.path.join(DIRECTORIO_BASE, \"a2c\")\n",
    "red_a2c_entrenada = entrenar_a2c(\n",
    "    directorio_checkpoints=carpeta_a2c, \n",
    "    episodios_totales=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dueling-ddqn-per-implementation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Usando GPU de Apple Silicon (MPS)\n"
     ]
    }
   ],
   "source": [
    "# Implementación de Dueling Double DQN con Prioritized Experience Replay (PER)\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Detectar dispositivo: MPS (Mac M1/M2/M3/M4) > CUDA > CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    dispositivo = torch.device(\"mps\")\n",
    "    print(\"✅ Usando GPU de Apple Silicon (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    dispositivo = torch.device(\"cuda\")\n",
    "    print(\"✅ Usando GPU CUDA\")\n",
    "else:\n",
    "    dispositivo = torch.device(\"cpu\")\n",
    "    print(\"⚠️ Usando CPU (entrenamiento será más lento)\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Árboles de Segmentos para PER\n",
    "# ===========================\n",
    "class ArbolSegmentos:\n",
    "    def __init__(self, capacidad, funcion_reduccion):\n",
    "        assert capacidad > 0 and (capacidad & (capacidad - 1)) == 0, \\\n",
    "            \"Capacidad debe ser potencia de 2\"\n",
    "        self.capacidad = capacidad\n",
    "        self.arbol = np.zeros(2 * capacidad, dtype=np.float32)\n",
    "        self.funcion_reduccion = funcion_reduccion\n",
    "\n",
    "    def actualizar(self, indice, valor):\n",
    "        i = indice + self.capacidad\n",
    "        self.arbol[i] = valor\n",
    "        i //= 2\n",
    "        while i >= 1:\n",
    "            self.arbol[i] = self.funcion_reduccion(self.arbol[2 * i], self.arbol[2 * i + 1])\n",
    "            i //= 2\n",
    "\n",
    "    def reducir(self, inicio, fin):\n",
    "        resultado_izq = None\n",
    "        resultado_der = None\n",
    "        inicio += self.capacidad\n",
    "        fin += self.capacidad\n",
    "        while inicio <= fin:\n",
    "            if (inicio % 2) == 1:\n",
    "                resultado_izq = self.arbol[inicio] if resultado_izq is None else self.funcion_reduccion(resultado_izq, self.arbol[inicio])\n",
    "                inicio += 1\n",
    "            if (fin % 2) == 0:\n",
    "                resultado_der = self.arbol[fin] if resultado_der is None else self.funcion_reduccion(self.arbol[fin], resultado_der)\n",
    "                fin -= 1\n",
    "            inicio //= 2\n",
    "            fin //= 2\n",
    "        if resultado_izq is None:\n",
    "            return resultado_der\n",
    "        if resultado_der is None:\n",
    "            return resultado_izq\n",
    "        return self.funcion_reduccion(resultado_izq, resultado_der)\n",
    "\n",
    "    def __getitem__(self, indice):\n",
    "        return self.arbol[indice + self.capacidad]\n",
    "\n",
    "\n",
    "class ArbolSuma(ArbolSegmentos):\n",
    "    def __init__(self, capacidad):\n",
    "        super().__init__(capacidad, funcion_reduccion=lambda a, b: a + b)\n",
    "\n",
    "    def suma_total(self):\n",
    "        return self.arbol[1]\n",
    "\n",
    "    def encontrar_suma_prefijo(self, suma_objetivo):\n",
    "        \"\"\"Encuentra índice i tal que sum(0..i) >= suma_objetivo\"\"\"\n",
    "        idx = 1\n",
    "        while idx < self.capacidad:\n",
    "            izquierda = 2 * idx\n",
    "            if self.arbol[izquierda] >= suma_objetivo:\n",
    "                idx = izquierda\n",
    "            else:\n",
    "                suma_objetivo -= self.arbol[izquierda]\n",
    "                idx = izquierda + 1\n",
    "        return idx - self.capacidad\n",
    "\n",
    "\n",
    "class ArbolMinimo(ArbolSegmentos):\n",
    "    def __init__(self, capacidad):\n",
    "        super().__init__(capacidad, funcion_reduccion=min)\n",
    "\n",
    "    def minimo_total(self):\n",
    "        return self.arbol[1]\n",
    "\n",
    "\n",
    "# =======================================\n",
    "#  Buffer de Replay Priorizado\n",
    "# =======================================\n",
    "class BufferReplayPriorizado:\n",
    "    def __init__(self, capacidad_maxima: int, alfa_prioridad: float = 0.6, epsilon_per: float = 1e-6):\n",
    "        potencia_2 = 1\n",
    "        while potencia_2 < capacidad_maxima:\n",
    "            potencia_2 *= 2\n",
    "        self.capacidad = potencia_2\n",
    "        self.alfa_prioridad = alfa_prioridad\n",
    "        self.epsilon_per = epsilon_per\n",
    "\n",
    "        self.posicion = 0\n",
    "        self.tamanio = 0\n",
    "\n",
    "        self.estados = [None] * self.capacidad\n",
    "        self.acciones = np.zeros(self.capacidad, dtype=np.int64)\n",
    "        self.recompensas = np.zeros(self.capacidad, dtype=np.float32)\n",
    "        self.estados_siguientes = [None] * self.capacidad\n",
    "        self.terminados = np.zeros(self.capacidad, dtype=np.bool_)\n",
    "\n",
    "        self.arbol_suma = ArbolSuma(self.capacidad)\n",
    "        self.arbol_minimo = ArbolMinimo(self.capacidad)\n",
    "        self.prioridad_maxima = 1.0\n",
    "\n",
    "        for i in range(self.capacidad):\n",
    "            self.arbol_suma.actualizar(i, 0.0)\n",
    "            self.arbol_minimo.actualizar(i, float(\"inf\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tamanio\n",
    "\n",
    "    def agregar(self, estado, accion, recompensa, estado_sig, terminal):\n",
    "        idx = self.posicion\n",
    "        self.estados[idx] = estado\n",
    "        self.acciones[idx] = accion\n",
    "        self.recompensas[idx] = recompensa\n",
    "        self.estados_siguientes[idx] = estado_sig\n",
    "        self.terminados[idx] = terminal\n",
    "\n",
    "        prioridad = (self.prioridad_maxima + self.epsilon_per) ** self.alfa_prioridad\n",
    "        self.arbol_suma.actualizar(idx, prioridad)\n",
    "        self.arbol_minimo.actualizar(idx, prioridad)\n",
    "\n",
    "        self.posicion = (self.posicion + 1) % self.capacidad\n",
    "        self.tamanio = min(self.tamanio + 1, self.capacidad)\n",
    "\n",
    "    def muestrear(self, tam_lote: int, beta_importancia: float = 0.4):\n",
    "        \"\"\"Devuelve (indices, pesos_is, batch)\"\"\"\n",
    "        indices_salida = []\n",
    "        estados_salida = []\n",
    "        acciones_salida = np.empty(tam_lote, dtype=np.int64)\n",
    "        recompensas_salida = np.empty(tam_lote, dtype=np.float32)\n",
    "        estados_sig_salida = []\n",
    "        terminados_salida = np.empty(tam_lote, dtype=np.float32)\n",
    "\n",
    "        suma_total = self.arbol_suma.suma_total()\n",
    "        segmento = suma_total / tam_lote\n",
    "        probabilidad_minima = self.arbol_minimo.minimo_total() / suma_total\n",
    "        peso_maximo = (probabilidad_minima * self.tamanio) ** (-beta_importancia)\n",
    "\n",
    "        for i in range(tam_lote):\n",
    "            a = segmento * i\n",
    "            b = segmento * (i + 1)\n",
    "            masa = random.random() * (b - a) + a\n",
    "            idx = self.arbol_suma.encontrar_suma_prefijo(masa)\n",
    "            indices_salida.append(idx)\n",
    "            estados_salida.append(self.estados[idx])\n",
    "            acciones_salida[i] = self.acciones[idx]\n",
    "            recompensas_salida[i] = self.recompensas[idx]\n",
    "            estados_sig_salida.append(self.estados_siguientes[idx])\n",
    "            terminados_salida[i] = float(self.terminados[idx])\n",
    "\n",
    "        # Pesos de importance sampling\n",
    "        probabilidades = np.array([self.arbol_suma[idx] / suma_total for idx in indices_salida], dtype=np.float32)\n",
    "        pesos_is = (probabilidades * self.tamanio) ** (-beta_importancia)\n",
    "        pesos_is = pesos_is / peso_maximo\n",
    "        pesos_is = pesos_is.astype(np.float32)\n",
    "\n",
    "        return np.array(indices_salida), pesos_is, (np.array(estados_salida), acciones_salida, recompensas_salida, np.array(estados_sig_salida), terminados_salida)\n",
    "\n",
    "    def actualizar_prioridades(self, indices, prioridades):\n",
    "        for idx, prioridad in zip(indices, prioridades):\n",
    "            prioridad = float(prioridad + self.epsilon_per)\n",
    "            self.arbol_suma.actualizar(idx, prioridad ** self.alfa_prioridad)\n",
    "            self.arbol_minimo.actualizar(idx, prioridad ** self.alfa_prioridad)\n",
    "            self.prioridad_maxima = max(self.prioridad_maxima, prioridad)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Arquitectura Dueling DQN\n",
    "# ===========================\n",
    "class RedDuelingDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Arquitectura Dueling DQN:\n",
    "    Q(s,a) = V(s) + (A(s,a) - mean(A))\n",
    "    Dos streams separados para Valor y Ventaja.\n",
    "    \"\"\"\n",
    "    def __init__(self, forma_entrada: Tuple[int, int, int], num_acciones: int):\n",
    "        super().__init__()\n",
    "        canales, alto, ancho = forma_entrada\n",
    "        self.canales_esperados = canales\n",
    "\n",
    "        self.extractor_caracteristicas = nn.Sequential(\n",
    "            nn.Conv2d(canales, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            tam_aplanado = self.extractor_caracteristicas(torch.zeros(1, canales, alto, ancho)).shape[1]\n",
    "\n",
    "        self.stream_valor = nn.Sequential(\n",
    "            nn.Linear(tam_aplanado, 512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        self.stream_ventaja = nn.Sequential(\n",
    "            nn.Linear(tam_aplanado, 512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_acciones)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor_entrada):\n",
    "        if tensor_entrada.ndim != 4:\n",
    "            raise ValueError(f\"tensor_entrada.ndim={tensor_entrada.ndim}, esperado 4\")\n",
    "        if tensor_entrada.shape[1] != self.canales_esperados and tensor_entrada.shape[-1] == self.canales_esperados:\n",
    "            tensor_entrada = tensor_entrada.permute(0, 3, 1, 2)\n",
    "        tensor_entrada = tensor_entrada.float() / 255.0\n",
    "        caracteristicas = self.extractor_caracteristicas(tensor_entrada)\n",
    "        valor_estado = self.stream_valor(caracteristicas)\n",
    "        ventaja_acciones = self.stream_ventaja(caracteristicas)\n",
    "        valores_q = valor_estado + (ventaja_acciones - ventaja_acciones.mean(dim=1, keepdim=True))\n",
    "        return valores_q\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Política de Evaluación\n",
    "# ===========================\n",
    "class PoliticaDQN:\n",
    "    def __init__(self, red_q: 'RedDuelingDQN'):\n",
    "        self.red_q = red_q.to(dispositivo)\n",
    "        self.red_q.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, observacion: np.ndarray, info: dict) -> int:\n",
    "        if observacion.ndim != 3:\n",
    "            raise ValueError(\"obs debe ser 3D\")\n",
    "        obs_lote = np.expand_dims(observacion, axis=0)\n",
    "        obs_tensor = torch.from_numpy(obs_lote).to(dispositivo)\n",
    "        valores_q = self.red_q(obs_tensor)\n",
    "        return int(torch.argmax(valores_q, dim=1).item())\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Utilidades de logging\n",
    "# ===========================\n",
    "def _calcular_media_movil(valores: List[float], ventana: int = 100):\n",
    "    if len(valores) == 0:\n",
    "        return []\n",
    "    resultado = []\n",
    "    suma_acum = 0.0\n",
    "    cola = []\n",
    "    for v in valores:\n",
    "        cola.append(v)\n",
    "        suma_acum += v\n",
    "        if len(cola) > ventana:\n",
    "            suma_acum -= cola.pop(0)\n",
    "        resultado.append(suma_acum / len(cola))\n",
    "    return resultado\n",
    "\n",
    "def _guardar_grafica_y_csv(dir_checkpoints: str, recompensas: List[float], episodio: int):\n",
    "    os.makedirs(dir_checkpoints, exist_ok=True)\n",
    "    \n",
    "    # CSV\n",
    "    ruta_csv = os.path.join(dir_checkpoints, \"registro_recompensas.csv\")\n",
    "    archivo_nuevo = not os.path.exists(ruta_csv)\n",
    "    with open(ruta_csv, \"a\", newline=\"\") as archivo:\n",
    "        escritor = csv.writer(archivo)\n",
    "        if archivo_nuevo:\n",
    "            escritor.writerow([\"episodio\", \"recompensa\"])\n",
    "        escritor.writerow([episodio, recompensas[-1]])\n",
    "\n",
    "    # Gráfica PNG con nuevos colores\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(recompensas, label=\"Recompensa\", color=\"#FF6B35\", linewidth=1.2, alpha=0.7)\n",
    "    media_movil = _calcular_media_movil(recompensas, ventana=100)\n",
    "    if len(media_movil) > 0:\n",
    "        plt.plot(media_movil, label=\"Media Móvil (100)\", color=\"#00A896\", linewidth=2.5)\n",
    "    plt.xlabel(\"Episodio\", fontsize=12)\n",
    "    plt.ylabel(\"Recompensa Total\", fontsize=12)\n",
    "    plt.title(\"Progreso de Entrenamiento - Dueling DDQN + PER\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    ruta_png = os.path.join(dir_checkpoints, f\"recompensas_ep{episodio}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ruta_png, dpi=120)\n",
    "    plt.close()\n",
    "    print(f\"[REGISTRO] Gráfica y CSV guardados: {ruta_png} / {ruta_csv}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Función de Entrenamiento\n",
    "# ===========================\n",
    "def entrenar_dueling_ddqn_per(\n",
    "    directorio_checkpoints: str,\n",
    "    episodios_totales: int = 50000,\n",
    "    capacidad_buffer: int = 100_000,\n",
    "    tam_lote: int = 32,\n",
    "    factor_descuento: float = 0.99,\n",
    "    tasa_aprendizaje: float = 1e-4,\n",
    "    epsilon_inicial: float = 1.0,\n",
    "    epsilon_final: float = 0.1,\n",
    "    episodios_decaimiento_eps: int = 30000,\n",
    "    intervalo_actualizacion_target: int = 1000,\n",
    "    pasos_inicio_entrenamiento: int = 10_000,\n",
    "    alfa_per: float = 0.6,\n",
    "    beta_per_inicial: float = 0.4,\n",
    "    beta_per_final: float = 1.0,\n",
    "    episodios_annealing_beta: int = 50000,\n",
    "    epsilon_per: float = 1e-6,\n",
    "    intervalo_guardado: int = 500,\n",
    "    intervalo_graficas: int = 200,\n",
    "    pasos_maximos_por_episodio: int | None = None,\n",
    "    semilla_aleatoria: int = 42,\n",
    "):\n",
    "    os.makedirs(directorio_checkpoints, exist_ok=True)\n",
    "\n",
    "    entorno_juego = crear_entorno_galaxian(semilla=semilla_aleatoria, modo_render=None)\n",
    "\n",
    "    observacion, _ = entorno_juego.reset()\n",
    "    if observacion.ndim != 3:\n",
    "        entorno_juego.close()\n",
    "        raise ValueError(\"obs.ndim inesperado\")\n",
    "    if observacion.shape[0] in (1, 3, 4):\n",
    "        forma_entrada = (observacion.shape[0], observacion.shape[1], observacion.shape[2])\n",
    "    else:\n",
    "        forma_entrada = (observacion.shape[2], observacion.shape[0], observacion.shape[1])\n",
    "    num_acciones = entorno_juego.action_space.n\n",
    "\n",
    "    red_q_principal = RedDuelingDQN(forma_entrada, num_acciones).to(dispositivo)\n",
    "    red_q_objetivo = RedDuelingDQN(forma_entrada, num_acciones).to(dispositivo)\n",
    "    red_q_objetivo.load_state_dict(red_q_principal.state_dict())\n",
    "    red_q_objetivo.eval()\n",
    "\n",
    "    optimizador = optim.Adam(red_q_principal.parameters(), lr=tasa_aprendizaje)\n",
    "    memoria_experiencias = BufferReplayPriorizado(capacidad_buffer, alfa_prioridad=alfa_per, epsilon_per=epsilon_per)\n",
    "\n",
    "    registro_recompensas: List[float] = []\n",
    "    pasos_globales = 0\n",
    "\n",
    "    def calcular_epsilon(ep):\n",
    "        if ep >= episodios_decaimiento_eps:\n",
    "            return epsilon_final\n",
    "        fraccion = ep / float(episodios_decaimiento_eps)\n",
    "        return epsilon_inicial + fraccion * (epsilon_final - epsilon_inicial)\n",
    "\n",
    "    def calcular_beta(ep):\n",
    "        fraccion = min(1.0, ep / float(episodios_annealing_beta))\n",
    "        return beta_per_inicial + fraccion * (beta_per_final - beta_per_inicial)\n",
    "\n",
    "    for episodio_actual in range(1, episodios_totales + 1):\n",
    "        observacion, _ = entorno_juego.reset()\n",
    "        finalizado = False\n",
    "        recompensa_total = 0.0\n",
    "        pasos_en_episodio = 0\n",
    "\n",
    "        epsilon_actual = calcular_epsilon(episodio_actual)\n",
    "        beta_actual = calcular_beta(episodio_actual)\n",
    "\n",
    "        while not finalizado:\n",
    "            pasos_globales += 1\n",
    "            pasos_en_episodio += 1\n",
    "\n",
    "            # Política ε-greedy\n",
    "            if random.random() < epsilon_actual:\n",
    "                accion_elegida = entorno_juego.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    obs_lote = np.expand_dims(observacion, axis=0)\n",
    "                    obs_tensor = torch.from_numpy(obs_lote).to(dispositivo)\n",
    "                    valores_q = red_q_principal(obs_tensor)\n",
    "                    accion_elegida = int(torch.argmax(valores_q, dim=1).item())\n",
    "\n",
    "            siguiente_obs, recompensa_step, terminado, truncado, _ = entorno_juego.step(accion_elegida)\n",
    "            finalizado = terminado or truncado\n",
    "            recompensa_total += float(recompensa_step)\n",
    "\n",
    "            memoria_experiencias.agregar(observacion, accion_elegida, recompensa_step, siguiente_obs, finalizado)\n",
    "            observacion = siguiente_obs\n",
    "\n",
    "            # Entrenamiento\n",
    "            if len(memoria_experiencias) >= pasos_inicio_entrenamiento:\n",
    "                indices, pesos_is, lote = memoria_experiencias.muestrear(tam_lote, beta_importancia=beta_actual)\n",
    "                estados, acciones, recompensas, estados_sig, terminados = lote\n",
    "\n",
    "                estados_t = torch.from_numpy(estados).to(dispositivo)\n",
    "                estados_sig_t = torch.from_numpy(estados_sig).to(dispositivo)\n",
    "                acciones_t = torch.from_numpy(acciones).long().to(dispositivo)\n",
    "                recompensas_t = torch.from_numpy(recompensas).float().to(dispositivo)\n",
    "                terminados_t = torch.from_numpy(terminados).float().to(dispositivo)\n",
    "                pesos_is_t = torch.from_numpy(pesos_is).float().to(dispositivo)\n",
    "\n",
    "                # Q(s,a) actual\n",
    "                valores_q = red_q_principal(estados_t).gather(1, acciones_t.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Double DQN: selección con red principal, evaluación con red objetivo\n",
    "                with torch.no_grad():\n",
    "                    q_principal_sig = red_q_principal(estados_sig_t)\n",
    "                    acciones_optimas = torch.argmax(q_principal_sig, dim=1, keepdim=True)\n",
    "\n",
    "                    q_objetivo_sig = red_q_objetivo(estados_sig_t)\n",
    "                    q_siguientes = q_objetivo_sig.gather(1, acciones_optimas).squeeze(1)\n",
    "\n",
    "                    q_objetivo_valores = recompensas_t + factor_descuento * q_siguientes * (1.0 - terminados_t)\n",
    "\n",
    "                errores_td = q_objetivo_valores - valores_q\n",
    "                perdida = (pesos_is_t * errores_td.pow(2)).mean()\n",
    "\n",
    "                optimizador.zero_grad()\n",
    "                perdida.backward()\n",
    "                nn.utils.clip_grad_norm_(red_q_principal.parameters(), 10.0)\n",
    "                optimizador.step()\n",
    "\n",
    "                # Actualizar prioridades\n",
    "                nuevas_prioridades = errores_td.detach().abs().cpu().numpy() + epsilon_per\n",
    "                memoria_experiencias.actualizar_prioridades(indices, nuevas_prioridades)\n",
    "\n",
    "            # Sincronizar red objetivo\n",
    "            if pasos_globales % intervalo_actualizacion_target == 0:\n",
    "                red_q_objetivo.load_state_dict(red_q_principal.state_dict())\n",
    "\n",
    "            if pasos_maximos_por_episodio is not None and pasos_en_episodio >= pasos_maximos_por_episodio:\n",
    "                break\n",
    "\n",
    "        registro_recompensas.append(recompensa_total)\n",
    "        print(f\"[Dueling-DDQN+PER] Ep {episodio_actual}/{episodios_totales} | R: {recompensa_total:.1f} | ε={epsilon_actual:.3f} | β={beta_actual:.3f} | Memoria={len(memoria_experiencias)}\")\n",
    "\n",
    "        # Guardar checkpoint\n",
    "        if episodio_actual % intervalo_guardado == 0:\n",
    "            ruta_ckpt = os.path.join(directorio_checkpoints, f\"dueling_ddqn_per_ep{episodio_actual}.pth\")\n",
    "            torch.save({\n",
    "                \"red_q\": red_q_principal.state_dict(),\n",
    "                \"red_objetivo\": red_q_objetivo.state_dict(),\n",
    "                \"optimizador\": optimizador.state_dict(),\n",
    "                \"episodio\": episodio_actual,\n",
    "                \"pasos_globales\": pasos_globales,\n",
    "                \"recompensas\": registro_recompensas,\n",
    "                \"forma_entrada\": forma_entrada,\n",
    "                \"num_acciones\": num_acciones,\n",
    "            }, ruta_ckpt)\n",
    "            print(f\"[CHECKPOINT] Guardado: {ruta_ckpt}\")\n",
    "\n",
    "        # Guardar gráficas\n",
    "        if episodio_actual % intervalo_graficas == 0:\n",
    "            _guardar_grafica_y_csv(directorio_checkpoints, registro_recompensas, episodio_actual)\n",
    "\n",
    "    entorno_juego.close()\n",
    "\n",
    "    ruta_final = os.path.join(directorio_checkpoints, \"dueling_ddqn_per_final.pth\")\n",
    "    torch.save(red_q_principal.state_dict(), ruta_final)\n",
    "    print(f\"[FINALIZADO] Modelo final guardado en: {ruta_final}\")\n",
    "    _guardar_grafica_y_csv(directorio_checkpoints, registro_recompensas, episodios_totales)\n",
    "\n",
    "    return red_q_principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "train-dueling-ddqn-per",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dueling-DDQN+PER] Ep 1/100000 | R: 360.0 | ε=1.000 | β=0.400 | Memoria=155\n",
      "[Dueling-DDQN+PER] Ep 2/100000 | R: 870.0 | ε=1.000 | β=0.400 | Memoria=366\n",
      "[Dueling-DDQN+PER] Ep 3/100000 | R: 400.0 | ε=1.000 | β=0.400 | Memoria=435\n",
      "[Dueling-DDQN+PER] Ep 4/100000 | R: 150.0 | ε=1.000 | β=0.400 | Memoria=510\n",
      "[Dueling-DDQN+PER] Ep 2/100000 | R: 870.0 | ε=1.000 | β=0.400 | Memoria=366\n",
      "[Dueling-DDQN+PER] Ep 3/100000 | R: 400.0 | ε=1.000 | β=0.400 | Memoria=435\n",
      "[Dueling-DDQN+PER] Ep 4/100000 | R: 150.0 | ε=1.000 | β=0.400 | Memoria=510\n",
      "[Dueling-DDQN+PER] Ep 5/100000 | R: 180.0 | ε=1.000 | β=0.400 | Memoria=585\n",
      "[Dueling-DDQN+PER] Ep 6/100000 | R: 470.0 | ε=1.000 | β=0.400 | Memoria=726\n",
      "[Dueling-DDQN+PER] Ep 5/100000 | R: 180.0 | ε=1.000 | β=0.400 | Memoria=585\n",
      "[Dueling-DDQN+PER] Ep 6/100000 | R: 470.0 | ε=1.000 | β=0.400 | Memoria=726\n",
      "[Dueling-DDQN+PER] Ep 7/100000 | R: 270.0 | ε=1.000 | β=0.400 | Memoria=794\n",
      "[Dueling-DDQN+PER] Ep 8/100000 | R: 480.0 | ε=1.000 | β=0.400 | Memoria=890\n",
      "[Dueling-DDQN+PER] Ep 7/100000 | R: 270.0 | ε=1.000 | β=0.400 | Memoria=794\n",
      "[Dueling-DDQN+PER] Ep 8/100000 | R: 480.0 | ε=1.000 | β=0.400 | Memoria=890\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Entrenar Dueling DDQN + PER con configuración local\u001b[39;00m\n\u001b[1;32m      2\u001b[0m carpeta_dueling \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DIRECTORIO_BASE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdueling_ddqn_per\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m modelo_dueling \u001b[38;5;241m=\u001b[39m \u001b[43mentrenar_dueling_ddqn_per\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectorio_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcarpeta_dueling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodios_totales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervalo_guardado\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervalo_graficas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 383\u001b[0m, in \u001b[0;36mentrenar_dueling_ddqn_per\u001b[0;34m(directorio_checkpoints, episodios_totales, capacidad_buffer, tam_lote, factor_descuento, tasa_aprendizaje, epsilon_inicial, epsilon_final, episodios_decaimiento_eps, intervalo_actualizacion_target, pasos_inicio_entrenamiento, alfa_per, beta_per_inicial, beta_per_final, episodios_annealing_beta, epsilon_per, intervalo_guardado, intervalo_graficas, pasos_maximos_por_episodio, semilla_aleatoria)\u001b[0m\n\u001b[1;32m    380\u001b[0m         valores_q \u001b[38;5;241m=\u001b[39m red_q_principal(obs_tensor)\n\u001b[1;32m    381\u001b[0m         accion_elegida \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(torch\u001b[38;5;241m.\u001b[39margmax(valores_q, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m--> 383\u001b[0m siguiente_obs, recompensa_step, terminado, truncado, _ \u001b[38;5;241m=\u001b[39m \u001b[43mentorno_juego\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccion_elegida\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m finalizado \u001b[38;5;241m=\u001b[39m terminado \u001b[38;5;129;01mor\u001b[39;00m truncado\n\u001b[1;32m    385\u001b[0m recompensa_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(recompensa_step)\n",
      "Cell \u001b[0;32mIn[2], line 84\u001b[0m, in \u001b[0;36mEnvolturaApilamiento.step\u001b[0;34m(self, accion)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, accion):\n\u001b[0;32m---> 84\u001b[0m     observacion, recompensa, terminado, truncado, informacion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuadros_memoria\u001b[38;5;241m.\u001b[39mappend(observacion)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obtener_obs(), recompensa, terminado, truncado, informacion\n",
      "Cell \u001b[0;32mIn[2], line 46\u001b[0m, in \u001b[0;36mEnvolturaPreprocesamiento.step\u001b[0;34m(self, accion)\u001b[0m\n\u001b[1;32m     44\u001b[0m terminado \u001b[38;5;241m=\u001b[39m truncado \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msalto_cuadros):\n\u001b[0;32m---> 46\u001b[0m     observacion_raw, recomp, term, trunc, informacion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     recompensa_acumulada \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m recomp\n\u001b[1;32m     48\u001b[0m     terminado \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m term\n",
      "File \u001b[0;32m~/Desktop/Galaxian/.venv/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Galaxian/.venv/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Galaxian/.venv/lib/python3.10/site-packages/shimmy/atari_env.py:294\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[0;34m(self, action_ind)\u001b[0m\n\u001b[1;32m    292\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[0;32m--> 294\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43male\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    296\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Entrenar Dueling DDQN + PER con configuración local\n",
    "carpeta_dueling = os.path.join(DIRECTORIO_BASE, \"dueling_ddqn_per\")\n",
    "modelo_dueling = entrenar_dueling_ddqn_per(\n",
    "    directorio_checkpoints=carpeta_dueling,\n",
    "    episodios_totales=100000,\n",
    "    intervalo_guardado=500,\n",
    "    intervalo_graficas=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "resume-training-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint cargado desde episodio: 21000\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo: Reanudar entrenamiento desde checkpoint (opcional)\n",
    "# Descomenta y modifica según necesites\n",
    "\n",
    "ruta_checkpoint = os.path.join(carpeta_dueling, \"dueling_ddqn_per_ep21000.pth\")\n",
    "\n",
    "# Cargar checkpoint (weights_only=False para checkpoints con objetos numpy)\n",
    "checkpoint = torch.load(ruta_checkpoint, weights_only=False)\n",
    "\n",
    "# Crear red y cargar pesos\n",
    "forma_entrada = tuple(checkpoint['forma_entrada'])\n",
    "num_acciones = checkpoint['num_acciones']\n",
    "red_reanudada = RedDuelingDQN(forma_entrada, num_acciones).to(dispositivo)\n",
    "red_reanudada.load_state_dict(checkpoint['red_q'])\n",
    "\n",
    "episodio_inicio = checkpoint['episodio']\n",
    "print(f\"Checkpoint cargado desde episodio: {episodio_inicio}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
